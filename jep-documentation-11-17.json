[
  {
    "number": "JEP 403",
    "title": "Strongly Encapsulate JDK Internals",
    "url": "https://openjdk.org/jeps/403",
    "summary": "Strongly encapsulate all internal elements of the JDK, except for critical internal APIs such as sun.misc.Unsafe. It will no longer be possible to relax the strong encapsulation of internal elements via a single command-line option, as was possible in JDK&#160;9 through JDK&#160;16.",
    "goals": "Continue to improve the security and maintainability of the JDK, which is one of the primary goals of Project Jigsaw. Encourage developers to migrate from using internal elements to using standard APIs, so that both they and their users can upgrade without fuss to future Java releases.",
    "motivation": "Over the years the developers of various libraries, frameworks, tools, and applications have used internal elements of the JDK in ways that compromise both security and maintainability. In particular: Some non-public classes, methods, and fields of java.* packages define privileged operations such as the ability to define a new class in a specific class loader, while others convey sensitive data such as cryptographic keys. These elements are internal to the JDK, despite being in java.* packages. The use of these internal elements by external code, via reflection, puts the security of the platform at risk. All classes, methods, and fields of sun.* packages are internal APIs of the JDK. Most classes, methods, and fields of com.sun.*, jdk.*, and org.* packages are also internal APIs. These APIs were never standard, never supported, and never intended for external use. The use of these internal elements by external code is an ongoing maintenance burden. Time and effort spent preserving these APIs, so as not to break existing code, could be better spent moving the platform forward. In Java&#160;9, we improved both the security and the maintainability of the JDK by leveraging modules to limit access to its internal elements. Modules provide strong encapsulation, which means that Code outside of a module can only access the public and protected elements of the packages exported by that module, and protected elements can, further, only be accessed from subclasses of the classes that define them. Strong encapsulation applies at both compile time and run time, including when compiled code attempts to access elements via reflection at run time. The non-public elements of exported packages, and all elements of unexported packages, are said to be strongly encapsulated. In JDK 9 and later releases we strongly encapsulated all new internal elements, thereby limiting access to them. As an aid to migration, however, we deliberately chose not to strongly encapsulate, at run time, the internal elements that had existed in JDK&#160;8. Library and application code on the class path could thus continue to use reflection to access the non-public elements of java.* packages, and all elements of sun.* and other internal packages, for packages that existed in JDK&#160;8. This arrangement is called relaxed strong encapsulation, and was the default behavior in JDK&#160;9. We released JDK&#160;9 back in September 2017. Most of the commonly-used internal elements of the JDK now have standard replacements. Developers have had over three years in which to migrate away from internal elements of the JDK to standard APIs such as java.lang.invoke.MethodHandles.Lookup::defineClass, java.util.Base64, and java.lang.ref.Cleaner. Many library, framework, and tool maintainers have completed that migration and released updated versions of their components. The need for relaxed strong encapsulation is weaker now than it was in 2017, and it weakens further every year. In JDK&#160;16, released in March 2021, we took the next step toward strongly encapsulating all internal elements of the JDK. JEP 396 made strong encapsulation the default behavior except for critical internal APIs such as sun.misc.Unsafe, which remained available. In JDK&#160;16 it was still possible for end users to choose relaxed strong encapsulation in order to gain access to internal elements that existed in JDK&#160;8. We are now ready to take one more step in this journey by removing the ability to choose relaxed strong encapsulation. This means that all internal elements of the JDK will be strongly encapsulated except for critical internal APIs such as sun.misc.Unsafe.",
    "description": "Relaxed strong encapsulation is controlled by the launcher option --illegal-access. This option, introduced by JEP&#160;261, was provocatively named in order to discourage its use. In JDK&#160;16 and earlier releases, it works as follows: --illegal-access=permit arranges for every package that existed in JDK&#160;8 to be open to code in unnamed modules. Code on the class path can thus continue to use reflection to access the non-public elements of java.* packages, and all elements of sun.* and other internal packages, for packages that existed in JDK&#160;8. The first reflective-access operation to any such element causes a warning to be issued, but no warnings are issued after that point. This mode was the default from JDK&#160;9 through JDK&#160;15. --illegal-access=warn is identical to permit except that a warning message is issued for every illegal reflective-access operation. --illegal-access=debug is identical to warn except that both a warning message and a stack trace are issued for every illegal reflective-access operation. --illegal-access=deny disables all illegal-access operations except for those enabled by other command-line options, e.g., --add-opens. This mode was the default in JDK&#160;16. As the next step toward strongly encapsulating all internal elements of the JDK, we propose to make the --illegal-access option obsolete. Any use of this option, whether with permit, warn, debug, or deny, will have no effect other than to issue a warning message. We expect to remove the --illegal-access option entirely in a future release. With this change, it will no longer be possible for end users to use the --illegal-access option to enable access to internal elements of the JDK. (A list of the packages affected is available here.) The sun.misc and sun.reflect packages will still be exported by the jdk.unsupported module, and will still be open so that code can access their non-public elements via reflection. No other JDK packages will be open in this way. It will still be possible to use the --add-opens command-line option, or the Add-Opens JAR-file manifest attribute, to open specific packages. Exported com.sun APIs Most com.sun.* packages in the JDK are for internal use, but a few are supported for external use. These supported packages were exported in JDK&#160;9 and will continue to be exported, so you can continue to program against their public APIs. They will, however, no longer be open. Examples include The Compiler Tree API in the jdk.compiler module, The HTTP Server API in the jdk.httpserver module, The SCTP API in the jdk.sctp module, and JDK-specific extensions to the NIO API in the com.sun.nio.file package of the jdk.unsupported module.",
    "specification": ""
  },
  {
    "number": "JEP 390",
    "title": "Warnings for Value-Based Classes",
    "url": "https://openjdk.org/jeps/390",
    "summary": "Designate the primitive wrapper classes as value-based and deprecate their constructors for removal, prompting new deprecation warnings. Provide warnings about improper attempts to synchronize on instances of any value-based classes in the Java Platform.",
    "goals": "",
    "motivation": "The Valhalla Project is pursuing a significant enhancement to the Java programming model in the form of primitive classes. Such classes declare their instances to be identity-free and capable of inline or flattened representations, where instances can be copied freely between memory locations and encoded using solely the values of the instances' fields. The design and implementation of primitive classes is sufficiently mature that we can confidently anticipate migrating certain classes of the Java Platform to become primitive classes in a future release. Candidates for migration are informally designated as value-based classes in the API specifications. Broadly, this means that they encode immutable objects whose identity is unimportant to the behavior of the class, and that they don't provide instance creation mechanisms, such as public constructors, that promise a unique identity with each call. The primitive wrapper classes (java.lang.Integer, java.lang.Double, etc.) are also intended to become primitive classes. These classes satisfy most of the requirements to be designated as value-based, with the exception that they expose deprecated (since Java 9) public constructors. With some adjustments to the definition, they can be considered value-based classes, too. Clients of the value-based classes will generally be unaffected by primitive class migration, except where they violate recommendations for usage of these classes. In particular, when running on a future Java release in which the migration has occurred: Instances of these classes that are equal (per equals) may also be considered identical (per ==), potentially breaking programs that rely on a != result for correct behavior. Attempts to create wrapper class instances with new Integer, new Double, etc., rather than implicit boxing or calls to the valueOf factory methods, will produce LinkageErrors. Attempts to synchronize on instances of these classes will produce exceptions. These changes may be inconvenient for some, but the workarounds are straightforward: if you need an identity, use a different class&#8212;often one you define yourself, but Object or AtomicReference may also be suitable. The benefits of migrating to primitive classes&#8212;better performance, reliable equality semantics, unifying primitives and classes&#8212;will be well worth the inconvenience. (1) has already been discouraged by avoiding promises about unique identities in the value-based classes' factory methods. There is not a practical way to automatically detect programs that ignore these specifications and rely on current implementation behavior, but we expect such cases to be rare. We can discourage (2) by deprecating the wrapper class constructors for removal, which will amplify the warnings that occur when compiling calls to these constructors. A significant portion of existing Java projects (perhaps 1%-10% of them) call the wrapper class constructors, though in many cases they intend only to run on pre-9 Java releases. Many popular open-source projects have already responded to the deprecation warnings of Java 9 by removing wrapper constructor calls from their sources, and we can expect many more to do so, given the heightened urgency of \"deprecated for removal\" warnings. Additional features to mitigate this problem are described in the Dependencies section. We can discourage (3) by implementing warnings, at compile time and run time, to inform programmers that their synchronization operations will not work in a future release.",
    "description": "The primitive wrapper classes in java.lang (Byte, Short, Integer, Long, Float, Double, Boolean, and Character) have been designated as value-based. The description of value-based classes has been updated to allow for deprecated constructors and interning factories, and to better align with the requirements for primitive class migration (for example, a value-based class should not inherit any instance fields). To discourage misuse of value-based class instances: The primitive wrapper class constructors, originally deprecated in Java 9, have been deprecated for removal. Wherever the constructors are called in source, javac by default produces removal warnings. The jdeprscan tool may be used to identify usage of deprecated APIs in binaries. javac implements a new warning category, synchronization, which identifies usages of the synchronized statement with an operand of a value-based class type, or of a type whose subtypes are all specified to be value-based. The warning category is turned on by default, and can be manually selected with -Xlint:synchronization. HotSpot implements runtime detection of monitorenter occurring on a value-based class instance. The command-line option -XX:DiagnoseSyncOnValueBasedClasses=1 will treat the operation as a fatal error. The command-line option -XX:DiagnoseSyncOnValueBasedClasses=2 will turn on logging, both via the console and via JDK Flight Recorder events. Compile-time synchronization warnings depend on static typing, while runtime warnings can respond to synchronization on non-value-based class and interface types, like Object. For example: Double d = 20.0; synchronized (d) { ... } // javac warning &amp; HotSpot warning Object o = d; synchronized (o) { ... } // HotSpot warning The monitorexit bytecode and the Object methods wait, notify, and notifyAll have always thrown an IllegalMonitorStateException if invoked outside of a synchronized statement or method. There is thus no need for warnings about these operations. Identifying value-based classes Within the JDK, the @jdk.internal.ValueBased annotation is used to signal to javac and HotSpot that a class is value-based, or that an abstract class or interface requires value-based subclasses. @ValueBased is applied to the following declarations in the Java Platform API and the JDK: The primitive wrapper classes in java.lang; The class java.lang.Runtime.Version; The \"optional\" classes in java.util: Optional, OptionalInt, OptionalLong, and OptionalDouble; Many classes in the java.time API: Instant, LocalDate, LocalTime, LocalDateTime, ZonedDateTime, ZoneId, OffsetTime, OffsetDateTime, ZoneOffset, Duration, Period, Year, YearMonth, and MonthDay, and, in java.time.chrono: MinguoDate, HijrahDate, JapaneseDate, and ThaiBuddhistDate; The interface java.lang.ProcessHandle and its implementation classes; The implementation classes of the collection factories in java.util: List.of, List.copyOf, Set.of, Set.copyOf, Map.of, Map.copyOf, Map.ofEntries, and Map.entry. Wherever the annotation is applied to an abstract class or interface, it is also applied to all subclasses in the JDK. Some classes and interfaces in java.lang.constant and jdk.incubator.foreign have claimed to be value-based, but do not meet the revised requirements&#8212;for example, they inherit instance fields&#8212;and so cannot be migrated to be primitive classes. In this case, it's no longer appropriate to describe these as value-based classes, and their specifications have been revised. Scope of changes Java SE: This JEP modifies Java SE by refining the specifications of the primitive wrapper classes, existing value-based classes, and related interfaces and factory methods. It also deprecates for removal the primitive wrapper class constructors. It does not make any changes to the Java Language or Java Virtual Machine specifications. JDK: In the JDK, this JEP also adds new warning and logging capabilities to javac and HotSpot. And it defines the annotation @jdk.internal.ValueBased and applies it to a number of JDK classes.",
    "specification": ""
  },
  {
    "number": "JEP 386",
    "title": "Alpine Linux Port",
    "url": "https://openjdk.org/jeps/386",
    "summary": "Port the JDK to Alpine Linux, and to other Linux distributions that use musl as their primary C library, on both the x64 and AArch64 architectures,",
    "goals": "",
    "motivation": "Musl is an implementation, for Linux-based systems, of the standard library functionality described in the ISO C and POSIX standards. Several Linux distributions including Alpine Linux and OpenWrt are based on musl, while some others provide an optional musl package (e.g., Arch Linux). The Alpine Linux distribution is widely adopted in cloud deployments, microservices, and container environments due to its small image size. A Docker base image for Alpine Linux, for example, is less than 6 MB. Enabling Java to run out-of-the-box in such settings will allow Tomcat, Jetty, Spring, and other popular frameworks to work in such environments natively. By using jlink (JEP 282) to reduce the size of the Java runtime, a user will be able to create an even smaller image targeted to run a specific application. The set of modules required by an application can be determined via the jdeps command. For example, if a target application depends only on the java.base module then a Docker image with Alpine Linux and a Java runtime with just that module and the server VM fits in 38 MB. The same motivation applies to embedded deployments, which also have size constraints.",
    "description": "This JEP intends to integrate the Portola Project upstream. This port will not support the attach mechanism of the HotSpot Serviceability Agent. To build a musl variant of the JDK on Alpine Linux, the following packages are required: alpine-sdk alsa-lib alsa-lib-dev autoconf bash cups-dev cups-libs fontconfig fontconfig-dev freetype freetype-dev grep libx11 libx11-dev libxext libxext-dev libxrandr libxrandr-dev libxrender libxrender-dev libxt libxt-dev libxtst libxtst-dev linux-headers zip Once these packages are installed, the JDK build process works as usual. Musl ports for other architectures may be implemented in follow-up enhancements, if there is demand.",
    "specification": ""
  },
  {
    "number": "JEP 391",
    "title": "macOS/AArch64 Port",
    "url": "https://openjdk.org/jeps/391",
    "summary": "Port the JDK to macOS/AArch64.",
    "goals": "",
    "motivation": "Apple has announced a long-term plan to transition their line of Macintosh computers from x64 to AArch64. We therefore expect to see broad demand for a macOS/AArch64 port of the JDK. Although it will be possible to run a macOS/x64 build of the JDK on AArch64-based systems via macOS's built-in Rosetta 2 translator, the translation will almost certainly introduce a significant performance penalty.",
    "description": "An AArch64 port already exists for Linux (JEP 237), and work is underway on an AArch64 port for Windows (JEP 388). We expect to reuse existing AArch64 code from these ports by employing conditional compilation &#8212; as is usual in ports of the JDK &#8212; to accommodate differences in low-level conventions such as the application binary interface (ABI) and the set of reserved processor registers. macOS/AArch64 forbids memory segments from being executable and writeable at the same time, a policy known as write-xor-execute (W^X). The HotSpot VM routinely creates and modifies executable code, so this JEP will implement W^X support in HotSpot for macOS/AArch64.",
    "specification": ""
  },
  {
    "number": "JEP 340",
    "title": "One AArch64 Port, Not Two",
    "url": "https://openjdk.org/jeps/340",
    "summary": "Remove all of the sources related to the arm64 port while retaining the 32-bit ARM port and the 64-bit aarch64 port.",
    "goals": "",
    "motivation": "Removing this port will allow all contributors to focus their efforts on a single 64-bit ARM implementation, and eliminate the duplicate work required to maintain two ports.",
    "description": "Two 64-bit ARM ports exist in the JDK. The main sources for these are in the src/hotspot/cpu/arm and open/src/hotspot/cpu/aarch64 directories. Although both ports produce aarch64 implementations, for the sake of this JEP we shall refer to the former, which was contributed by Oracle, as arm64, and the latter as aarch64. Here are the tasks that will be completed as part of this JEP: Remove all arm64-specific sources and #ifdefs related to 64-bit versus 32-bit builds in open/src/hotspot/cpu/arm Scan the remaining JDK sources for #ifdefs related to this port Remove the build option for building this port. Make the aarch64 port the default build for the 64-bit ARM architecture. Validate that the remaining 32-bit ARM port continues to build and run the conformance tests with no regressions that did not exist prior to the changes related to this JEP. InstallingContributingSponsoringDevelopers' GuideVulnerabilitiesJDK GA/EA BuildsMailing listsWiki &#183; IRCMastodonBlueskyBylaws &#183; CensusLegalWorkshopJEP ProcessSource codeGitHubMercurialToolsGitjtreg harnessGroups(overview)AdoptionBuildClient LibrariesCompatibility &amp; Specification ReviewCompilerConformanceCore LibrariesGoverning BoardHotSpotIDE Tooling &amp; SupportInternationalizationJMXMembersNetworkingPortersQualitySecurityServiceabilityVulnerabilityWebProjects(overview, archive)AmberBabylonCRaCCode ToolsCoinCommon VM InterfaceDevelopers' GuideDevice I/ODukeGalahadGraalIcedTeaJDK 8 UpdatesJDK 9JDK (&#8230;, 24, 25, 26)JDK UpdatesJMCJigsawKonaLanaiLeydenLilliputLocale EnhancementLoomMemory Model UpdateMetropolisMulti-Language VMNashornNew I/OOpenJFXPanamaPenrosePort: AArch32Port: AArch64Port: BSDPort: HaikuPort: Mac OS XPort: MIPSPort: MobilePort: PowerPC/AIXPort: RISC-VPort: s390xSCTPShenandoahSkaraSumatraTsanValhallaVeronaVisualVMWakefieldZeroZGC &#169; 2025 Oracle Corporation and/or its affiliates Terms of Use &#183; License: GPLv2 &#183; Privacy &#183; Trademarks",
    "specification": ""
  },
  {
    "number": "JEP 388",
    "title": "Windows/AArch64 Port",
    "url": "https://openjdk.org/jeps/388",
    "summary": "Port the JDK to Windows/AArch64.",
    "goals": "",
    "motivation": "With the release of new consumer and server-class AArch64 (ARM64) hardware, Windows/AArch64 has become an important platform due to end-user demand.",
    "description": "We have ported the JDK to Windows/AArch64, by extending the work previously done for the Linux/AArch64 port (JEP 237). This port includes the template interpreter, the C1 and C2 JIT compilers, and garbage collectors (serial, parallel, G1, Z and Shenandoah). It supports both the Windows 10 and Windows Server 2016 operating systems. The focus of this JEP is not the porting effort itself, which is mostly complete, but rather the integration of the port into the JDK main-line repository. Currently, we have a little over a dozen changesets. We have kept the changes to the shared code to a minimum. Our changes extend the support of the AArch64 memory model to Windows, address some MSVC issues, add LLP64 support to the AArch64 port, and perform CPU feature detection on Windows. We have also modified the build scripts to better support cross-compilation and the Windows toolchain. The new platform code by itself is confined to 15 (+4) files and 1222 lines (+322). Early-access binaries are available here.",
    "specification": ""
  },
  {
    "number": "JEP 349",
    "title": "JFR Event Streaming",
    "url": "https://openjdk.org/jeps/349",
    "summary": "Expose JDK Flight Recorder data for continuous monitoring.",
    "goals": "Provide an API for the continuous consumption of JFR data on disk, both for in-process and out-of-process applications. Record the same set of events as in the non-streaming case, with overhead less than 1% if possible. Event streaming must be able to co-exist with non-streaming recordings, both disk and memory based.",
    "motivation": "The HotSpot VM emits more than 500 data points using JFR, most of them not available through other means besides parsing log files. To consume the data today, a user must start a recording, stop it, dump the contents to disk and then parse the recording file. This works well for application profiling, where typically at least a minute of data is being recorded at a time, but not for monitoring purposes. An example of monitoring usage is a dashboard which displays dynamic updates to the data. There is overhead associated with creating a recording, such as: Emitting events that must occur when a new recording is created, Writing event metadata, such as the field layout, Writing checkpoint data, such as stack traces, and Copying data from the disk repository to a separate recording file. If there were a way to read data being recorded from the disk repository without creating a new recording file, much of this overhead could be avoided.",
    "description": "The package jdk.jfr.consumer, in module jdk.jfr, is extended with functionality to subscribe to events asynchronously. Users can read recording data directly, or stream, from the disk repository without dumping a recording file. The way to interact with a stream is to register a handler, for example a lambda function, to be invoked in response to the arrival of an event. The following example prints the overall CPU usage and locks contended for more than 10 ms. try (var rs = new RecordingStream()) { rs.enable(\"jdk.CPULoad\").withPeriod(Duration.ofSeconds(1)); rs.enable(\"jdk.JavaMonitorEnter\").withThreshold(Duration.ofMillis(10)); rs.onEvent(\"jdk.CPULoad\", event -&gt; { System.out.println(event.getFloat(\"machineTotal\")); }); rs.onEvent(\"jdk.JavaMonitorEnter\", event -&gt; { System.out.println(event.getClass(\"monitorClass\")); }); rs.start(); } The RecordingStream class implements the interface jdk.jfr.consumer.EventStream that provides a uniform way to filter and consume events regardless if the source is a live stream or a file on disk. public interface EventStream extends AutoCloseable { public static EventStream openRepository(); public static EventStream openRepository(Path directory); public static EventStream openFile(Path file); void setStartTime(Instant startTime); void setEndTime(Instant endTime); void setOrdered(boolean ordered); void setReuse(boolean reuse); void onEvent(Consumer&lt;RecordedEvent&gt; handler); void onEvent(String eventName, Consumer&lt;RecordedEvent handler); void onFlush(Runnable handler); void onClose(Runnable handler); void onError(Runnable handler); void remove(Object handler); void start(); void startAsync(); void awaitTermination(); void awaitTermination(Duration duration); void close(); } There are three factory methods to create a stream. EventStream::openRepository(Path) constructs a stream from a disk repository. This is a way to monitor other processes by working directly against the file system. The location of the disk repository is stored in the system property \"jdk.jfr.repository\" that can be read using the attach API. It is also possible to perform in-process monitoring using the EventStream::openRepository() method. Unlike RecordingStream, it does not start a recording. Instead, the stream receives events only when recordings are started by external means, for example using JCMD or JMX. The method EventStream::openFile(Path) creates a stream from a recording file. It complements the RecordingFile class that already exists today. The interface can also be used to set the amount of data to buffer and if events should be ordered chronologically. To minimize allocation pressure, there is also an option to control if a new event object should be allocated for each event, or if a previous object can be reused. A stream can be started in the current thread or asynchronously. Events stored in thread-local buffers are flushed periodically to the disk repository by the Java Virtual Machine (JVM) once every second. A separate thread parses the most recent file, up to the point in which data has been written, and pushes the events to subscribers. To keep overhead low, only actively subscribed events are read from the file. To receive a notification when a flush is complete, a handler can be registered using the EventStream::onFlush(Runnable) method. This is an opportunity to aggregate or push data to external systems while the JVM is preparing the next set of events.",
    "specification": ""
  },
  {
    "number": "JEP 344",
    "title": "Abortable Mixed Collections for G1",
    "url": "https://openjdk.org/jeps/344",
    "summary": "Make G1 mixed collections abortable if they might exceed the pause target.",
    "goals": "",
    "motivation": "One of the goals of G1 is to meet a user supplied pause time target for its collection pauses. G1 uses an advanced analysis engine to select the amount of work to be done during a collection (this is partly based on application behavior). The result of this selection is a set of regions called the collection set. Once the collection set has been determined and the collection has been started then G1 must collect all live objects in all regions of the collection set without stopping. This behavior can lead to G1 exceeding the pause time goal if the heuristics choose a too-large collection set, which for example can happen if the application&#8217;s behavior changes such that the heuristics work on \"stale\" data. This can in particular be observed during mixed collections, where the collection set can often contain too many old regions. There is need for a mechanism that detects when the heuristics repeatedly select a wrong amount of work for collections, and if so, have G1 perform the collection work incrementally in steps, where the collection can be aborted after each step. Such a mechanism would allow G1 to meet the pause time goal more often.",
    "description": "If G1 discovers that the collection set selection heuristics repeatedly select the wrong number of regions, switch to a more incremental way of doing mixed collections: split the collection set into two parts, a mandatory and an optional part. The mandatory part comprises parts of the collection set that G1 cannot process incrementally (e.g., the young regions) but can also contain old regions for improved efficiency. This may, e.g., be 80% of the predicted collection set. The remaining 20% of the predicted collection set, which would consist of only old regions, then forms the optional part. After G1 finishes collecting the mandatory part, G1 starts collecting the optional part at a much more granular level, if there is time left. The granularity of collection of this optional part depends on the amount of time left, at most down to one region at a time. After completing collection of any part of the optional collection set, G1 can decide to stop the collection depending on the remaining time. As the predictions get more accurate again, the optional part of a collection are made smaller and smaller, until the mandatory part once again comprises all of the collection set (i.e., G1 completely relies on its heuristics). If the predictions becomes inaccurate again, then the next collections will consist of both a mandatory and optional part again.",
    "specification": ""
  },
  {
    "number": "JEP 345",
    "title": "NUMA-Aware Memory Allocation for G1",
    "url": "https://openjdk.org/jeps/345",
    "summary": "Improve G1 performance on large machines by implementing NUMA-aware memory allocation.",
    "goals": "",
    "motivation": "Modern multi-socket machines increasingly have non-uniform memory access (NUMA), that is, memory is not equidistant from every socket or core. Memory accesses between sockets have different performance characteristics, with access to more-distant sockets typically having more latency. The parallel collector, enabled by by -XX:+UseParallelGC, has been NUMA-aware for many years. This has helped to improve the performance of configurations that run a single JVM across multiple sockets. Other HotSpot collectors have not had the benefit of this feature, which means they have not been able to take advantage of such vertical multi-socket NUMA scaling. Large enterprise applications in particular tend run with large heap configurations on multiple sockets, yet they want the manageability advantage of running within a single JVM. Users who use the G1 collector are increasingly running up against this scaling bottleneck.",
    "description": "G1's heap is organized as a collection of fixed-size regions. A region is typically a set of physical pages, although when using large pages (via -XX:+UseLargePages) several regions may make up a single physical page. If the +XX:+UseNUMA option is specified then, when the JVM is initialized, the regions will be evenly spread across the total number of available NUMA nodes. Fixing the NUMA node of each region at the beginning is a bit inflexible, but this can be mitigated by the following enhancements. In order to allocate a new object for a mutator thread, G1 may need to allocate a new region. It will do so by preferentially selecting a free region from the NUMA node to which the current thread is bound, so that the object will be kept on the same NUMA node in the young generation. If there is no free region on the same NUMA node during region allocation for a mutator then G1 will trigger a garbage collection. An alternative idea to be evaluated is to search other NUMA nodes for free regions in order of distance, starting with the closest NUMA node. We will not attempt to keep objects on the same NUMA node in the old generation. Humongous regions are excluded from in this allocation policy. We will do nothing special for these regions.",
    "specification": ""
  },
  {
    "number": "JEP 346",
    "title": "Promptly Return Unused Committed Memory from G1",
    "url": "https://openjdk.org/jeps/346",
    "summary": "Enhance the G1 garbage collector to automatically return Java heap memory to the operating system when idle.",
    "goals": "",
    "motivation": "Currently the G1 garbage collector may not return committed Java heap memory to the operating system in a timely manner. G1 only returns memory from the Java heap at either a full GC or during a concurrent cycle. Since G1 tries hard to completely avoid full GCs, and only triggers a concurrent cycle based on Java heap occupancy and allocation activity, it will not return Java heap memory in many cases unless forced to do so externally. This behavior is particularly disadvantageous in container environments where resources are paid by use. Even during phases where the VM only uses a fraction of its assigned memory resources due to inactivity, G1 will retain all of the Java heap. This results in customers paying for all resources all the time, and cloud providers not being able to fully utilize their hardware. If the VM were able to detect phases of Java heap under-utilization (\"idle\" phases), and automatically reduce its heap usage during that time, both would benefit. Shenandoah and OpenJ9's GenCon collector already provide similar functionality. Tests with a prototype in Bruno et al., section 5.5, shows that based on the real-world utilization of a Tomcat server that serves HTTP requests during the day, and is mostly idle during the night, this solution can reduce the amount of memory committed by the Java VM by 85%.",
    "description": "To accomplish the goal of returning a maximum amount of memory to the operating system, G1 will, during inactivity of the application, periodically try to continue or trigger a concurrent cycle to determine overall Java heap usage. This will cause it to automatically return unused portions of the Java heap back to the operating system. Optionally, under user control, a full GC can be performed to maximize the amount of memory returned. The application is considered inactive, and G1 triggers a periodic garbage collection if both: More than G1PeriodicGCInterval milliseconds have passed since any previous garbage collection pause and there is no concurrent cycle in progress at this point. A value of zero indicates that periodic garbage collections to promptly reclaim memory are disabled. The average one-minute system load value as returned by the getloadavg() call on the JVM host system (e.g. container) is below G1PeriodicGCSystemLoadThreshold. This condition is ignored if G1PeriodicGCSystemLoadThreshold is zero. If either of these conditions is not met, the current prospective periodic garbage collection is cancelled. A periodic garbage collection is reconsidered the next time G1PeriodicGCInterval time passes. The type of periodic garbage collection is determined by the value of the G1PeriodicGCInvokesConcurrent option: if set, G1 continues or starts a concurrent cycle, otherwise G1 performs a full GC. At the end of either collection, G1 adjusts the current Java heap size, potentially returning memory to the operation system. The new Java heap size is determined by the existing configuration for adjusting the Java heap size, including but not limited to the MinHeapFreeRatio, the MaxHeapFreeRatio, and minimum and maximum heap size configuration. By default, G1 starts or continues a concurrent cycle during this periodic garbage collection. This minimizes disruption of the application, but compared to a full collection may ultimately not be able to return as much memory. Any garbage collection triggered by this mechanism is tagged with the G1 Periodic Collection cause. An example of how such a log could look like is as follows: (1) [6.084s][debug][gc,periodic ] Checking for periodic GC. [6.086s][info ][gc ] GC(13) Pause Young (Concurrent Start) (G1 Periodic Collection) 37M-&gt;36M(78M) 1.786ms (2) [9.087s][debug][gc,periodic ] Checking for periodic GC. [9.088s][info ][gc ] GC(15) Pause Young (Prepare Mixed) (G1 Periodic Collection) 9M-&gt;9M(32M) 0.722ms (3) [12.089s][debug][gc,periodic ] Checking for periodic GC. [12.091s][info ][gc ] GC(16) Pause Young (Mixed) (G1 Periodic Collection) 9M-&gt;5M(32M) 1.776ms (4) [15.092s][debug][gc,periodic ] Checking for periodic GC. [15.097s][info ][gc ] GC(17) Pause Young (Mixed) (G1 Periodic Collection) 5M-&gt;1M(32M) 4.142ms (5) [18.098s][debug][gc,periodic ] Checking for periodic GC. [18.100s][info ][gc ] GC(18) Pause Young (Concurrent Start) (G1 Periodic Collection) 1M-&gt;1M(32M) 1.685ms (6) [21.101s][debug][gc,periodic ] Checking for periodic GC. [21.102s][info ][gc ] GC(20) Pause Young (Concurrent Start) (G1 Periodic Collection) 1M-&gt;1M(32M) 0.868ms (7) [24.104s][debug][gc,periodic ] Checking for periodic GC. [24.104s][info ][gc ] GC(22) Pause Young (Concurrent Start) (G1 Periodic Collection) 1M-&gt;1M(32M) 0.778ms In the above example, run with a G1PeriodicGCInterval of 3000ms, in step (1) G1 initiates a concurrent cycle, as indicated by (Concurrent Start) and (G1 Periodic Collection), after some inactivity of the application. This concurrent cycle initially returns some memory, shown by the decrease in the capacity numbers (78M) and (32M) from (1) to (2). In the interval between (2) to (4) more periodic collections are triggered, this time triggering a mixed collection to compact the heap. The following periodic garbage collections (5) to (7) start a concurrent cycle as G1 policy determines that at that time there is not enough garbage in the old generation to start a mixed GC phase. In this case, periodic garbage collections (5) to (7) will not further shrink the heap since the minimum heap size has already been reached. Changes to object liveness during application inactivity (e.g., due to soft references expiring) may trigger further reductions in committed Java heap during that idle time.",
    "specification": ""
  },
  {
    "number": "JEP 379",
    "title": "Shenandoah: A Low-Pause-Time Garbage Collector",
    "url": "https://openjdk.org/jeps/379",
    "summary": "Change the Shenandoah garbage collector from an experimental feature into a product feature.",
    "goals": "",
    "motivation": "Shenandoah was integrated into JDK 12 by JEP 189. It was marked as experimental in order to match the status of other new GCs, notably Epsilon GC and ZGC. Now Shenandoah is ready to drop its experimental status in mainline JDKs, as was recently suggested for ZGC in JEP 377.",
    "description": "In JDK 12 and later, Shenandoah is enabled via the -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC options. Making Shenandoah a product feature means that -XX:+UnlockExperimentalVMOptions would no longer be needed. A bunch of related Shenandoah options would turn from \"experimental\" to \"product\", subject to review. The default values for the options would not change. It makes this change a rather cosmetic change in flag classes. At the time of its integration into JDK 12, Shenandoah had been already shipping in Red Hat 8u and 11u downstream releases as a supported garbage collector, and used by RHEL and RHEL downstream users. Because of this, Shenandoah 8u and Shenandoah 11u are already non-experimental and thus not affected by this change. Since there are only a few users that run something other than 8u and 11u, we expect the actual impact of this change to be minuscule.",
    "specification": ""
  },
  {
    "number": "JEP 377",
    "title": "ZGC: A Scalable Low-Latency Garbage Collector",
    "url": "https://openjdk.org/jeps/377",
    "summary": "Change the Z Garbage Collector from an experimental feature into a product feature.",
    "goals": "",
    "motivation": "ZGC was integrated into JDK 11 by JEP 333. New features of such size and complexity are best introduced carefully and gradually, so we made it an experimental feature. This helped set user expectations and allowed users to provide feedback without having to download or build a separate JDK binary, which would have been the case if ZGC development had continued outside of the JDK. Since its introduction in JDK 11 we&#8217;ve received positive feedback, we&#8217;ve ironed out many bugs, and we&#8217;ve added a number of features and enhancements. To highlight some of the more important ones: Concurrent class unloading Uncommitting unused memory (JEP 351) Maximum heap size increased from 4TB to 16TB Minimum heap size decreased to 8MB -XX:SoftMaxHeapSize Support for the JFR leak profiler Support for class-data sharing Limited and discontiguous address spaces Support for placing the heap on NVRAM Improved NUMA awareness Multi-threaded heap pre-touching Furthermore, all commonly used platforms are now supported: Linux/x86_64 (JEP 333) Linux/aarch64 (8214527) Windows (JEP 365) macOS (JEP 364) Testing of ZGC suggests that it is stable and, as of this writing, we have received no new ZGC-specific bugs for a few months. With the stability, feature set, and platform support that ZGC has today, it is time to remove its experimental status and make it a product feature.",
    "description": "ZGC is enabled today via the -XX:+UnlockExperimentalVMOptions -XX:+UseZGC command-line options. Making ZGC a product (non-experimental) feature means that the -XX:+UnlockExperimentalVMOptions option will no longer be needed. Turning ZGC into a product (non-experimental) feature is mainly a matter of changing the UseZGC command-line option type from experimental to product. In addition, we will also change the following ZGC-specific options, currently marked experimental, to product. We will not change the default values for these options. ZAllocationSpikeTolerance ZCollectionInterval ZFragmentationLimit ZMarkStackSpaceLimit ZProactive ZUncommit ZUncommitDelay The following ZGC-specific JFR events, currently marked experimental, will also be changed to product. ZAllocationStall ZPageAllocation ZPageCacheFlush ZRelocationSet ZRelocationSetGroup ZUncommit",
    "specification": ""
  },
  {
    "number": "JEP 376",
    "title": "ZGC: Concurrent Thread-Stack Processing",
    "url": "https://openjdk.org/jeps/376",
    "summary": "Move ZGC thread-stack processing from safepoints to a concurrent phase.",
    "goals": "Remove thread-stack processing from ZGC safepoints. Make stack processing lazy, cooperative, concurrent, and incremental. Remove all other per-thread root processing from ZGC safepoints. Provide a mechanism by which other HotSpot subsystems can lazily process stacks.",
    "motivation": "The ZGC garbage collector (GC) aims to make GC pauses and scalability issues in HotSpot a thing of the past. We have, so far, moved all GC operations that scale with the size of the heap and the size of metaspace out of safepoint operations and into concurrent phases. Those include marking, relocation, reference processing, class unloading, and most root processing. The only activities still done in GC safepoints are a subset of root processing and a time-bounded marking termination operation. The roots include Java thread stacks and various other thread roots. These roots are problematic, since they scale with the number of threads. With many threads on large machine, root processing becomes a problem. In order to move beyond what we have today, and to meet the expectation that time spent inside of GC safepoints does not exceed one millisecond, even on large machines, we must move this per-thread processing, including stack scanning, out to a concurrent phase. After this work, essentially nothing of significance will be done inside ZGC safepoint operations. The infrastructure built as part of this project may eventually be used by other projects, such as Loom and JFR, to unify lazy stack processing.",
    "description": "We propose to address the stack-scanning problem with a stack watermark barrier. A GC safepoint will logically invalidate Java thread stacks by flipping a global variable. Each invalidated stack will be processed concurrently, keeping track of what remains to be processed. As each thread wakes up from the safepoint it will notice that its stack is invalid by comparing some epoch counters, so it will install a stack watermark to track the state of its stack scan. The stack watermark makes it possible to distinguish whether a given frame is above the watermark (assuming that stacks grow downward) and hence must not be used by a Java thread since it may contain stale object references. In all operations that either pop a frame or walk below the last frame of the stack (e.g., stack walkers, returns, and exceptions), hooks will compare some stack-local address to the watermark. (This stack-local address may be a frame pointer, where available, or a stack pointer for compiled frames where the frame pointer is optimized away but frames have a reasonably constant size.) When above the watermark, a slow path will be taken to fix up one frame by updating the object references within it and moving the watermark upward. In order to make returns as fast as they are today, the stack watermark barrier will use a slightly modified safepoint poll. The new poll not only takes a slow path when safepoints (or indeed thread-local handshakes) are pending, but also when returning to a frame that has not yet been fixed up. This can be encoded for compiled methods with a single conditional branch. An invariant of the stack watermark is that, given a callee which is the last frame of the stack, both the callee and the caller are processed. To ensure this, when the stack watermark state is installed when waking up from safepoints, both the caller and the callee are processed. The callee is armed so that returns from that callee will trigger further processing of the caller, moving the armed frame to the caller, and so on. Hence processing triggered by frame unwinding or walking always occurs two frames above the frame being unwound or walked. This simplifies the passing of arguments that have to be owned by the caller yet are used by the callee; both the caller and the callee frames (and hence the extra stack arguments) can be accessed freely. Java threads will process the minimum number of frames needed to continue execution. Concurrent GC threads will take care of the remaining frames, ensuring that all thread stacks and other thread roots are eventually processed. Synchronization, utilizing the stack watermark barrier, will ensure that Java threads do not return into a frame while the GC is processing it.",
    "specification": ""
  },
  {
    "number": "JEP 341",
    "title": "Default CDS Archives",
    "url": "https://openjdk.org/jeps/341",
    "summary": "Enhance the JDK build process to generate a class data-sharing (CDS) archive, using the default class list, on 64-bit platforms.",
    "goals": "Improve out-of-the-box startup time Eliminate the need for users to run -Xshare:dump to benefit from CDS",
    "motivation": "Numerous enhancements have been added to the base CDS feature since JDK 8u40. The startup time and memory sharing benefits provided by enabling CDS have increased significantly. Measurements done on Linux/x64 using JDK 11 early-access build 14 show a 32% startup time reduction running HelloWorld. On other 64-bit platforms, similar or higher startup performance gains have been observed. Currently, a JDK image includes a default class list, generated at build time, in the lib directory. Users who want to take advantage of CDS, even with just the default class list provided in the JDK, must run java -Xshare:dump as an extra step. This option is documented, but many users are unaware of it.",
    "description": "Modify the JDK build to run java -Xshare:dump after linking the image. (Additional command-line options may be included to fine-tune GC heap size, etc., in order to obtain better memory layout for common cases.) Leave the resulting CDS archive in the lib/server directory, so that it is part of the resulting image. Users will benefit from the CDS feature automatically, since -Xshare:auto was enabled by default for the server VM in JDK 11 (JDK-8197967). To disable CDS, run with -Xshare:off. Users with more advanced requirements (e.g., using custom class lists that include application classes, different GC configurations, etc.) can still create a custom CDS archive as before.",
    "specification": ""
  },
  {
    "number": "JEP 350",
    "title": "Dynamic CDS Archives",
    "url": "https://openjdk.org/jeps/350",
    "summary": "Extend application class-data sharing to allow the dynamic archiving of classes at the end of Java application execution. The archived classes will include all loaded application classes and library classes that are not present in the default, base-layer CDS archive.",
    "goals": "Improve the usability of application class-data sharing (AppCDS). Eliminate the need for users to do trial runs to create a class list for each application. Static archiving enabled by the -Xshare:dump option, using a class list, should continue work. That includes classes for both built-in class loaders and user-defined class loaders.",
    "motivation": "Archiving application classes using AppCDS in HotSpot provides additional startup time and memory benefits relative to the default CDS archive. However, currently a three-step procedure is required in order to use AppCDS for a Java application: Do one or more trial runs to create a class list Dump an archive using the created class list Run with the archive This procedure, moreover, works only for applications that use only builtin class loaders. There is experimental support for archiving classes loaded by user-defined class loaders in HotSpot, but it is not easy to use. Dynamic archiving enabled by a command-line option will simplify AppCDS usage by eliminating trial runs (step 1 above), and will support both builtin class loaders and user-defined class loaders effectively and uniformly. A follow-up enhancement to this JEP could perform automatic archive generation during the first run of an application. This would eliminate the explicit archive creation step (step 2 above). The usage of CDS/AppCDS could then be completely transparent and automatic.",
    "description": "Supported archive configurations The following configurations will be supported at run time: Static base archive (the default CDS archive) + dynamic archive &#8212; when both archives are mapped successfully Static base archive only &#8212; when the dynamic archive cannot be mapped The dynamic archive currently requires the default CDS archive to be used as the base archive. If the base-layer archive cannot be mapped and used at run time then the top-layer dynamic archive is automatically disabled. Archiving classes at exit A shared archive is dynamically created when an application exits if the -XX:ArchiveClassesAtExit option is specified. The dynamically-generated archive is created on top of the default system archive packaged with the running JDK image. A separate top-layer archive file is generated for each application. The user can specify the filename of the dynamic archive name as the argument to the -XX:ArchiveClassesAtExit option. For example, the following command creates hello.jsa: % bin/java -XX:ArchiveClassesAtExit=hello.jsa -cp hello.jar Hello To run the same application using this dynamic archive: % bin/java -XX:SharedArchiveFile=hello.jsa -cp hello.jar Hello Base-layer dependency The dynamically created top-layer archive depends upon the base-layer archive (i.e., it contains pointers to base-layer data), so the CRC values of the base archive header and all shared spaces are recorded in the top layer. At run time, when the dynamic archive is mapped in, all recorded CRC values are compared with the currently mapped base archive CRC values. If any of the CRC values do not match then the dynamic archive is disabled, without affecting the usage of the currently-mapped base archive. Using CRC values to check the base-archive dependency is more robust than using file name, size, and timestamp checks. Copying and relocating class metadata All loaded application classes and library classes (excluding the ones included in the base layer) are dynamically archived in the top layer. Currently, copying and relocating class metadata is done at the end of the application execution and before VM exit. Archived data is cleaned up in order to remove any non-shareable information. For a user-defined class loader the JVM needs to do extra copying before the class loader and its loaded classes are unloaded. The buffered data is copied into shared spaces with the rest of the dynamically-archived class metadata. Shared Spaces in dynamic archives The layout of the dynamic archive is similar to the existing static archive. Metadata is separated into the following four spaces. No &#8220;md&#8221; space needed. rw: Readable/writable data ro: Read-only data mc: Trampoline The shared spaces are mapped individually at runtime. The &#8220;ro&#8221; space is mapped read-only to enable cross-process sharing. Memory Savings When you have multiple related processes running on the same host, you can improve memory sharing by using two levels of archives. For example, when you have these programs that share the same set of libraries: 2 processes running with \"-cp:lib.jar:foo.jar FooApp\" 4 processes running with \"-cp:lib.jar:bar.jar BarApp\" you can create a static archive (using a classlist) for only the classes in lib.jar (and other system classes used by these apps). Then, create two different dynamic archives, one for the \"foo\" app, and the other for the \"bar\" app. This way, the static archive can be shared across all 6 processes, and the dynamic archives can be shared among the processes running the same program.",
    "specification": ""
  },
  {
    "number": "JEP 387",
    "title": "Elastic Metaspace",
    "url": "https://openjdk.org/jeps/387",
    "summary": "Return unused HotSpot class-metadata (i.e., metaspace) memory to the operating system more promptly, reduce metaspace footprint, and simplify the metaspace code in order to reduce maintenance costs.",
    "goals": "",
    "motivation": "Since its inception in JEP 122, metaspace has been somewhat notorious for high off-heap memory usage. Most normal applications don't have problems, but it is easy to tickle the metaspace allocator in just the wrong way to cause excessive memory waste. Unfortunately these types of pathological cases are not uncommon. Metaspace memory is managed in per-class-loader arenas. An arena contains one or more chunks, from which its loader allocates via inexpensive pointer bumps. Metaspace chunks are coarse-grained, in order to keep allocation operations efficient. This can, however, cause applications that use many small class loaders to suffer unreasonably high metaspace usage. When a class loader is reclaimed, the chunks in its metaspace arena are placed on freelists for later reuse. That reuse may not happen for a long time, however, or it may never happen. Applications with heavy class loading and unloading activity can thus accrue a lot of unused space in the metaspace freelists. That space can be returned to the operating system to be used for other purposes if it is not fragmented, but that&#8217;s often not the case.",
    "description": "We propose to replace the existing metaspace memory allocator with a buddy-based allocation scheme. This is an old and proven algorithm which has been used successfully in, e.g., the Linux kernel. This scheme will make it practical to allocate metaspace memory in smaller chunks, which will reduce class-loader overhead. It will also reduce fragmentation, which will allow us to improve elasticity by returning unused metaspace memory to the operating system. We will also commit memory from the operating system to arenas lazily, on demand. This will reduce footprint for loaders that start out with large arenas but do not use them immediately or might never use them to their full extent, e.g., the boot class loader. Finally, to fully exploit the elasticity offered by buddy allocation we will arrange metaspace memory into uniformly-sized granules which can be committed and uncommitted independently of each other. The size of these granules can be controlled by a new command-line option, which provides a simple way to control virtual-memory fragmentation. A document describing the new algorithm in detail can be found here. A working prototype exists as a branch in the JDK sandbox repository.",
    "specification": ""
  },
  {
    "number": "JEP 358",
    "title": "Helpful NullPointerExceptions",
    "url": "https://openjdk.org/jeps/358",
    "summary": "Improve the usability of NullPointerExceptions generated by the JVM by describing precisely which variable was null.",
    "goals": "Offer helpful information to developers and support staff about the premature termination of a program. Improve program understanding by more clearly associating a dynamic exception with static program code. Reduce the confusion and concern that new developers often have about NullPointerExceptions.",
    "motivation": "Every Java developer has encountered NullPointerExceptions (NPEs). Since NPEs can occur almost anywhere in a program, it is generally impractical to attempt to catch and recover from them. As a result, developers rely on the JVM to pinpoint the source of an NPE when it actually occurs. For example, suppose an NPE occurs in this code: a.i = 99; The JVM will print out the method, filename, and line number that caused the NPE: Exception in thread \"main\" java.lang.NullPointerException at Prog.main(Prog.java:5) Using the message, which is typically included in a bug report, the developer can locate a.i = 99; and infer that a must have been null. However, for more complex code, it is impossible to decide which variable was null without using a debugger. Suppose an NPE occurs in this code: a.b.c.i = 99; The filename and line number do not pinpoint exactly which variable was null. Was it a or b or c? A similar problem occurs with array access and assignment. Suppose an NPE occurs in this code: a[i][j][k] = 99; The filename and line number do not pinpoint exactly which array component was null. Was it a or a[i] or a[i][j]? A single line of code may contain several access paths, each one potentially the source of an NPE. Suppose an NPE occurs in this code: a.i = b.j; The filename and line number do not pinpoint the offending access path. Was a null, or b? Finally, an NPE could stem from a method call. Suppose an NPE occurs in this code: x().y().i = 99; The filename and line number do not pinpoint which method call returned null. Was it x() or y()? Various strategies can mitigate the lack of accurate pinpointing by the JVM. For example, a developer faced with an NPE can break up the access paths by assigning to intermediate local variables. (The var keyword may be helpful here.) The result will be a more accurate report of the null variable in the JVM's message, but reformatting code to track down an exception is undesirable. In any case, most NPEs occur in production environments, where the support engineer who observes the NPE is many steps removed from the developer whose code caused it. The entire Java ecosystem would benefit if the JVM could give the information needed to pinpoint the source of an NPE and then identify its root cause, without using extra tooling or shuffling code around. SAP's commercial JVM has done this since 2006, to great acclaim from developers and support engineers.",
    "description": "The JVM throws a NullPointerException (NPE) at the point in a program where code tries to dereference a null reference. By analyzing the program's bytecode instructions, the JVM will determine precisely which variable was null, and describe the variable (in terms of source code) with a null-detail message in the NPE. The null-detail message will then be shown in the JVM's message, alongside the method, filename, and line number. Note: The JVM displays an exception message on the same line as the exception type, which can result in long lines. For readability in a web browser, this JEP shows the null-detail message on a second line, after the exception type. For example, an NPE from the assignment statement a.i = 99; would generate this message: Exception in thread \"main\" java.lang.NullPointerException: Cannot assign field \"i\" because \"a\" is null at Prog.main(Prog.java:5) If the more complex statement a.b.c.i = 99; throws an NPE, the message would dissect the statement and pinpoint the cause by showing the full access path which led up to the null: Exception in thread \"main\" java.lang.NullPointerException: Cannot read field \"c\" because \"a.b\" is null at Prog.main(Prog.java:5) Giving the full access path is more helpful than giving just the name of the null field because it helps the developer to navigate a line of complex source code, especially if the line of code uses the same name multiple times. Similarly if the array access and assignment statement a[i][j][k] = 99; throws an NPE: Exception in thread \"main\" java.lang.NullPointerException: Cannot load from object array because \"a[i][j]\" is null at Prog.main(Prog.java:5) Similarly if a.i = b.j; throws an NPE: Exception in thread \"main\" java.lang.NullPointerException: Cannot read field \"j\" because \"b\" is null at Prog.main(Prog.java:5) In every example, the null-detail message in conjunction with the line number is sufficient to spot the expression that is null in the source code. Ideally, the null-detail message would show the actual source code, but this is difficult to do given the nature of the correspondence between source code and bytecode instructions (see below). In addition, when the expression involves an array access, the null-detail message is unable to show the actual array indices which led to a null element, such as the run-time values of i and j when a[i][j] is null. This is because the array indices were stored on the method's operand stack, which was lost when the NPE was thrown. Only NPEs that are created and thrown directly by the JVM will include the null-detail message. NPEs that are explicitly created and/or explicitly thrown by programs running on the JVM are not subject to the bytecode analysis and null-detail message creation described below. In addition, the null-detail message is not reported for NPEs caused by code in hidden methods, which are special-purpose low-level methods generated and called by the JVM to, e.g., optimize string concatenation. A hidden method has no filename or line number that could help to pinpoint the source of an NPE, so printing a null-detail message would be futile. Computing the null-detail message Source code such as a.b.c.i = 99; is compiled to several bytecode instructions. When an NPE is thrown, the JVM knows exactly which bytecode instruction in which method is responsible, and uses this information to compute the null-detail message. The message has two parts: The first part -- Cannot read field \"c\" -- is the consequence of the NPE. It says which action could not be performed because a bytecode instruction popped a null reference from the operand stack. The second part -- because \"a.b\" is null -- is the reason for the NPE. It recreates the part of the source code that pushed the null reference on to the operand stack. The first part of the null-detail message is computed from the bytecode instruction that popped null, as detailed here in Table 1: bytecode 1st part aload \"Cannot load from &lt;element type&gt; array\" arraylength \"Cannot read the array length\" astore \"Cannot store to &lt;element type&gt; array\" athrow \"Cannot throw exception\" getfield \"Cannot read field \"&lt;field name&gt;\"\" invokeinterface, invokespecial, invokevirtual \"Cannot invoke \"&lt;method&gt;\"\" monitorenter \"Cannot enter synchronized block\" monitorexit \"Cannot exit synchronized block\" putfield \"Cannot assign field \"&lt;field name&gt;\"\" Any other bytecode No NPE possible, no message &lt;method&gt; breaks down to &lt;class name&gt;.&lt;method name&gt;(&lt;parameter types&gt;) The second part of the null-detail message is more complex. It identifies the access path that led to a null reference on the operand stack, but complex access paths involve several bytecode instructions. Given a sequence of instructions in a method, it is not obvious which previous instruction pushed the null reference. Accordingly, a simple data flow analysis is performed on all the method's instructions. It computes which instruction pushes to which operand stack slot, and propagates this information to the instruction which pops the slot. (The analysis is linear in the number of instructions.) Given the analysis, it is possible to step back through the instructions which make up an access path in source code. The second part of the message is assembled step-by-step, given the bytecode instruction at each step as detailed here in Table 2: bytecode2nd part aconst_null\"null\" aaloadcompute the 2nd part for the instruction which pushed the array reference, then append \"[\", then compute the 2nd part for the instruction that pushed the index, then append \"]\" iconst_*, bipush, sipushthe constant value getfieldcompute the 2nd part for the instruction which pushed the reference that is accessed by this getfield, then append \".&lt;field name&gt;\" getstatic\"&lt;class name&gt;.&lt;field name&gt;\" invokeinterface, invokevirtual, invokespecial, invokestatic If in the first step, \"the return value of &lt;method&gt;\", else \"&lt;method&gt;\" iload*, aload*For local variable 0, \"this\". For other local variables and parameters, the variable name if a local variable table is available, otherwise \"&lt;parameter i &gt;\" or \"&lt;local i &gt;\". Any other bytecodeNot applicable to the second part. Access paths can be made up of an arbitrary number of bytecode instructions. The null-detail message does not necessarily cover all of these. The algorithm takes only a limited number of steps back through the instructions in order to limit the complexity of the output. If the maximum number of steps is reached, placeholders such as \"...\" are emitted. In rare cases, stepping back over instructions is not possible, and then the null-detail message will contain only the first part (\"Cannot ...\", with no \"because ...\" explanation). The null-detail message -- Cannot read field \"c\" because \"a.b\" is null -- is computed on demand, when the JVM calls Throwable::getMessage as part of its message. Usually, a message carried by an exception must be supplied when the exception object is created, but the computation is expensive and may not always be needed, since many NPEs are caught and discarded by programs. The computation requires the bytecode instructions of the method which caused the NPE, and the index of the instruction which popped null; fortunately, the implementation of Throwable includes this information about the origin of the exception. The feature can be toggled with the new boolean command-line option -XX:{+|-}ShowCodeDetailsInExceptionMessages. The option will first have default 'false' so that the message is not printed. It is intended to enable code details in exception messages by default in a later release. Example of computing the null-detail message Here is an example based on the following snippet of source code: a().b[i][j] = 99; The source code has the following representation in bytecode: 5: invokestatic #7 // Method a:()LA; 8: getfield #13 // Field A.b, an array 11: iload_1 // Load local variable i, an array index 12: aaload // Load b[i], another array 13: iload_2 // Load local variable j, another array index 14: bipush 99 16: iastore // Store to b[i][j] Suppose a().b[i] is null. This will cause an NPE to be thrown when storing to b[i][j]. The JVM will execute bytecode 16: iastore and throw an NPE because bytecode 12: aaload pushed null on to the operand stack. The null-detail message will be computed as follows: Cannot store to int array because \"Test.a().b[i]\" is null The computation starts with the method containing the bytecode instructions, and the bytecode index 16. Since the instruction at index 16 is iastore, the first part of the message is \"Cannot store to int array\", per Table 1. For the second part of the message, the algorithm steps back to the instruction that pushed the null which iastore was unfortunate enough to pop. Data flow analysis reveals this is 12: aaload, an array load. Per Table 2, when an array load is responsible for a null array reference, we step back to the instruction which pushed the array reference (rather than the array index) on to the operand stack, 8: getfield. Then again per Table 2, when a getfield is part of the access path, we step back to the instruction that pushed the reference used by getfield, 5: invokestatic. We can now assemble the second part of the message: For 5: invokestatic, emit \"Test.a()\" For 8: getfield, emit \".b\" For 12: aaload, emit \"[\" and stepback to the instruction that pushed the index, 11: iload_1. Emit \"i\", the name of local variable #1, then \"]\". The algorithm never steps to 13: iload_2 which pushes the index j, or to 14: bipush which pushes 99, because they are not related to the cause of the NPE. Files with many examples of null-detail messages are attached to this JEP: output_with_debug_info.txt lists messages when class files contain a local variable table. and output_no_debug_info.txt messages when class files do not contain a local variable table.",
    "specification": ""
  },
  {
    "number": "JEP 394",
    "title": "Pattern Matching for instanceof",
    "url": "https://openjdk.org/jeps/394",
    "summary": "Enhance the Java programming language with pattern matching for the instanceof operator. Pattern matching allows common logic in a program, namely the conditional extraction of components from objects, to be expressed more concisely and safely.",
    "goals": "",
    "motivation": "Nearly every program includes some sort of logic that combines testing if an expression has a certain type or structure, and then conditionally extracting components of its state for further processing. For example, all Java programmers are familiar with the instanceof-and-cast idiom: if (obj instanceof String) { String s = (String) obj; // grr... ... } There are three things going on here: a test (is obj a String?), a conversion (casting obj to String), and the declaration of a new local variable (s) so that we can use the string value. This pattern is straightforward and understood by all Java programmers, but is suboptimal for several reasons. It is tedious; doing both the type test and cast should be unnecessary (what else would you do after an instanceof test?). This boilerplate &#8212; in particular, the three occurrences of the type String &#8212; obfuscates the more significant logic that follows. But most importantly, the repetition provides opportunities for errors to creep unnoticed into programs. Rather than reach for ad-hoc solutions, we believe it is time for Java to embrace pattern matching. Pattern matching allows the desired \"shape\" of an object to be expressed concisely (the pattern), and for various statements and expressions to test that \"shape\" against their input (the matching). Many languages, from Haskell to C#, have embraced pattern matching for its brevity and safety.",
    "description": "A pattern is a combination of (1) a predicate, or test, that can be applied to a target, and (2) a set of local variables, known as pattern variables, that are extracted from the target only if the predicate successfully applies to it. A type pattern consists of a predicate that specifies a type, along with a single pattern variable. The instanceof operator (JLS 15.20.2) is extended to take a type pattern instead of just a type. This allows us to refactor the tedious code above to the following: if (obj instanceof String s) { // Let pattern matching do the work! ... } (In this code, the phrase String s is the type pattern.) The meaning is intuitive. The instanceof operator matches the target obj to the type pattern as follows: If obj is an instance of String, then it is cast to String and the value is assigned to the variable s. The conditionality of pattern matching &#8212; if a value does not match a pattern, then the pattern variable is not assigned a value &#8212; means that we have to consider carefully the scope of the pattern variable. We could do something simple and say that the scope of the pattern variable is the containing statement and all subsequent statements in the enclosing block. But this has unfortunate poisoning consequences, for example: if (a instanceof Point p) { ... } if (b instanceof Point p) { // ERROR - p is in scope ... } In other words, by the second statement the pattern variable p would be in a poisoned state &#8212; it is in scope, but it should not be accessible since it may not be assigned a value. But even though it shouldn't be accessed, since it is in scope, we can't just declare it again. This means that a pattern variable can become poisoned after it is declared, so programmers would have to think of lots of distinct names for their pattern variables. Rather than using a coarse approximation for the scope of pattern variables, pattern variables instead use the concept of flow scoping. A pattern variable is only in scope where the compiler can deduce that the pattern has definitely matched and the variable will have been assigned a value. This analysis is flow sensitive and works in a similar way to existing flow analyses such as definite assignment. Returning to our example: if (a instanceof Point p) { // p is in scope ... } // p not in scope here if (b instanceof Point p) { // Sure! ... } The motto is: \"A pattern variable is in scope where it has definitely matched\". This allows for the safe reuse of pattern variables and is both intuitive and familiar, since Java developers are already used to flow sensitive analyses. When the conditional expression of the if statement grows more complicated than a single instanceof, the scope of the pattern variable grows accordingly. For example, in this code: if (obj instanceof String s &amp;&amp; s.length() &gt; 5) { flag = s.contains(\"jdk\"); } the pattern variable s is in scope on the right hand side of the &amp;&amp; operator, as well as in the true block. (The right hand side of the &amp;&amp; operator is only evaluated if the pattern match succeeded and assigned a value to s.) On the other hand, the following code does not compile: if (obj instanceof String s || s.length() &gt; 5) { // Error! ... } Because of the semantics of the || operator, the pattern variable s might not have been assigned and so the flow analysis dictates that the variable s is not in scope on the right hand side of the || operator. The use of pattern matching in instanceof should significantly reduce the overall number of explicit casts in Java programs. Type test patterns are particularly useful when writing equality methods. Consider the following equality method taken from Item 10 of Effective Java: public final boolean equals(Object o) { return (o instanceof CaseInsensitiveString) &amp;&amp; ((CaseInsensitiveString) o).s.equalsIgnoreCase(s); } Using a type pattern means it can be rewritten to the clearer: public final boolean equals(Object o) { return (o instanceof CaseInsensitiveString cis) &amp;&amp; cis.s.equalsIgnoreCase(s); } Other equals methods are even more dramatically improved. Consider the class Point from above, where we might write an equals method as follows: public final boolean equals(Object o) { if (!(o instanceof Point)) return false; Point other = (Point) o; return x == other.x &amp;&amp; y == other.y; } Using pattern matching instead, we can combine these multiple statements into a single expression, eliminating the repetition and simplifying the control flow: public final boolean equals(Object o) { return (o instanceof Point other) &amp;&amp; x == other.x &amp;&amp; y == other.y; } The flow scoping analysis for pattern variables is sensitive to the notion of whether a statement can complete normally. For example, consider the following method: public void onlyForStrings(Object o) throws MyException { if (!(o instanceof String s)) throw new MyException(); // s is in scope System.out.println(s); ... } This method tests whether its parameter o is a String, and throws an exception if not. It is only possible to reach the println statement if the conditional statement has completed normally. Because the contained statement of the conditional statement can never complete normally, this can only occur if the conditional expression has evaluated to the value false, which, in turn, means that the pattern matching has succeeded. Accordingly, the scope of the pattern variable s safely includes the statements following the conditional statement in the method block. Pattern variables are just a special case of local variables, and aside from the definition of their scope, in all other respects pattern variables are treated as local variables. In particular, this means that (1) they can be assigned to, and (2) they can shadow a field declaration. For example: class Example1 { String s; void test1(Object o) { if (o instanceof String s) { System.out.println(s); // Field s is shadowed s = s + \"\\n\"; // Assignment to pattern variable ... } System.out.println(s); // Refers to field s ... } } However, the flow scoping nature of pattern variables means that some care must be taken to determine whether a name refers to a pattern variable declaration shadowing a field declaration or to the field declaration itself. class Example2 { Point p; void test2(Object o) { if (o instanceof Point p) { // p refers to the pattern variable ... } else { // p refers to the field ... } } } The instanceof grammar is extended accordingly: RelationalExpression: &#160;&#160;&#160;&#160; ... &#160;&#160;&#160;&#160; RelationalExpression instanceof ReferenceType &#160;&#160;&#160;&#160; RelationalExpression instanceof Pattern Pattern: &#160;&#160;&#160;&#160; ReferenceType Identifier Future Work Future JEPs will enhance the Java programming language with richer forms of patterns, such as deconstruction patterns for record classes, and pattern matching for other language constructs, such as switch expressions and statements.",
    "specification": ""
  },
  {
    "number": "JEP 395",
    "title": "Records",
    "url": "https://openjdk.org/jeps/395",
    "summary": "Enhance the Java programming language with records, which are classes that act as transparent carriers for immutable data. Records can be thought of as nominal tuples.",
    "goals": "Devise an object-oriented construct that expresses a simple aggregation of values. Help developers to focus on modeling immutable data rather than extensible behavior. Automatically implement data-driven methods such as equals and accessors. Preserve long-standing Java principles such as nominal typing and migration compatibility.",
    "motivation": "It is a common complaint that \"Java is too verbose\" or has \"too much ceremony\". Some of the worst offenders are classes that are nothing more than immutable data carriers for a handful of values. Properly writing such a data-carrier class involves a lot of low-value, repetitive, error-prone code: constructors, accessors, equals, hashCode, toString, etc. For example, a class to carry x and y coordinates inevitably ends up like this: class Point { private final int x; private final int y; Point(int x, int y) { this.x = x; this.y = y; } int x() { return x; } int y() { return y; } public boolean equals(Object o) { if (!(o instanceof Point)) return false; Point other = (Point) o; return other.x == x &amp;&amp; other.y == y; } public int hashCode() { return Objects.hash(x, y); } public String toString() { return String.format(\"Point[x=%d, y=%d]\", x, y); } } Developers are sometimes tempted to cut corners by omitting methods such as equals, leading to surprising behavior or poor debuggability, or by pressing an alternate but not entirely appropriate class into service because it has the \"right shape\" and they don't want to declare yet another class. IDEs help us to write most of the code in a data-carrier class, but don't do anything to help the reader distill the design intent of \"I'm a data carrier for x and y\" from the dozens of lines of boilerplate. Writing Java code that models a handful of values should be easier to write, to read, and to verify as correct. While it is superficially tempting to treat records as primarily being about boilerplate reduction, we instead choose a more semantic goal: modeling data as data. (If the semantics are right, the boilerplate will take care of itself.) It should be easy and concise to declare data-carrier classes that by default make their data immutable and provide idiomatic implementations of methods that produce and consume the data.",
    "description": "Record classes are a new kind of class in the Java language. Record classes help to model plain data aggregates with less ceremony than normal classes. The declaration of a record class primarily consists of a declaration of its state; the record class then commits to an API that matches that state. This means that record classes give up a freedom that classes usually enjoy &#8212; the ability to decouple a class's API from its internal representation &#8212; but, in return, record class declarations become significantly more concise. More precisely, a record class declaration consists of a name, optional type parameters, a header, and a body. The header lists the components of the record class, which are the variables that make up its state. (This list of components is sometimes referred to as the state description.) For example: record Point(int x, int y) { } Because record classes make the semantic claim of being transparent carriers for their data, a record class acquires many standard members automatically: For each component in the header, two members: a public accessor method with the same name and return type as the component, and a private final field with the same type as the component; A canonical constructor whose signature is the same as the header, and which assigns each private field to the corresponding argument from a new expression which instantiates the record; equals and hashCode methods which ensure that two record values are equal if they are of the same type and contain equal component values; and A toString method that returns a string representation of all the record components, along with their names. In other words, the header of a record class describes its state, i.e., the types and names of its components, and the API is derived mechanically and completely from that state description. The API includes protocols for construction, member access, equality, and display. (We expect a future version to support deconstruction patterns to allow powerful pattern matching.) Constructors for record classes The rules for constructors in a record class are different than in a normal class. A normal class without any constructor declarations is automatically given a default constructor. In contrast, a record class without any constructor declarations is automatically given a canonical constructor that assigns all the private fields to the corresponding arguments of the new expression which instantiated the record. For example, the record declared earlier &#8212; record Point(int x, int y) { } &#8212; is compiled as if it were: record Point(int x, int y) { // Implicitly declared fields private final int x; private final int y; // Other implicit declarations elided ... // Implicitly declared canonical constructor Point(int x, int y) { this.x = x; this.y = y; } } The canonical constructor may be declared explicitly with a list of formal parameters which match the record header, as shown above. It may also be declared more compactly, by eliding the list of formal parameters. In such a compact canonical constructor the parameters are declared implicitly, and the private fields corresponding to record components cannot be assigned in the body but are automatically assigned to the corresponding formal parameter (this.x = x;) at the end of the constructor. The compact form helps developers focus on validating and normalizing parameters without the tedious work of assigning parameters to fields. For example, here is a compact canonical constructor that validates its implicit formal parameters: record Range(int lo, int hi) { Range { if (lo &gt; hi) // referring here to the implicit constructor parameters throw new IllegalArgumentException(String.format(\"(%d,%d)\", lo, hi)); } } Here is a compact canonical constructor that normalizes its formal parameters: record Rational(int num, int denom) { Rational { int gcd = gcd(num, denom); num /= gcd; denom /= gcd; } } This declaration is equivalent to the conventional constructor form: record Rational(int num, int denom) { Rational(int num, int demon) { // Normalization int gcd = gcd(num, denom); num /= gcd; denom /= gcd; // Initialization this.num = num; this.denom = denom; } } Record classes with implicitly declared constructors and methods satisfy important, and intuitive, semantic properties. For example, consider a record class R declared as follows: record R(T1 c1, ..., Tn cn){ } If an instance r1 of R is copied in the following way: R r2 = new R(r1.c1(), r1.c2(), ..., r1.cn()); then, assuming r1 is not the null reference, it is always the case that the expression r1.equals(r2) will evaluate to true. Explicitly declared accessor and equals methods should respect this invariant. However, it is not generally possible for a compiler to check that explicitly declared methods respect this invariant. As an example, the following declaration of a record class should be considered bad style because its accessor methods \"silently\" adjust the state of a record instance, and the invariant above is not satisfied: record SmallPoint(int x, int y) { public int x() { return this.x &lt; 100 ? this.x : 100; } public int y() { return this.y &lt; 100 ? this.y : 100; } } In addition, for all record classes the implicitly declared equals method is implemented so that it is reflexive and that it behaves consistently with hashCode for record classes that have floating point components. Again, explicitly declared equals and hashCode methods should behave similarly. Rules for record classes There are numerous restrictions on the declaration of a record class in comparison to a normal class: A record class declaration does not have an extends clause. The superclass of a record class is always java.lang.Record, similar to how the superclass of an enum class is always java.lang.Enum. Even though a normal class can explicitly extend its implicit superclass Object, a record cannot explicitly extend any class, even its implicit superclass Record. A record class is implicitly final, and cannot be abstract. These restrictions emphasize that the API of a record class is defined solely by its state description, and cannot be enhanced later by another class. The fields derived from the record components are final. This restriction embodies an immutable by default policy that is widely applicable for data-carrier classes. A record class cannot explicitly declare instance fields, and cannot contain instance initializers. These restrictions ensure that the record header alone defines the state of a record value. Any explicit declarations of a member that would otherwise be automatically derived must match the type of the automatically derived member exactly, disregarding any annotations on the explicit declaration. Any explicit implementation of accessors or the equals or hashCode methods should be careful to preserve the semantic invariants of the record class. A record class cannot declare native methods. If a record class could declare a native method then the behavior of the record class would by definition depend on external state rather than the record class's explicit state. No class with native methods is likely to be a good candidate for migration to a record. Beyond the restrictions above, a record class behaves like a normal class: Instances of record classes are created using a new expression. A record class can be declared top level or nested, and can be generic. A record class can declare static methods, fields, and initializers. A record class can declare instance methods. A record class can implement interfaces. A record class cannot specify a superclass since that would mean inherited state, beyond the state described in the header. A record class can, however, freely specify superinterfaces and declare instance methods to implement them. Just as for classes, an interface can usefully characterize the behavior of many records. The behavior may be domain-independent (e.g., Comparable) or domain-specific, in which case records can be part of a sealed hierarchy which captures the domain (see below). A record class can declare nested types, including nested record classes. If a record class is itself nested, then it is implicitly static; this avoids an immediately enclosing instance which would silently add state to the record class. A record class, and the components in its header, may be decorated with annotations. Any annotations on the record components are propagated to the automatically derived fields, methods, and constructor parameters, according to the set of applicable targets for the annotation. Type annotations on the types of record components are also propagated to the corresponding type uses in the automatically derived members. Instances of record classes can be serialized and deserialized. However, the process cannot be customized by providing writeObject, readObject, readObjectNoData, writeExternal, or readExternal methods. The components of a record class govern serialization, while the canonical constructor of a record class governs deserialization. Local record classes A program that produces and consumes instances of a record class is likely to deal with many intermediate values that are themselves simple groups of variables. It will often be convenient to declare record classes to model those intermediate values. One option is to declare \"helper\" record classes that are static and nested, much as many programs declare helper classes today. A more convenient option would be to declare a record inside a method, close to the code which manipulates the variables. Accordingly we define local record classes, akin to the existing construct of local classes. In the following example, the aggregation of a merchant and a monthly sales figure is modeled with a local record class, MerchantSales. Using this record class improves the readability of the stream operations which follow: List&lt;Merchant&gt; findTopMerchants(List&lt;Merchant&gt; merchants, int month) { // Local record record MerchantSales(Merchant merchant, double sales) {} return merchants.stream() .map(merchant -&gt; new MerchantSales(merchant, computeSales(merchant, month))) .sorted((m1, m2) -&gt; Double.compare(m2.sales(), m1.sales())) .map(MerchantSales::merchant) .collect(toList()); } Local record classes are a particular case of nested record classes. Like nested record classes, local record classes are implicitly static. This means that their own methods cannot access any variables of the enclosing method; in turn, this avoids capturing an immediately enclosing instance which would silently add state to the record class. The fact that local record classes are implicitly static is in contrast to local classes, which are not implicitly static. In fact, local classes are never static &#8212; implicitly or explicitly &#8212; and can always access variables in the enclosing method. Local enum classes and local interfaces The addition of local record classes is an opportunity to add other kinds of implicitly-static local declarations. Nested enum classes and nested interfaces are already implicitly static, so for consistency we define local enum classes and local interfaces, which are also implicitly static. Static members of inner classes It is currently specified to be a compile-time error if an inner class declares a member that is explicitly or implicitly static, unless the member is a constant variable. This means that, for example, an inner class cannot declare a record class member, since nested record classes are implicitly static. We relax this restriction in order to allow an inner class to declare members that are either explicitly or implicitly static. In particular, this allows an inner class to declare a static member that is a record class. Annotations on record components Record components have multiple roles in record declarations. A record component is a first-class concept, but each component also corresponds to a field of the same name and type, an accessor method of the same name and return type, and a formal parameter of the canonical constructor of the same name and type. This raises the question: When a component is annotated, what actually is being annotated? The answer is, \"all of the elements to which this particular annotation is applicable.\" This enables classes that use annotations on their fields, constructor parameters, or accessor methods to be migrated to records without having to redundantly declare these members. For example, a class such as the following public final class Card { private final @MyAnno Rank rank; private final @MyAnno Suit suit; @MyAnno Rank rank() { return this.rank; } @MyAnno Suit suit() { return this.suit; } ... } can be migrated to the equivalent, and considerably more readable, record declaration: public record Card(@MyAnno Rank rank, @MyAnno Suit suit) { ... } The applicability of an annotation is declared using a @Target meta-annotation. Consider the following: @Target(ElementType.FIELD) public @interface I1 {...} This declares the annotation @I1 that it is applicable to field declarations. We can declare that an annotation is applicable to more than one declaration; for example: @Target({ElementType.FIELD, ElementType.METHOD}) public @interface I2 {...} This declares an annotation @I2 that it is applicable to both field declarations and method declarations. Returning to annotations on a record component, these annotations appear at the corresponding program points where they are applicable. In other words, the propagation is under the control of the developer using the @Target meta-annotation. The propagation rules are systematic and intuitive, and all that apply are followed: If an annotation on a record component is applicable to a field declaration, then the annotation appears on the corresponding private field. If an annotation on a record component is applicable to a method declaration, then the annotation appears on the corresponding accessor method. If an annotation on a record component is applicable to a formal parameter, then the annotation appears on the corresponding formal parameter of the canonical constructor if one is not declared explicitly, or else to the corresponding formal parameter of the compact constructor if one is declared explicitly. If an annotation on a record component is applicable to a type, the annotation will be propagated to all of the following: the type of the corresponding field the return type of the corresponding accessor method the type of the corresponding formal parameter of the canonical constructor the type of the record component (which is accessible at runtime via reflection) If a public accessor method or (non-compact) canonical constructor is declared explicitly, then it only has the annotations which appear on it directly; nothing is propagated from the corresponding record component to these members. A declaration annotation on a record component will not be amongst those associated with the record component at run time via the reflection API unless the annotation is meta-annotated with @Target(RECORD_COMPONENT). Compatibility and migration The abstract class java.lang.Record is the common superclass of all record classes. Every Java source file implicitly imports the java.lang.Record class, as well as all other types in the java.lang package, regardless of whether you enable or disable preview features. However, if your application imports another class named Record from a different package, you might get a compiler error. Consider the following class declaration of com.myapp.Record: package com.myapp; public class Record { public String greeting; public Record(String greeting) { this.greeting = greeting; } } The following example, org.example.MyappPackageExample, imports com.myapp.Record with a wildcard but doesn't compile: package org.example; import com.myapp.*; public class MyappPackageExample { public static void main(String[] args) { Record r = new Record(\"Hello world!\"); } } The compiler generates an error message similar to the following: ./org/example/MyappPackageExample.java:6: error: reference to Record is ambiguous Record r = new Record(\"Hello world!\"); ^ both class com.myapp.Record in com.myapp and class java.lang.Record in java.lang match ./org/example/MyappPackageExample.java:6: error: reference to Record is ambiguous Record r = new Record(\"Hello world!\"); ^ both class com.myapp.Record in com.myapp and class java.lang.Record in java.lang match Both Record in the com.myapp package and Record in the java.lang package are imported with wildcards. Consequently, neither class takes precedence, and the compiler generates an error message when it encounters the use of the simple name Record. To enable this example to compile, the import statement can be changed so that it imports the fully qualified name of Record: import com.myapp.Record; The introduction of classes in the java.lang package is rare but sometimes necessary. Previous examples are Enum in Java&#160;5, Module in Java&#160;9, and Record in Java&#160;14. Java grammar RecordDeclaration: {ClassModifier} `record` TypeIdentifier [TypeParameters] RecordHeader [SuperInterfaces] RecordBody RecordHeader: `(` [RecordComponentList] `)` RecordComponentList: RecordComponent { `,` RecordComponent} RecordComponent: {Annotation} UnannType Identifier VariableArityRecordComponent VariableArityRecordComponent: {Annotation} UnannType {Annotation} `...` Identifier RecordBody: `{` {RecordBodyDeclaration} `}` RecordBodyDeclaration: ClassBodyDeclaration CompactConstructorDeclaration CompactConstructorDeclaration: {ConstructorModifier} SimpleTypeName ConstructorBody Class-file representation The class file of a record uses a Record attribute to store information about the record's components: Record_attribute { u2 attribute_name_index; u4 attribute_length; u2 components_count; record_component_info components[components_count]; } record_component_info { u2 name_index; u2 descriptor_index; u2 attributes_count; attribute_info attributes[attributes_count]; } If the record component has a generic signature that is different from the erased descriptor then there must be a Signature attribute in the record_component_info structure. Reflection API We add two public methods to java.lang.Class: RecordComponent[] getRecordComponents() &#8212; Returns an array of java.lang.reflect.RecordComponent objects. The elements of this array correspond to the record's components, in the same order as they appear in the record declaration. Additional information can be extracted from each element of the array, including its name, annotations, and accessor method. boolean isRecord() &#8212; Returns true if the given class was declared as a record. (Compare with isEnum.)",
    "specification": ""
  },
  {
    "number": "JEP 306",
    "title": "Restore Always-Strict Floating-Point Semantics",
    "url": "https://openjdk.org/jeps/306",
    "summary": "Make floating-point operations consistently strict, rather than have both strict floating-point semantics (strictfp) and subtly different default floating-point semantics. This will restore the original floating-point semantics to the language and VM, matching the semantics before the introduction of strict and default floating-point modes in Java SE 1.2.",
    "goals": "Ease development of numerically-sensitive libraries, including java.lang.Math and java.lang.StrictMath. Provide more regularity in a tricky aspect of the platform.",
    "motivation": "The impetus for changing the default floating-point semantics of the platform in the late 1990's stemmed from a bad interaction between the original Java language and JVM semantics and some unfortunate peculiarities of the x87 floating-point co-processor instruction set of the popular x86 architecture. Matching the exact floating-point semantics in all cases, including subnormal operands and results, required large overheads of additional instructions. Matching the results in the absence of overflow or underflow could be achieved with much less overhead and that is roughly what is allowed by the revised default floating-point semantics introduced in Java SE 1.2. However, the SSE2 (Streaming SIMD Extensions 2) extensions, shipped in Pentium 4 and later processors starting circa 2001, could support strict JVM floating-point operations in a straightforward manner without undue overhead. Since Intel and AMD have both long supported SSE2 and later extensions which allow natural support for strict floating-point semantics, the technical motivation for having a default floating-point semantics different than strict is no longer present.",
    "description": "The interfaces this JEP would modify include the Java Language Specification in its coverage of floating-point expressions (see JLS sections 4.2.3 Floating-Point Types, Formats, and Values, 5.1.13 *Value Set Conversion, 15.4 FP-strict Expressions, many small updates to other sections later in chapter 15) and similar sections of the Java Virtual Machine Specification (JVMS 2.3.2 Floating-Point Types, Values Sets, and Values, section 2.8.2 Floating-Point Modes, 2.8.3 Value Set Conversion, and many small updates to individual floating-point instructions). The concepts of value sets and value set conversion would be removed from the JLS and JVMS. Implementation changes in the JDK would include updating the HotSpot virtual machine to never run in a floating-point mode which allowed the extended exponent value set (such a mode is mandated to be present for strictfp operations) and updating javac to issue new lint warnings to unnecessary use of the strictfp modifier.",
    "specification": ""
  },
  {
    "number": "JEP 409",
    "title": "Sealed Classes",
    "url": "https://openjdk.org/jeps/409",
    "summary": "Enhance the Java programming language with sealed classes and interfaces. Sealed classes and interfaces restrict which other classes or interfaces may extend or implement them.",
    "goals": "Allow the author of a class or interface to control which code is responsible for implementing it. Provide a more declarative way than access modifiers to restrict the use of a superclass. Support future directions in pattern matching by providing a foundation for the exhaustive analysis of patterns.",
    "motivation": "The object-oriented data model of inheritance hierarchies of classes and interfaces has proven to be highly effective in modeling the real-world data processed by modern applications. This expressiveness is an important aspect of the Java language. There are, however, cases where such expressiveness can usefully be tamed. For example, Java supports enum classes to model the situation where a given class has only a fixed number of instances. In the following code, an enum class lists a fixed set of planets. They are the only values of the class, therefore you can switch over them exhaustively &#8212; without having to write a default clause: enum Planet { MERCURY, VENUS, EARTH } Planet p = ... switch (p) { case MERCURY: ... case VENUS: ... case EARTH: ... } Using enum classes to model fixed sets of values is often helpful, but sometimes we want to model a fixed set of kinds of values. We can do this by using a class hierarchy not as a mechanism for code inheritance and reuse but, rather, as a way to list kinds of values. Building on our planetary example, we might model the kinds of values in the astronomical domain as follows: interface Celestial { ... } final class Planet implements Celestial { ... } final class Star implements Celestial { ... } final class Comet implements Celestial { ... } This hierarchy does not, however, reflect the important domain knowledge that there are only three kinds of celestial objects in our model. In these situations, restricting the set of subclasses or subinterfaces can streamline the modeling. Consider another example: In a graphics library, the author of a class Shape may intend that only particular classes can extend Shape, since much of the library's work involves handling each kind of shape in the appropriate way. The author is interested in the clarity of code that handles known subclasses of Shape, and not interested in writing code to defend against unknown subclasses of Shape. Allowing arbitrary classes to extend Shape, and thus inherit its code for reuse, is not a goal in this case. Unfortunately, Java assumes that code reuse is always a goal: If Shape can be extended at all, then it can be extended by any number of classes. It would be helpful to relax this assumption so that an author can declare a class hierarchy that is not open for extension by arbitrary classes. Code reuse would still be possible within such a closed class hierarchy, but not beyond. Java developers are familiar with the idea of restricting the set of subclasses because it often crops up in API design. The language provides limited tools in this area: Either make a class final, so it has zero subclasses, or make the class or its constructor package-private, so it can only have subclasses in the same package. An example of a package-private superclass appears in the JDK: package java.lang; abstract class AbstractStringBuilder { ... } public final class StringBuffer extends AbstractStringBuilder { ... } public final class StringBuilder extends AbstractStringBuilder { ... } The package-private approach is useful when the goal is code reuse, such as having the subclasses of AbstractStringBuilder share its code for append. However, the approach is useless when the goal is modeling alternatives, since user code cannot access the key abstraction &#8212; the superclass &#8212; in order to switch over it. Allowing users to access the superclass without also allowing them to extend it cannot be specified without resorting to brittle tricks involving non-public constructors &#8212; which do not work for interfaces. In a graphics library that declares Shape and its subclasses, it would be unfortunate if only one package could access Shape. In summary, it should be possible for a superclass to be widely accessible (since it represents an important abstraction for users) but not widely extensible (since its subclasses should be restricted to those known to the author). The author of such a superclass should be able to express that it is co-developed with a given set of subclasses, both to document intent for the reader and to allow enforcement by the Java compiler. At the same time, the superclass should not unduly constrain its subclasses by, e.g., forcing them to be final or preventing them from defining their own state.",
    "description": "A sealed class or interface can be extended or implemented only by those classes and interfaces permitted to do so. A class is sealed by applying the sealed modifier to its declaration. Then, after any extends and implements clauses, the permits clause specifies the classes that are permitted to extend the sealed class. For example, the following declaration of Shape specifies three permitted subclasses: package com.example.geometry; public abstract sealed class Shape permits Circle, Rectangle, Square { ... } The classes specified by permits must be located near the superclass: either in the same module (if the superclass is in a named module) or in the same package (if the superclass is in the unnamed module). For example, in the following declaration of Shape its permitted subclasses are all located in different packages of the same named module: package com.example.geometry; public abstract sealed class Shape permits com.example.polar.Circle, com.example.quad.Rectangle, com.example.quad.simple.Square { ... } When the permitted subclasses are small in size and number, it may be convenient to declare them in the same source file as the sealed class. When they are declared in this way, the sealed class may omit the permits clause and the Java compiler will infer the permitted subclasses from the declarations in the source file. (The subclasses may be auxiliary or nested classes.) For example, if the following code is found in Root.java then the sealed class Root is inferred to have three permitted subclasses: abstract sealed class Root { ... final class A extends Root { ... } final class B extends Root { ... } final class C extends Root { ... } } Classes specified by permits must have a canonical name, otherwise a compile-time error is reported. This means that anonymous classes and local classes cannot be permitted subtypes of a sealed class. A sealed class imposes three constraints on its permitted subclasses: The sealed class and its permitted subclasses must belong to the same module, and, if declared in an unnamed module, to the same package. Every permitted subclass must directly extend the sealed class. Every permitted subclass must use a modifier to describe how it propagates the sealing initiated by its superclass: A permitted subclass may be declared final to prevent its part of the class hierarchy from being extended further. (Record classes are implicitly declared final.) A permitted subclass may be declared sealed to allow its part of the hierarchy to be extended further than envisaged by its sealed superclass, but in a restricted fashion. A permitted subclass may be declared non-sealed so that its part of the hierarchy reverts to being open for extension by unknown subclasses. A sealed class cannot prevent its permitted subclasses from doing this. (The modifier non-sealed is the first hyphenated keyword proposed for Java.) As an example of the third constraint, Circle and Square may be final while Rectangle is sealed and we add a new subclass, WeirdShape, that is non-sealed: package com.example.geometry; public abstract sealed class Shape permits Circle, Rectangle, Square, WeirdShape { ... } public final class Circle extends Shape { ... } public sealed class Rectangle extends Shape permits TransparentRectangle, FilledRectangle { ... } public final class TransparentRectangle extends Rectangle { ... } public final class FilledRectangle extends Rectangle { ... } public final class Square extends Shape { ... } public non-sealed class WeirdShape extends Shape { ... } Even though the WeirdShape is open to extension by unknown classes, all instances of those subclasses are also instances of WeirdShape. Therefore code written to test whether an instance of Shape is either a Circle, a Rectangle, a Square, or a WeirdShape remains exhaustive. Exactly one of the modifiers final, sealed, and non-sealed must be used by each permitted subclass. It is not possible for a class to be both sealed (implying subclasses) and final (implying no subclasses), or both non-sealed (implying subclasses) and final (implying no subclasses), or both sealed (implying restricted subclasses) and non-sealed (implying unrestricted subclasses). (The final modifier can be considered a special case of sealing, where extension/implementation is prohibited completely. That is, final is conceptually equivalent to sealed plus a permits clause which specifies nothing, though such a permits clause cannot be written.) A class which is sealed or non-sealed may be abstract, and have abstract members. A sealed class may permit subclasses which are abstract, providing they are then sealed or non-sealed, rather than final. It is a compile-time error if any class extends a sealed class but is not permitted to do so. Class accessibility Because extends and permits clauses make use of class names, a permitted subclass and its sealed superclass must be accessible to each other. However, permitted subclasses need not have the same accessibility as each other, or as the sealed class. In particular, a subclass may be less accessible than the sealed class. This means that, in a future release when pattern matching is supported by switches, some code will not be able to exhaustively switch over the subclasses unless a default clause (or other total pattern) is used. Java compilers will be encouraged to detect when switch is not as exhaustive as its original author imagined it would be, and customize the error message to recommend a default clause. Sealed interfaces As for classes, an interface can be sealed by applying the sealed modifier to the interface. After any extends clause to specify superinterfaces, the implementing classes and subinterfaces are specified with a permits clause. For example, the planetary example from above can be rewritten as follows: sealed interface Celestial permits Planet, Star, Comet { ... } final class Planet implements Celestial { ... } final class Star implements Celestial { ... } final class Comet implements Celestial { ... } Here is another classic example of a class hierarchy where there is a known set of subclasses: modeling mathematical expressions. package com.example.expression; public sealed interface Expr permits ConstantExpr, PlusExpr, TimesExpr, NegExpr { ... } public final class ConstantExpr implements Expr { ... } public final class PlusExpr implements Expr { ... } public final class TimesExpr implements Expr { ... } public final class NegExpr implements Expr { ... } Sealing and record classes Sealed classes work well with record classes. Record classes are implicitly final, so a sealed hierarchy of record classes is slightly more concise than the example above: package com.example.expression; public sealed interface Expr permits ConstantExpr, PlusExpr, TimesExpr, NegExpr { ... } public record ConstantExpr(int i) implements Expr { ... } public record PlusExpr(Expr a, Expr b) implements Expr { ... } public record TimesExpr(Expr a, Expr b) implements Expr { ... } public record NegExpr(Expr e) implements Expr { ... } The combination of sealed classes and record classes is sometimes referred to as algebraic data types: Record classes allow us to express product types, and sealed classes allow us to express sum types. Sealed classes and conversions A cast expression converts a value to a type. A type instanceof expression tests a value against a type. Java is extremely permissive about the types that are allowed in these kinds of expressions. For example: interface I {} class C {} // does not implement I void test (C c) { if (c instanceof I) System.out.println(\"It's an I\"); } This program is legal even though it is currently not possible for a C object to implement the interface I. Of course, as the program evolves, it might be: ... class B extends C implements I {} test(new B()); // Prints \"It's an I\" The type conversion rules capture a notion of open extensibility. The Java type system does not assume a closed world. Classes and interfaces can be extended at some future time, and casting conversions compile to runtime tests, so we can safely be flexible. However, at the other end of the spectrum the conversion rules do address the case where a class can definitely not be extended, i.e., when it is a final class. interface I {} final class C {} void test (C c) { if (c instanceof I) // Compile-time error! System.out.println(\"It's an I\"); } The method test fails to compile, since the compiler knows that there can be no subclass of C, so since C does not implement I then it is never possible for a C value to implement I. This is a compile-time error. What if C is not final, but sealed? Its direct subclasses are explicitly enumerated, and &#8212; by the definition of being sealed &#8212; in the same module, so we expect the compiler to look to see if it can spot a similar compile-time error. Consider the following code: interface I {} sealed class C permits D {} final class D extends C {} void test (C c) { if (c instanceof I) // Compile-time error! System.out.println(\"It's an I\"); } Class C does not implement I, and is not final, so by the existing rules we might conclude that a conversion is possible. C is sealed, however, and there is one permitted direct subclass of C, namely D. By the definition of sealed types, D must be either final, sealed, or non-sealed. In this example, all the direct subclasses of C are final and do not implement I. This program should therefore be rejected, since there cannot be a subtype of C that implements I. In contrast, consider a similar program where one of the direct subclasses of the sealed class is non-sealed: interface I {} sealed class C permits D, E {} non-sealed class D extends C {} final class E extends C {} void test (C c) { if (c instanceof I) System.out.println(\"It's an I\"); } This is type-correct, since it is possible for a subtype of the non-sealed type D to implement I. Consequently, supporting sealed classes leads to a change in the definition of narrowing reference conversion to navigate sealed hierarchies to determine at compile time which conversions are not possible. Sealed classes in the JDK An example of how sealed classes might be used in the JDK is in the java.lang.constant package that models descriptors for JVM entities: package java.lang.constant; public sealed interface ConstantDesc permits String, Integer, Float, Long, Double, ClassDesc, MethodTypeDesc, DynamicConstantDesc { ... } // ClassDesc is designed for subclassing by JDK classes only public sealed interface ClassDesc extends ConstantDesc permits PrimitiveClassDescImpl, ReferenceClassDescImpl { ... } final class PrimitiveClassDescImpl implements ClassDesc { ... } final class ReferenceClassDescImpl implements ClassDesc { ... } // MethodTypeDesc is designed for subclassing by JDK classes only public sealed interface MethodTypeDesc extends ConstantDesc permits MethodTypeDescImpl { ... } final class MethodTypeDescImpl implements MethodTypeDesc { ... } // DynamicConstantDesc is designed for subclassing by user code public non-sealed abstract class DynamicConstantDesc implements ConstantDesc { ... } Sealed classes and pattern matching A significant benefit of sealed classes will be realized in JEP 406, which proposes to extend switch with pattern matching. Instead of inspecting an instance of a sealed class with if-else chains, user code will be able to use a switch enhanced with patterns. The use of sealed classes will allow the Java compiler to check that the patterns are exhaustive. For example, consider this code using the sealed hierarchy declared earlier: Shape rotate(Shape shape, double angle) { if (shape instanceof Circle) return shape; else if (shape instanceof Rectangle) return shape; else if (shape instanceof Square) return shape; else throw new IncompatibleClassChangeError(); } The Java compiler cannot ensure that the instanceof tests cover all the permitted subclasses of Shape. The final else clause is actually unreachable, but this cannot be verified by the compiler. More importantly, no compile-time error message would be issued if the instanceof Rectangle test was omitted. In contrast, with pattern matching for switch (JEP 406)the compiler can confirm that every permitted subclass of Shape is covered, so no default clause or other total pattern is needed. The compiler will, moreover, issue an error message if any of the three cases is missing: Shape rotate(Shape shape, double angle) { return switch (shape) { // pattern matching switch case Circle c -&gt; c; case Rectangle r -&gt; shape.rotate(angle); case Square s -&gt; shape.rotate(angle); // no default needed! } } Java grammar The grammar for class declarations is amended to the following: NormalClassDeclaration: {ClassModifier} class TypeIdentifier [TypeParameters] [Superclass] [Superinterfaces] [PermittedSubclasses] ClassBody ClassModifier: (one of) Annotation public protected private abstract static sealed final non-sealed strictfp PermittedSubclasses: permits ClassTypeList ClassTypeList: ClassType {, ClassType} JVM support for sealed classes The Java Virtual Machine recognizes sealed classes and interfaces at runtime, and prevents extension by unauthorized subclasses and subinterfaces. Although sealed is a class modifier, there is no ACC_SEALED flag in the ClassFile structure. Instead, the class file of a sealed class has a PermittedSubclasses attribute which implicitly indicates the sealed modifier and explicitly specifies the permitted subclasses: PermittedSubclasses_attribute { u2 attribute_name_index; u4 attribute_length; u2 number_of_classes; u2 classes[number_of_classes]; } The list of permitted subclasses is mandatory. Even when the permitted subclasses are inferred by the compiler, those inferred subclasses are explicitly included in the PermittedSubclasses attribute. The class file of a permitted subclass carries no new attributes. When the JVM attempts to define a class whose superclass or superinterface has a PermittedSubclasses attribute, the class being defined must be named by the attribute. Otherwise, an IncompatibleClassChangeError is thrown. Reflection API We add the following public methods to java.lang.Class: Class&lt;?&gt;[] getPermittedSubclasses() boolean isSealed() The method getPermittedSubclasses() returns an array containing java.lang.Class objects representing the permitted subclasses of the class, if the class is sealed. It returns an empty array if the class is not sealed. The method isSealed returns true if the given class or interface is sealed. (Compare with isEnum.)",
    "specification": ""
  },
  {
    "number": "JEP 361",
    "title": "Switch Expressions",
    "url": "https://openjdk.org/jeps/361",
    "summary": "Extend switch so it can be used as either a statement or an expression, and so that both forms can use either traditional case ... : labels (with fall through) or new case ... -&gt; labels (with no fall through), with a further new statement for yielding a value from a switch expression. These changes will simplify everyday coding, and prepare the way for the use of pattern matching in switch. This was a preview language feature in JDK 12 and JDK 13.",
    "goals": "",
    "motivation": "As we prepare to enhance the Java programming language to support pattern matching (JEP 305), several irregularities of the existing switch statement -- which have long been an irritation to users -- become impediments. These include the default control flow behavior between switch labels (fall through), the default scoping in switch blocks (the whole block is treated as one scope), and the fact that switch works only as a statement, even though it is often more natural to express multi-way conditionals as expressions. The current design of Java's switch statement follows closely languages such as C and C++, and supports fall through semantics by default. Whilst this traditional control flow is often useful for writing low-level code (such as parsers for binary encodings), as switch is used in higher-level contexts, its error-prone nature starts to outweigh its flexibility. For example, in the following code, the many break statements make it unnecessarily verbose, and this visual noise often masks hard to debug errors, where missing break statements would mean accidental fall through. switch (day) { case MONDAY: case FRIDAY: case SUNDAY: System.out.println(6); break; case TUESDAY: System.out.println(7); break; case THURSDAY: case SATURDAY: System.out.println(8); break; case WEDNESDAY: System.out.println(9); break; } We propose to introduce a new form of switch label, \"case L -&gt;\", to signify that only the code to the right of the label is to be executed if the label is matched. We also propose to allow multiple constants per case, separated by commas. The previous code can now be written: switch (day) { case MONDAY, FRIDAY, SUNDAY -&gt; System.out.println(6); case TUESDAY -&gt; System.out.println(7); case THURSDAY, SATURDAY -&gt; System.out.println(8); case WEDNESDAY -&gt; System.out.println(9); } The code to the right of a \"case L -&gt;\" switch label is restricted to be an expression, a block, or (for convenience) a throw statement. This has the pleasing consequence that should an arm introduce a local variable, it must be contained in a block and is thus not in scope for any of the other arms in the switch block. This eliminates another annoyance with traditional switch blocks where the scope of a local variable is the entire block: switch (day) { case MONDAY: case TUESDAY: int temp = ... // The scope of 'temp' continues to the } break; case WEDNESDAY: case THURSDAY: int temp2 = ... // Can't call this variable 'temp' break; default: int temp3 = ... // Can't call this variable 'temp' } Many existing switch statements are essentially simulations of switch expressions, where each arm either assigns to a common target variable or returns a value: int numLetters; switch (day) { case MONDAY: case FRIDAY: case SUNDAY: numLetters = 6; break; case TUESDAY: numLetters = 7; break; case THURSDAY: case SATURDAY: numLetters = 8; break; case WEDNESDAY: numLetters = 9; break; default: throw new IllegalStateException(\"Wat: \" + day); } Expressing this as a statement is roundabout, repetitive, and error-prone. The author meant to express that we should compute a value of numLetters for each day. It should be possible to say that directly, using a switch expression, which is both clearer and safer: int numLetters = switch (day) { case MONDAY, FRIDAY, SUNDAY -&gt; 6; case TUESDAY -&gt; 7; case THURSDAY, SATURDAY -&gt; 8; case WEDNESDAY -&gt; 9; }; In turn, extending switch to support expressions raises some additional needs, such as extending flow analysis (an expression must always compute a value or complete abruptly), and allowing some case arms of a switch expression to throw an exception rather than yield a value.",
    "description": "Arrow labels In addition to traditional \"case L :\" labels in a switch block, we define a new simplified form, with \"case L -&gt;\" labels. If a label is matched, then only the expression or statement to the right of the arrow is executed; there is no fall through. For example, given the following switch statement that uses the new form of labels: static void howMany(int k) { switch (k) { case 1 -&gt; System.out.println(\"one\"); case 2 -&gt; System.out.println(\"two\"); default -&gt; System.out.println(\"many\"); } } The following code: howMany(1); howMany(2); howMany(3); results in the following output: one two many Switch expressions We extend the switch statement so it can be used as an expression. For example, the previous howMany method can be rewritten to use a switch expression, so it uses only a single println. static void howMany(int k) { System.out.println( switch (k) { case 1 -&gt; \"one\"; case 2 -&gt; \"two\"; default -&gt; \"many\"; } ); } In the common case, a switch expression will look like: T result = switch (arg) { case L1 -&gt; e1; case L2 -&gt; e2; default -&gt; e3; }; A switch expression is a poly expression; if the target type is known, this type is pushed down into each arm. The type of a switch expression is its target type, if known; if not, a standalone type is computed by combining the types of each case arm. Yielding a value Most switch expressions will have a single expression to the right of the \"case L -&gt;\" switch label. In the event that a full block is needed, we introduce a new yield statement to yield a value, which becomes the value of the enclosing switch expression. int j = switch (day) { case MONDAY -&gt; 0; case TUESDAY -&gt; 1; default -&gt; { int k = day.toString().length(); int result = f(k); yield result; } }; A switch expression can, like a switch statement, also use a traditional switch block with \"case L:\" switch labels (implying fall through semantics). In this case, values are yielded using the new yield statement: int result = switch (s) { case \"Foo\": yield 1; case \"Bar\": yield 2; default: System.out.println(\"Neither Foo nor Bar, hmmm...\"); yield 0; }; The two statements, break (with or without a label) and yield, facilitate easy disambiguation between switch statements and switch expressions: a switch statement but not a switch expression can be the target of a break statement; and a switch expression but not a switch statement can be the target of a yield statement. Rather than being a keyword, yield is a restricted identifier (like var), which means that classes named yield are illegal. If there is a unary method yield in scope, then the expression yield(x) would be ambiguous (could be either a method call, or a yield statement whose operand is a parenthesized expression), and this ambiguity is resolved in favor of the yield statement. If the method invocation is preferred then the method should be qualified, with this for an instance method or the class name for a static method. Exhaustiveness The cases of a switch expression must be exhaustive; for all possible values there must be a matching switch label. (Obviously switch statements are not required to be exhaustive.) In practice this normally means that a default clause is required; however, in the case of an enum switch expression that covers all known constants, a default clause is inserted by the compiler to indicate that the enum definition has changed between compile-time and runtime. Relying on this implicit default clause insertion makes for more robust code; now when code is recompiled, the compiler checks that all cases are explicitly handled. Had the developer inserted an explicit default clause (as is the case today) a possible error will have been hidden. Furthermore, a switch expression must either complete normally with a value, or complete abruptly by throwing an exception. This has a number of consequences. First, the compiler checks that for every switch label, if it is matched then a value can be yielded. int i = switch (day) { case MONDAY -&gt; { System.out.println(\"Monday\"); // ERROR! Block doesn't contain a yield statement } default -&gt; 1; }; i = switch (day) { case MONDAY, TUESDAY, WEDNESDAY: yield 0; default: System.out.println(\"Second half of the week\"); // ERROR! Group doesn't contain a yield statement }; A further consequence is that the control statements, break, yield, return and continue, cannot jump through a switch expression, such as in the following: z: for (int i = 0; i &lt; MAX_VALUE; ++i) { int k = switch (e) { case 0: yield 1; case 1: yield 2; default: continue z; // ERROR! Illegal jump through a switch expression }; ... }",
    "specification": ""
  },
  {
    "number": "JEP 378",
    "title": "Text Blocks",
    "url": "https://openjdk.org/jeps/378",
    "summary": "Add text blocks to the Java language. A text block is a multi-line string literal that avoids the need for most escape sequences, automatically formats the string in a predictable way, and gives the developer control over the format when desired.",
    "goals": "Simplify the task of writing Java programs by making it easy to express strings that span several lines of source code, while avoiding escape sequences in common cases. Enhance the readability of strings in Java programs that denote code written in non-Java languages. Support migration from string literals by stipulating that any new construct can express the same set of strings as a string literal, interpret the same escape sequences, and be manipulated in the same ways as a string literal. Add escape sequences for managing explicit white space and newline control.",
    "motivation": "In Java, embedding a snippet of HTML, XML, SQL, or JSON in a string literal \"...\" usually requires significant editing with escapes and concatenation before the code containing the snippet will compile. The snippet is often difficult to read and arduous to maintain. More generally, the need to denote short, medium, and long blocks of text in a Java program is near universal, whether the text is code from other programming languages, structured text representing golden files, or messages in natural languages. On the one hand, the Java language recognizes this need by allowing strings of unbounded size and content; on the other hand, it embodies a design default that strings should be small enough to denote on a single line of a source file (surrounded by \" characters), and simple enough to escape easily. This design default is at odds with the large number of Java programs where strings are too long to fit comfortably on a single line. Accordingly, it would improve both the readability and the writability of a broad class of Java programs to have a linguistic mechanism for denoting strings more literally than a string literal -- across multiple lines and without the visual clutter of escapes. In essence, a two-dimensional block of text, rather than a one-dimensional sequence of characters. Still, it is impossible to predict the role of every string in Java programs. Just because a string spans multiple lines of source code does not mean that newline characters are desirable in the string. One part of a program may be more readable when strings are laid out over multiple lines, but the embedded newline characters may change the behavior of another part of the program. Accordingly, it would be helpful if the developer had precise control over where newlines appear, and, as a related matter, how much white space appears to the left and right of the \"block\" of text. HTML example Using \"one-dimensional\" string literals String html = \"&lt;html&gt;\\n\" + \" &lt;body&gt;\\n\" + \" &lt;p&gt;Hello, world&lt;/p&gt;\\n\" + \" &lt;/body&gt;\\n\" + \"&lt;/html&gt;\\n\"; Using a \"two-dimensional\" block of text String html = \"\"\" &lt;html&gt; &lt;body&gt; &lt;p&gt;Hello, world&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; \"\"\"; SQL example Using \"one-dimensional\" string literals String query = \"SELECT \\\"EMP_ID\\\", \\\"LAST_NAME\\\" FROM \\\"EMPLOYEE_TB\\\"\\n\" + \"WHERE \\\"CITY\\\" = 'INDIANAPOLIS'\\n\" + \"ORDER BY \\\"EMP_ID\\\", \\\"LAST_NAME\\\";\\n\"; Using a \"two-dimensional\" block of text String query = \"\"\" SELECT \"EMP_ID\", \"LAST_NAME\" FROM \"EMPLOYEE_TB\" WHERE \"CITY\" = 'INDIANAPOLIS' ORDER BY \"EMP_ID\", \"LAST_NAME\"; \"\"\"; Polyglot language example Using \"one-dimensional\" string literals ScriptEngine engine = new ScriptEngineManager().getEngineByName(\"js\"); Object obj = engine.eval(\"function hello() {\\n\" + \" print('\\\"Hello, world\\\"');\\n\" + \"}\\n\" + \"\\n\" + \"hello();\\n\"); Using a \"two-dimensional\" block of text ScriptEngine engine = new ScriptEngineManager().getEngineByName(\"js\"); Object obj = engine.eval(\"\"\" function hello() { print('\"Hello, world\"'); } hello(); \"\"\");",
    "description": "This section is identical to the same section in this JEP's predecessor, JEP 355, except for the addition of the subsection on new escape sequences. A text block is a new kind of literal in the Java language. It may be used to denote a string anywhere that a string literal could appear, but offers greater expressiveness and less accidental complexity. A text block consists of zero or more content characters, enclosed by opening and closing delimiters. The opening delimiter is a sequence of three double quote characters (\"\"\") followed by zero or more white spaces followed by a line terminator. The content begins at the first character after the line terminator of the opening delimiter. The closing delimiter is a sequence of three double quote characters. The content ends at the last character before the first double quote of the closing delimiter. The content may include double quote characters directly, unlike the characters in a string literal. The use of \\\" in a text block is permitted, but not necessary or recommended. Fat delimiters (\"\"\") were chosen so that \" characters could appear unescaped, and also to visually distinguish a text block from a string literal. The content may include line terminators directly, unlike the characters in a string literal. The use of \\n in a text block is permitted, but not necessary or recommended. For example, the text block: \"\"\" line 1 line 2 line 3 \"\"\" is equivalent to the string literal: \"line 1\\nline 2\\nline 3\\n\" or a concatenation of string literals: \"line 1\\n\" + \"line 2\\n\" + \"line 3\\n\" If a line terminator is not required at the end of the string, then the closing delimiter can be placed on the last line of content. For example, the text block: \"\"\" line 1 line 2 line 3\"\"\" is equivalent to the string literal: \"line 1\\nline 2\\nline 3\" A text block can denote the empty string, although this is not recommended because it needs two lines of source code: String empty = \"\"\" \"\"\"; Here are some examples of ill-formed text blocks: String a = \"\"\"\"\"\"; // no line terminator after opening delimiter String b = \"\"\" \"\"\"; // no line terminator after opening delimiter String c = \"\"\" \"; // no closing delimiter (text block continues to EOF) String d = \"\"\" abc \\ def \"\"\"; // unescaped backslash (see below for escape processing) Compile-time processing A text block is a constant expression of type String, just like a string literal. However, unlike a string literal, the content of a text block is processed by the Java compiler in three distinct steps: Line terminators in the content are translated to LF (\\u000A). The purpose of this translation is to follow the principle of least surprise when moving Java source code across platforms. Incidental white space surrounding the content, introduced to match the indentation of Java source code, is removed. Escape sequences in the content are interpreted. Performing interpretation as the final step means developers can write escape sequences such as \\n without them being modified or deleted by earlier steps. The processed content is recorded in the class file as a CONSTANT_String_info entry in the constant pool, just like the characters of a string literal. The class file does not record whether a CONSTANT_String_info entry was derived from a text block or a string literal. At run time, a text block is evaluated to an instance of String, just like a string literal. Instances of String that are derived from text blocks are indistinguishable from instances derived from string literals. Two text blocks with the same processed content will refer to the same instance of String due to interning, just like for string literals. The following sections discuss compile-time processing in more detail. 1. Line terminators Line terminators in the content are normalized from CR (\\u000D) and CRLF (\\u000D\\u000A) to LF (\\u000A) by the Java compiler. This ensures that the string derived from the content is equivalent across platforms, even if the source code has been translated to a platform encoding (see javac -encoding). For example, if Java source code that was created on a Unix platform (where the line terminator is LF) is edited on a Windows platform (where the line terminator is CRLF), then without normalization, the content would become one character longer for each line. Any algorithm that relied on LF being the line terminator might fail, and any test that needed to verify string equality with String::equals would fail. The escape sequences \\n (LF), \\f (FF), and \\r (CR) are not interpreted during normalization; escape processing happens later. 2. Incidental white space The text blocks shown above were easier to read than their concatenated string literal counterparts, but the obvious interpretation for the content of a text block would include the spaces added to indent the embedded string so that it lines up neatly with the opening delimiter. Here is the HTML example using dots to visualize the spaces that the developer added for indentation: String html = \"\"\" ..............&lt;html&gt; .............. &lt;body&gt; .............. &lt;p&gt;Hello, world&lt;/p&gt; .............. &lt;/body&gt; ..............&lt;/html&gt; ..............\"\"\"; Since the opening delimiter is generally positioned to appear on the same line as the statement or expression which consumes the text block, there is no real significance to the fact that 14 visualized spaces start each line. Including those spaces in the content would mean the text block denotes a string different from the one denoted by the concatenated string literals. This would hurt migration, and be a recurring source of surprise: it is overwhelmingly likely that the developer does not want those spaces in the string. Also, the closing delimiter is generally positioned to align with the content, which further suggests that the 14 visualized spaces are insignificant. Spaces may also appear at the end of each line, especially when a text block is populated by copy-pasting snippets from other files (which may themselves have been formed by copy-pasting from yet more files). Here is the HTML example reimagined with some trailing white space, again using dots to visualize spaces: String html = \"\"\" ..............&lt;html&gt;... .............. &lt;body&gt; .............. &lt;p&gt;Hello, world&lt;/p&gt;.... .............. &lt;/body&gt;. ..............&lt;/html&gt;... ..............\"\"\"; Trailing white space is most often unintentional, idiosyncratic, and insignificant. It is overwhelmingly likely that the developer does not care about it. Trailing white space characters are similar to line terminators, in that both are invisible artifacts of the source code editing environment. With no visual guide to the presence of trailing white space characters, including them in the content would be a recurring source of surprise, as it would affect the length, hash code, etc, of the string. Accordingly, an appropriate interpretation for the content of a text block is to differentiate incidental white space at the start and end of each line, from essential white space. The Java compiler processes the content by removing incidental white space to yield what the developer intended. String::indent can then be used to further manipulate indentation if desired. Using | to visualize margins: |&lt;html&gt;| | &lt;body&gt;| | &lt;p&gt;Hello, world&lt;/p&gt;| | &lt;/body&gt;| |&lt;/html&gt;| The re-indentation algorithm takes the content of a text block whose line terminators have been normalized to LF. It removes the same amount of white space from each line of content until at least one of the lines has a non-white space character in the leftmost position. The position of the opening \"\"\" characters has no effect on the algorithm, but the position of the closing \"\"\" characters does have an effect if placed on its own line. The algorithm is as follows: Split the content of the text block at every LF, producing a list of individual lines. Note that any line in the content which was just an LF will become an empty line in the list of individual lines. Add all non-blank lines from the list of individual lines into a set of determining lines. (Blank lines -- lines that are empty or are composed wholly of white space -- have no visible influence on the indentation. Excluding blank lines from the set of determining lines avoids throwing off step 4 of the algorithm.) If the last line in the list of individual lines (i.e., the line with the closing delimiter) is blank, then add it to the set of determining lines. (The indentation of the closing delimiter should influence the indentation of the content as a whole -- a significant trailing line policy.) Compute the common white space prefix of the set of determining lines, by counting the number of leading white space characters on each line and taking the minimum count. Remove the common white space prefix from each non-blank line in the list of individual lines. Remove all trailing white space from all lines in the modified list of individual lines from step 5. This step collapses wholly-white-space lines in the modified list so that they are empty, but does not discard them. Construct the result string by joining all the lines in the modified list of individual lines from step 6, using LF as the separator between lines. If the final line in the list from step 6 is empty, then the joining LF from the previous line will be the last character in the result string. The escape sequences \\b (backspace), \\t (tab) and \\s (space) are not interpreted by the algorithm; escape processing happens later. Similarly, the \\&lt;line-terminator&gt; escape sequence does not prevent the splitting of lines on the line-terminator since the sequence is treated as two separate characters until escape processing. The re-indentation algorithm will be normative in The Java Language Specification. Developers will have access to it via String::stripIndent, a new instance method. Significant trailing line policy Normally, one would format a text block in two ways: first, position the left edge of the content to appear under the first \" of the opening delimiter, and second, place the closing delimiter on its own line to appear exactly under the opening delimiter. The resulting string will have no white space at the start of any line, and will not include the trailing blank line of the closing delimiter. However, because the trailing blank line is considered a determining line, moving it to the left has the effect of reducing the common white space prefix, and therefore reducing the the amount of white space that is stripped from the start of every line. In the extreme case, where the closing delimiter is moved all the way to the left, that reduces the common white space prefix to zero, effectively opting out of white space stripping. For example, with the closing delimiter moved all the way to the left, there is no incidental white space to visualize with dots: String html = \"\"\" &lt;html&gt; &lt;body&gt; &lt;p&gt;Hello, world&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; \"\"\"; Including the trailing blank line with the closing delimiter, the common white space prefix is zero, so zero white space is removed from the start of each line. The algorithm thus produces: (using | to visualize the left margin) | &lt;html&gt; | &lt;body&gt; | &lt;p&gt;Hello, world&lt;/p&gt; | &lt;/body&gt; | &lt;/html&gt; Alternatively, suppose the closing delimiter is not moved all the way to the left, but rather under the t of html so it is eight spaces deeper than the variable declaration: String html = \"\"\" &lt;html&gt; &lt;body&gt; &lt;p&gt;Hello, world&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; \"\"\"; The spaces visualized with dots are considered to be incidental: String html = \"\"\" ........ &lt;html&gt; ........ &lt;body&gt; ........ &lt;p&gt;Hello, world&lt;/p&gt; ........ &lt;/body&gt; ........ &lt;/html&gt; ........\"\"\"; Including the trailing blank line with the closing delimiter, the common white space prefix is eight, so eight white spaces are removed from the start of each line. The algorithm thus preserves the essential indentation of the content relative to the closing delimiter: | &lt;html&gt; | &lt;body&gt; | &lt;p&gt;Hello, world&lt;/p&gt; | &lt;/body&gt; | &lt;/html&gt; Finally, suppose the closing delimiter is moved slightly to the right of the content: String html = \"\"\" &lt;html&gt; &lt;body&gt; &lt;p&gt;Hello, world&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; \"\"\"; The spaces visualized with dots are considered to be incidental: String html = \"\"\" ..............&lt;html&gt; .............. &lt;body&gt; .............. &lt;p&gt;Hello, world&lt;/p&gt; .............. &lt;/body&gt; ..............&lt;/html&gt; .............. \"\"\"; The common white space prefix is 14, so 14 white spaces are removed from the start of each line. The trailing blank line is stripped to leave an empty line, which being the last line is then discarded. In other words, moving the closing delimiter to the right of the content has no effect, and the algorithm again preserves the essential indentation of the content: |&lt;html&gt; | &lt;body&gt; | &lt;p&gt;Hello, world&lt;/p&gt; | &lt;/body&gt; |&lt;/html&gt; 3. Escape sequences After the content is re-indented, any escape sequences in the content are interpreted. Text blocks support all of the escape sequences supported in string literals, including \\n, \\t, \\', \\\", and \\\\. See section 3.10.6 of the The Java Language Specification for the full list. Developers will have access to escape processing via String::translateEscapes, a new instance method. Interpreting escapes as the final step allows developers to use \\n, \\f, and \\r for vertical formatting of a string without it affecting the translation of line terminators in step 1, and to use \\b and \\t for horizontal formatting of a string without it affecting the removal of incidental white space in step 2. For example, consider this text block that contains the \\r escape sequence (CR): String html = \"\"\" &lt;html&gt;\\r &lt;body&gt;\\r &lt;p&gt;Hello, world&lt;/p&gt;\\r &lt;/body&gt;\\r &lt;/html&gt;\\r \"\"\"; The CR escapes are not processed until after the line terminators have been normalized to LF. Using Unicode escapes to visualize LF (\\u000A) and CR (\\u000D), the result is: |&lt;html&gt;\\u000D\\u000A | &lt;body&gt;\\u000D\\u000A | &lt;p&gt;Hello, world&lt;/p&gt;\\u000D\\u000A | &lt;/body&gt;\\u000D\\u000A |&lt;/html&gt;\\u000D\\u000A Note that it is legal to use \" and \"\" freely inside a text block, except immediately before the closing delimiter. For example, the following text blocks are legal: String story = \"\"\" \"When I use a word,\" Humpty Dumpty said, in rather a scornful tone, \"it means just what I choose it to mean - neither more nor less.\" \"The question is,\" said Alice, \"whether you can make words mean so many different things.\" \"The question is,\" said Humpty Dumpty, \"which is to be master - that's all.\" \"\"\"; // Note the newline before the closing delimiter String code = \"\"\" String empty = \"\"; \"\"\"; However, a sequence of three \" characters requires at least one \" to be escaped, in order to avoid mimicking the closing delimiter. (A sequence of n \" characters requires at least Math.floorDiv(n,3) of them to be escaped.) The use of \" immediately before the closing delimiter also requires escaping. For example: String code = \"\"\" String text = \\\"\"\" A text block inside a text block \\\"\"\"; \"\"\"; String tutorial1 = \"\"\" A common character in Java programs is \\\"\"\"\"; String tutorial2 = \"\"\" The empty string literal is formed from \" characters as follows: \\\"\\\"\"\"\"; System.out.println(\"\"\" 1 \" 2 \"\" 3 \"\"\\\" 4 \"\"\\\"\" 5 \"\"\\\"\"\" 6 \"\"\\\"\"\"\\\" 7 \"\"\\\"\"\"\\\"\" 8 \"\"\\\"\"\"\\\"\"\" 9 \"\"\\\"\"\"\\\"\"\"\\\" 10 \"\"\\\"\"\"\\\"\"\"\\\"\" 11 \"\"\\\"\"\"\\\"\"\"\\\"\"\" 12 \"\"\\\"\"\"\\\"\"\"\\\"\"\"\\\" \"\"\"); New escape sequences To allow finer control of the processing of newlines and white space, we introduce two new escape sequences. First, the \\&lt;line-terminator&gt; escape sequence explicitly suppresses the insertion of a newline character. For example, it is common practice to split very long string literals into concatenations of smaller substrings, and then hard wrap the resulting string expression onto multiple lines: String literal = \"Lorem ipsum dolor sit amet, consectetur adipiscing \" + \"elit, sed do eiusmod tempor incididunt ut labore \" + \"et dolore magna aliqua.\"; With the \\&lt;line-terminator&gt; escape sequence this could be expressed as: String text = \"\"\" Lorem ipsum dolor sit amet, consectetur adipiscing \\ elit, sed do eiusmod tempor incididunt ut labore \\ et dolore magna aliqua.\\ \"\"\"; For the simple reason that character literals and traditional string literals don't allow embedded newlines, the \\&lt;line-terminator&gt; escape sequence is only applicable to text blocks. Second, the new \\s escape sequence simply translates to a single space (\\u0020). Escape sequences aren't translated until after incidental space stripping, so \\s can act as fence to prevent the stripping of trailing white space. Using \\s at the end of each line in this example guarantees that each line is exactly six characters long: String colors = \"\"\" red \\s green\\s blue \\s \"\"\"; The \\s escape sequence can be used in text blocks, traditional string literals, and character literals. Concatenation of text blocks Text blocks can be used anywhere a string literal can be used. For example, text blocks and string literals may be concatenated interchangeably: String code = \"public void print(Object o) {\" + \"\"\" System.out.println(Objects.toString(o)); } \"\"\"; However, concatenation involving a text block can become rather clunky. Take this text block as a starting point: String code = \"\"\" public void print(Object o) { System.out.println(Objects.toString(o)); } \"\"\"; Suppose it needs to be changed so that the type of o comes from a variable. Using concatenation, the text block that contains the trailing code will need to start on a new line. Unfortunately, the straightforward insertion of a newline in the program, as below, will cause a long span of white space between the type and the text beginning o : String code = \"\"\" public void print(\"\"\" + type + \"\"\" o) { System.out.println(Objects.toString(o)); } \"\"\"; The white space can be removed manually, but this hurts readability of the quoted code: String code = \"\"\" public void print(\"\"\" + type + \"\"\" o) { System.out.println(Objects.toString(o)); } \"\"\"; A cleaner alternative is to use String::replace or String::format, as follows: String code = \"\"\" public void print($type o) { System.out.println(Objects.toString(o)); } \"\"\".replace(\"$type\", type); String code = String.format(\"\"\" public void print(%s o) { System.out.println(Objects.toString(o)); } \"\"\", type); Another alternative involves the introduction of a new instance method, String::formatted, which could be used as follows: String source = \"\"\" public void print(%s object) { System.out.println(Objects.toString(object)); } \"\"\".formatted(type); Additional Methods The following methods will be added to support text blocks; String::stripIndent(): used to strip away incidental white space from the text block content String::translateEscapes(): used to translate escape sequences String::formatted(Object... args): simplify value substitution in the text block",
    "specification": ""
  },
  {
    "number": "JEP 382",
    "title": "New macOS Rendering Pipeline",
    "url": "https://openjdk.org/jeps/382",
    "summary": "Implement a Java 2D internal rendering pipeline for macOS using the Apple Metal API as alternative to the existing pipeline, which uses the deprecated Apple OpenGL API.",
    "goals": "Provide a fully functional rendering pipeline for the Java 2D API that uses the macOS Metal framework. Be ready in the event Apple removes the deprecated OpenGL API from a future version of macOS. Ensure transparency of the new pipeline to Java applications. Ensure functional parity of the implementation with the existing OpenGL pipeline. Provide performance as good or better than the OpenGL pipeline in select real applications and benchmarks. Create a clean architecture that fits into the existing Java 2D pipeline model. Co-exist with the OpenGL pipeline until it is obsolete.",
    "motivation": "Two major factors motivate the introduction of a new Metal-based rendering pipeline on macOS: Apple deprecated the OpenGL rendering library in macOS 10.14, in September 2018. Java 2D on macOS is completely reliant on OpenGL for its internal rendering pipeline, so a new pipeline implementation is needed. Apple claims that the Metal framework, their replacement for OpenGL, has superior performance. For the Java 2D API, this is generally the case with some exceptions.",
    "description": "Most graphical Java applications are written using the Swing UI toolkit, which renders via the Java 2D API. Internally, Java 2D can use software rendering plus a blit to the screen or it can use a platform-specific API, such as X11/Xrender on Linux, Direct3D on Windows, or OpenGL on macOS. These platform-specific APIs typically offer much better performance than software rendering, and generally off-load the CPU. Metal is the new macOS platform API for such rendering, replacing the deprecated OpenGL API. (The name has nothing to do with the Swing &#8220;Metal&#8221; Look and Feel; that is just a coincidence.) We created a substantial amount of new internal implementation code to use the Metal framework, just as we already had for the other platform-specific APIs. Whilst easily fitting into the existing framework the new code is much more modern in its use of graphics hardware, making use of shaders rather than a fixed function pipeline. The changes are confined to macOS-specific code and even there only a minimal amount of code shared between Metal and OpenGL is updated. We did not introduce any new Java APIs, nor did we change any existing API. The Metal pipeline can co-exist with the OpenGL pipeline. When a graphical application starts up, one or the other is chosen. For now, OpenGL remains the default. Metal is used only if it is specified on startup or if the initialization of OpenGL fails, as would happen in a future version of macOS with no OpenGL support. At the time of integration of this JEP, Apple have yet to remove OpenGL. Until that happens an application can opt-in to Metal by specifying -Dsun.java2d.metal=true on the java command line. We will make the Metal rendering pipeline the default in a future release. Prior to integration in the JDK, we conducted work on this JEP in Project Lanai.",
    "specification": ""
  },
  {
    "number": "JEP 339",
    "title": "Edwards-Curve Digital Signature Algorithm (EdDSA)",
    "url": "https://openjdk.org/jeps/339",
    "summary": "Implement cryptographic signatures using the Edwards-Curve Digital Signature Algorithm (EdDSA) as described by RFC 8032.",
    "goals": "EdDSA is a modern elliptic curve signature scheme that has several advantages over the existing signature schemes in the JDK. The primary goal of this JEP is an implementation of this scheme as standardized in RFC 8032. This new signature scheme does not replace ECDSA. Additional implementation goals: Develop a platform-independent implementation of EdDSA with better performance than the existing ECDSA implementation (which uses native C code) at the same security strength. For example, EdDSA using Curve25519 at ~126 bits of security should be as fast as ECDSA using curve secp256r1 at ~128 bits of security. Ensure that the timing is independent of secrets, assuming the platform performs 64-bit integer addition/multiplication in constant time. In addition, the implementation will not branch on secrets. These properties are valuable for preventing side-channel attacks.",
    "motivation": "EdDSA is in demand due to its improved security and performance compared to other signature schemes, and is already supported in many other crypto libraries such as OpenSSL and BoringSSL. This signature scheme is an optional component of TLS 1.3, but is one of only three signature schemes that are allowed in TLS 1.3. Some users may have EdDSA certificates, and may have a strong preference to use EdDSA. These users will appreciate the ability to use EdDSA without having to use a third-party library. An additional benefit of developing an implementation of EdDSA is that it allows us to more easily develop and test the support of this algorithm in TLS 1.3.",
    "description": "New Signature, KeyFactory, and KeyPairGenerator services will be added to the SunEC provider to support EdDSA. New classes and interfaces will be added to the API to represent EdDSA keys, and new standard algorithm names will be added to describe EdDSA signature schemes. The API and implementation will support all EdDSA variants (pure, prehashed, and context). The point arithmetic will use the double and add operations defined in RFC 8032 along with a branch-free conditional assignment operation to prevent side-channel attacks. The field arithmetic will use the modular arithmetic library that was developed for XDH (JEP 324). The combined implementation will not leak secrets into timing and cache side channels, under some reasonable assumptions on the behavior of the JVM and hardware. The API will reuse the NamedParameterSpec class developed for XDH in order to describe curve domain parameters and EdDSA variants. New classes and interfaces will be developed for Edwards curve points, EdDSA keys, and signature parameters which include context information. Example API usage: // example: generate a key pair and sign KeyPairGenerator kpg = KeyPairGenerator.getInstance(\"Ed25519\"); KeyPair kp = kpg.generateKeyPair(); // algorithm is pure Ed25519 Signature sig = Signature.getInstance(\"Ed25519\"); sig.initSign(kp.getPrivate()); sig.update(msg); byte[] s = sig.sign(); // example: use KeyFactory to contruct a public key KeyFactory kf = KeyFactory.getInstance(\"EdDSA\"); boolean xOdd = ... BigInteger y = ... NamedParameterSpec paramSpec = new NamedParameterSpec(\"Ed25519\"); EdECPublicKeySpec pubSpec = new EdECPublicKeySpec(paramSpec, new EdPoint(xOdd, y)); PublicKey pubKey = kf.generatePublic(pubSpec);",
    "specification": ""
  },
  {
    "number": "JEP 352",
    "title": "Non-Volatile Mapped Byte Buffers",
    "url": "https://openjdk.org/jeps/352",
    "summary": "Add new JDK-specific file mapping modes so that the FileChannel API can be used to create MappedByteBuffer instances that refer to non-volatile memory.",
    "goals": "This JEP proposes to upgrade MappedByteBuffer to support access to non-volatile memory (NVM). The only API change required is a new enumeration employed by FileChannel clients to request mapping of a file located on an NVM-backed file system rather than a conventional, file storage system. Recent changes to the MappedByteBufer API mean that it supports all the behaviours needed to allow direct memory updates and provide the durability guarantees needed for higher level, Java client libraries to implement persistent data types (e.g. block file systems, journaled logs, persistent objects, etc.). The implementations of FileChannel and MappedByteBuffer need revising to be aware of this new backing type for the mapped file. The primary goal of this JEP is to ensure that clients can access and update NVM from a Java program efficiently and coherently. A key element of this goal is to ensure that individual writes (or small groups of contiguous writes) to a buffer region can be committed with minimal overhead i.e. to ensure that any changes which might still be in cache are written back to memory. A second, subordinate goal is to implement this commit behaviour using a restricted, JDK-internal API defined in class Unsafe, allowing it to be re-used by classes other than MappedByteBuffer that may need to commit NVM. A final, related goal is to allow buffers mapped over NVM to be tracked by the existing monitoring and management APIs. N.B. It is already possible to map a NVM device file to a MappedByteBuffer and commit writes using the current force() method, for example using Intel's libpmem library as device driver or by calling out to libpmem as a native library. However, with the current API both those implementations provide a &#8220;sledgehammer&#8221; solution. A force cannot discriminate between clean and dirty lines and requires a system call or JNI call to implement each writeback. For both those reasons the existing capability fails to satisfy the efficiency requirement of this JEP. The target OS/CPU platform combinations for this JEP are Linux/x64 and Linux/AArch64. This restriction is imposed for two reasons. This feature will only work on OSes that support the mmap system call MAP_SYNC flag, which allows synchronous mapping of non-volatile memory. That is true of recent Linux releases. It will also only work on CPUs that support cache line writeback under user space control. x64 and AArch64 both provide instructions meeting this requirement.",
    "motivation": "NVM offers the opportunity for application programmers to create and update program state across program runs without incurring the significant copying and/or translation costs that output to and input from a persistent medium normally implies. This is particularly significant for transactional programs, where regular persistence of in-doubt state is required to enable crash recovery. Existing C libraries (such as Intel's libpmem) provide C programs with highly efficient access to NVM at the base level. They also build on this to support simple management of a variety of persistent data types. Currently, use of even just the base library from Java is costly because of the frequent need to make system calls or JNI calls to invoke the primitive operation which ensures memory changes are persistent. The same problem limits use of the higher-level libraries and is exacerbated by the fact that the persistent data types provided in C are allocated in memory not directly accessible from Java. This places Java applications and middleware (for example, a Java transaction manager) at a severe disadvantage compared with C or languages which can link into C libraries at low cost. This proposal attempts to remedy the first problem by allowing efficient writeback of NVM mapped to a ByteBuffer. Since ByteBuffer-mapped memory is directly accessible to Java this allows the second problem to be addressed by implementing client libraries equivalent to those provided in C to manage storage of different persistent data types.",
    "description": "Preliminary Changes This JEP makes use of two related enhancements to the Java SE API: Support implementation-defined Map Modes (JDK-8221397) MappedByteBuffer::force method to specify range (JDK-8221696) Proposed JDK-Specific API Changes Expose new MapMode enumeration values via a public API in a new module A new module, jdk.nio.mapmode, will export a single new package of the same name. A public extension enumeration ExtendedMapMode will be added to this package: package jdk.nio.mapmode; . . . public class ExtendedMapMode { private ExtendedMapMode() { } public static final MapMode READ_ONLY_SYNC = . . . public static final MapMode READ_WRITE_SYNC = . . . } The new enumeration values are used when calling the FileChannel::map method to create, respectively, a read-only or read-write MappedByteBuffer mapped over an NVM device file. An UnsupportedOperationException will be thrown if these flags are passed on platforms which do not support mapping of NVM device files. On supported platforms, it is only appropriate to pass these new values as arguments when the target FileChannel instance is derived from a file opened via an NVM device. In any other case an IOException will be thrown. Publish a BufferPoolMXBean tracking persistent MappedByteBuffer statistics The ManagementFactory class provides method List&lt;T&gt; getPlatformMXBeans(Class&lt;T&gt;) which can be used to retrieve a list of BufferPoolMXBean instances tracking count, total_capacity and memory_used for the existing categories of mapped or direct byte buffers. It will be modified to return an extra, new BufferPoolMXBean with name \"mapped - 'non-volatile memory'\", which will track the above stats for all MappedByteBuffer instances currently mapped with mode ExtendedMapMode.READ_ONLY_SYNC or ExtendedMapMode.READ_WRITE_SYNC. The existing BufferPoolMXBean with name mapped will continue only to track stats for MappedByteBuffer instances currently mapped with mode MapMode.READ_ONLY, MapMode.READ_WRITE or MapMode.PRIVATE. Proposed Internal JDK API Changes Add new method writebackMemory to class jdk.internal.misc.Unsafe public void writebackMemory(long address, long length) A call to this method ensures that any modifications to memory in the address range starting at address and continuing up to (but not necessarily including) address + length are guaranteed to have been written back from cache to memory. The implementation must guarantee that all stores by the current thread that i) are pending at the point of call and ii) address memory in the target range are included in the writeback (i.e., there is no need for the caller to perform any memory fence operation before the call). It must also guarantee that writeback of all addressed bytes has completed before returning (i.e., there is no need for the caller to perform any memory fence operation after the call). The writeback memory operation will be implemented using a small number of intrinsics recognised by the JIT compiler. The goal is to implement writeback of each successive cache line in the specified address range using an intrinsic that translates to a processor cache line writeback instruction, reducing the cost of persisting data to the bare minimum. The envisaged design also employs a pre-writeback and post-writeback memory synchronizaton intrinsic. These may translate to a memory synchronization instruction or to a no-op depending upon the specific choice of instruction for the processor writeback (x64 has three possible candidates) and the ordering requirements that choice entails. N.B. A good reason for implementing this capability in class Unsafe is that it is likely to be of more general use, say for alternative data persistence implementations employing non-volatile memory.",
    "specification": ""
  },
  {
    "number": "JEP 380",
    "title": "Unix-Domain Socket Channels",
    "url": "https://openjdk.org/jeps/380",
    "summary": "Add Unix-domain (AF_UNIX) socket support to the socket channel and server-socket channel APIs in the java.nio.channels package. Extend the inherited channel mechanism to support Unix-domain socket channels and server socket channels.",
    "goals": "Unix-domain sockets are used for inter-process communication (IPC) on the same host. They are similar to TCP/IP sockets in most respects, except that they are addressed by filesystem path names rather than Internet Protocol (IP) addresses and port numbers. The goal of this JEP is to support all of the features of Unix-domain sockets that are common across the major Unix platforms and Windows. Unix-domain socket channels will behave the same as existing TCP/IP channels in terms of read/write behavior, connection setup, acceptance of incoming connections by servers, multiplexing with other non-blocking selectable channels in a selector, and support of relevant socket options.",
    "motivation": "For local, inter-process communication, Unix-domain sockets are both more secure and more efficient than TCP/IP loopback connections. Unix-domain sockets are strictly for communication between processes on the same system. Applications that are not intended to accept remote connections can improve security by using Unix-domain sockets. Unix-domain sockets are further protected by operating-system enforced, filesystem-based access controls. Unix-domain sockets have faster setup times and higher data throughput than TCP/IP loopback connections. Unix-domain sockets may be a better solution than TCP/IP sockets for container environments, where communication between containers on the same system is required. This can be achieved using sockets located in shared volumes. Unix-domain sockets have long been a feature of most Unix platforms, and are now supported in Windows 10 and Windows Server 2019.",
    "description": "To support Unix-domain socket channels we will add the following API elements: A new socket address class, java.net.UnixDomainSocketAddress; A UNIX constant value in the existing java.net.StandardProtocolFamily enum; New open factory methods on SocketChannel and ServerSocketChannel that specify the protocol family Updates to the SocketChannel and ServerSocketChannel specifications to specify how channels to Unix domain sockets behave.",
    "specification": ""
  },
  {
    "number": "JEP 373",
    "title": "Reimplement the Legacy DatagramSocket API",
    "url": "https://openjdk.org/jeps/373",
    "summary": "Replace the underlying implementations of the java.net.DatagramSocket and java.net.MulticastSocket APIs with simpler and more modern implementations that are easy to maintain and debug. The new implementations will be easy to adapt to work with virtual threads, currently being explored in Project Loom. This is a follow-on to JEP 353, which already reimplemented the legacy Socket API.",
    "goals": "",
    "motivation": "The code base of the java.net.DatagramSocket and java.net.MulticastSocket APIs, and their underlying implementations, is old and brittle: The implementations date back to JDK 1.0. They are a mix of legacy Java and C code that is difficult to maintain and debug. The implementation of MulticastSocket is particularly problematic since it dates back to a time when IPv6 was still under development. Much of the underlying native implementation tries to reconcile IPv4 and IPv6 in ways that are difficult to maintain. The implementation also has several concurrency issues (e.g., with asynchronous close) that require an overhaul to address properly. In addition, in the context of virtual threads that park rather than block underlying kernel threads in system calls, the current implementation is not fit for purpose. As datagram-based transports gain traction again (e.g. QUIC), a simpler and more maintainable implementation is needed.",
    "description": "Currently, the DatagramSocket and MulticastSocket classes delegate all socket calls to a java.net.DatagramSocketImpl implementation, for which different platform-specific concrete implementations exist: PlainDatagramSocketImpl on Unix platforms, and TwoStackPlainDatagramSocketImpl and DualPlainDatagramSocketImpl on Windows platforms. The abstract DatagramSocketImpl class, which dates back to JDK 1.1, is very under-specified and contains several obsolete methods that are an impediment to providing an implementation of this class based on NIO (see alternatives, discussed below). Rather than provide a drop-in replacement for implementations of DatagramSocketImpl, similar to what was done in JEP 353 for SocketImpl, this JEP proposes to make DatagramSocket internally wrap another instance of DatagramSocket to which it delegates all calls directly. The wrapped instance is either a socket adapter created from a NIO DatagramChannel::socket (the new implementation), or else a clone of the legacy DatagramSocket class which then delegates to the legacy DatagramSocketImpl implementation (for the purpose of implementing a backward compatibility switch). If a DatagramSocketImplFactory is installed by an application, the old legacy implementation is selected. Otherwise, the new implementation is selected and used by default. To reduce the risk of switching the implementation after more than twenty years, the legacy implementation will not be removed. A JDK-specific system property, jdk.net.usePlainDatagramSocketImpl, is introduced to configure the JDK to use the legacy implementation (see risks and assumptions, below). If set with no value or set to the value &#8221;true\" at startup, the legacy implementation is used. Otherwise, the new (NIO-based) implementation is used. In some future release we will remove the legacy implementation and the system property. At some point we may also deprecate and remove DatagramSocketImpl and DatagramSocketImplFactory. The new implementation is enabled by default. It provides non-interruptible behavior for datagram and multicast sockets by directly using the platform-default implementation of the selector provider (sun.nio.ch.SelectorProviderImpl and sun.nio.ch.DatagramChannelImpl). Installing a custom selector provider will thus have no effect on DatagramSocket and MulticastSocket.",
    "specification": ""
  },
  {
    "number": "JEP 353",
    "title": "Reimplement the Legacy Socket API",
    "url": "https://openjdk.org/jeps/353",
    "summary": "Replace the underlying implementation used by the java.net.Socket and java.net.ServerSocket APIs with a simpler and more modern implementation that is easy to maintain and debug. The new implementation will be easy to adapt to work with user-mode threads, a.k.a. fibers, currently being explored in Project Loom.",
    "goals": "",
    "motivation": "The java.net.Socket and java.net.ServerSocket APIs, and their underlying implementations, date back to JDK 1.0. The implementation is a mix of legacy Java and C code that is painful to maintain and debug. The implementation uses the thread stack as the I/O buffer, an approach that has required increasing the default thread stack size on several occasions. The implementation uses a native data structure to support asynchronous close, a source of subtle reliability and porting issues over the years. The implementation also has several concurrency issues that require an overhaul to address properly. In the context of a future world of fibers that park instead of blocking threads in native methods, the current implementation is not fit for purpose.",
    "description": "The java.net.Socket and java.net.ServerSocket APIs delegate all socket operations to a java.net.SocketImpl, a Service Provider Interface (SPI) mechanism that has existed since JDK 1.0. The built-in implementation is termed the &#8220;plain&#8221; implementation, implemented by the non-public PlainSocketImpl with supporting classes SocketInputStream and SocketOutputStream. PlainSocketImpl is extended by two other JDK-internal implementations that support connections through SOCKS and HTTP proxy servers. By default, a Socket and ServerSocket is created (sometimes lazily) with a SOCKS based SocketImpl. In the case of ServerSocket, the use of the SOCKS implementation is an oddity that dates back to experimental (and since removed) support for proxying server connections in JDK 1.4. The new implementation, NioSocketImpl, is a drop-in replacement for PlainSocketImpl. It is developed to be easy to maintain and debug. It shares the same JDK-internal infrastructure as the New I/O (NIO) implementation so it doesn't need its own native code. It integrates with the existing buffer cache mechanism so that it doesn&#8217;t need to use the thread stack for I/O. It uses java.util.concurrent locks rather than synchronized methods so that it can play well with fibers in the future. In JDK 11, the NIO SocketChannel and the other SelectableChannel implementations were mostly re-implemented with the same goal in mind. The following are a few points about the new implementation: SocketImpl is a legacy SPI mechanism and is very under-specified. The new implementation attempts to be compatible with the old implementation by emulating unspecified behavior and exceptions where applicable. The Risks and Assumptions section below details the behavior differences between the old and new implementations. Socket operations using timeouts (connect, accept, read) are implemented by changing the socket to non-blocking mode and polling the socket. The java.lang.ref.Cleaner mechanism is used to close sockets when the SocketImpl is garbage collected and the socket has not been explicitly closed. Connection reset handling is implemented in the same way as the old implementation so that attempts to read after a connection reset will fail consistently. ServerSocket is modified to use NioSocketImpl (or PlainSocketImpl) by default. It no longer uses the SOCKS implementation. The SocketImpl implementations to support SOCKS and HTTP proxy servers are modified to delegate so they can work with the old and new implementations. The instrumentation support for socket I/O in Java Flight Recorder is modified to be independent of the SocketImpl so that socket I/O events can be recorded when running with either the new, old, or custom implementations. To reduce the risk of switching the implementation after more than twenty years, the old implementation will not be removed. The old implementation will remain in the JDK and a system property will be introduced to configure the JDK to use the old implementation. The JDK-specific system property to switch to the old implementation is jdk.net.usePlainSocketImpl. If set, or set to the value true, at startup, then the old implementation will be used. Some future release will remove PlainSocketImpl and the system property. This JEP does not propose to provide an alternative implementation of DatagramSocketImpl at this time (DatagramSocketImpl is the underlying implementation that instances of java.net.DatagramSocket delegate to). The built-in default implementation (PlainDatagramSocketImpl) is a maintenance (and porting) burden and may be the subject of another JEP.",
    "specification": ""
  },
  {
    "number": "JEP 371",
    "title": "Hidden Classes",
    "url": "https://openjdk.org/jeps/371",
    "summary": "Introduce hidden classes, which are classes that cannot be used directly by the bytecode of other classes. Hidden classes are intended for use by frameworks that generate classes at run time and use them indirectly, via reflection. A hidden class may be defined as a member of an access control nest, and may be unloaded independently of other classes.",
    "goals": "Allow frameworks to define classes as non-discoverable implementation details of the framework, so that they cannot be linked against by other classes nor discovered through reflection. Support extending an access control nest with non-discoverable classes. Support aggressive unloading of non-discoverable classes, so that frameworks have the flexibility to define as many as they need. Deprecate the non-standard API sun.misc.Unsafe::defineAnonymousClass, with the intent to deprecate it for removal in a future release. Do not change the Java programming language in any way.",
    "motivation": "Many language implementations built on the JVM rely upon dynamic class generation for flexibility and efficiency. For example, in the case of the Java language, javac does not translate a lambda expression into a dedicated class file at compile time but, rather, emits bytecode that dynamically generates and instantiates a class to yield an object corresponding to the lambda expression when needed. Similarly, runtimes for non-Java languages often implement the higher-order features of those languages by using dynamic proxies, which also generate classes dynamically. Language implementors usually intend for a dynamically generated class to be logically part of the implementation of a statically generated class. This intent suggests various properties that are desirable for dynamically generated classes: Non-discoverability. Being independently discoverable by name is not only unnecessary but harmful. It undermines the goal that the dynamically generated class is merely an implementation detail of the statically generated class. Access control. It may be desirable to extend the access control context of the statically generated class to include the dynamically generated class. Lifecycle. Dynamically generated classes may only be needed for a limited time, so retaining them for the lifetime of the statically generated class might unnecessarily increase memory footprint. Existing workarounds for this situation, such as per-class class loaders, are cumbersome and inefficient. Unfortunately, the standard APIs that define a class -- ClassLoader::defineClass and Lookup::defineClass -- are indifferent to whether the bytecodes of the class were generated dynamically (at run time) or statically (at compile time). These APIs always define a visible class that will be used every time another class in the same loader hierarchy tries to link a class of that name. Consequently, the class may be more discoverable or have a longer lifecycle than desired. In addition, the APIs can only define a class that will act as a member of a nest if the nest's host class knows the name of the member class in advance; practically speaking, this prevents dynamically generated classes from being members of a nest. If a standard API could define hidden classes that are not discoverable and have a limited lifecycle, then frameworks both inside and outside of the JDK that generate classes dynamically could instead define hidden classes. This would improve the efficiency of all language implementations built on the JVM. For example: java.lang.reflect.Proxy could define hidden classes to act as the proxy classes which implement proxy interfaces; java.lang.invoke.StringConcatFactory could generate hidden classes to hold the constant-concatenation methods; java.lang.invoke.LambdaMetaFactory could generate hidden nestmate classes to hold lambda bodies that access enclosing variables; and A JavaScript engine could generate hidden classes for the bytecode translated from JavaScript programs, knowing that the classes will be unloaded when the engine no longer uses them.",
    "description": "The Lookup API introduced in Java 7 allows a class to obtain a lookup object that provides reflective access to classes, methods, and fields. Crucially, no matter what code ends up using a lookup object, the reflective access always occurs in the context of the class which originally obtained the lookup object -- the lookup class. In effect, a lookup object transmits the access rights of the lookup class to any code which receives the object. Java 9 enhanced the transmission capabilities of lookup objects by introducing the method Lookup::defineClass(byte[]). From the bytes supplied, this method defines a new class in the same context as the class which originally obtained the lookup object. That is, the newly-defined class has the same defining class loader, run-time package, and protection domain as the lookup class. This JEP proposes to extend the Lookup API to support defining a hidden class that can only be accessed by reflection. A hidden class is not discoverable by the JVM during bytecode linkage, nor by programs making explicit use of class loaders (via, e.g., Class::forName and ClassLoader::loadClass). A hidden class can be unloaded when it is no longer reachable, or it can share the lifetime of a class loader so that it is unloaded only when the class loader is garbage collected. Optionally, a hidden class can be created as a member of an access control nest. For brevity, this JEP speaks of a \"hidden class\", but it should be understood to mean a hidden class or interface. Similarly, a \"normal class\" means a normal class or interface, the result of ClassLoader::defineClass. Creating a hidden class Whereas a normal class is created by invoking ClassLoader::defineClass, a hidden class is created by invoking Lookup::defineHiddenClass. This causes the JVM to derive a hidden class from the supplied bytes, link the hidden class, and return a lookup object that provides reflective access to the hidden class. The invoking program should store the lookup object carefully, for it is the only way to obtain the Class object of the hidden class. The supplied bytes must be a ClassFile structure (JVMS 4.1). The derivation of a hidden class by Lookup::defineHiddenClass is similar to the derivation of a normal class by ClassLoader::defineClass, with one major difference discussed below. After the hidden class is derived, it is linked as for a normal class (JVMS 5.4), except that no loading constraints are imposed. After the hidden class is linked, it is initialized if the initialize argument of Lookup::defineHiddenClass is true; if the argument is false, then the hidden class will be initialized when reflective methods instantiate it or access its members. The major difference in how a hidden class is created lies in the name it is given. A hidden class is not anonymous. It has a name that is available via Class::getName and may be shown in diagnostics (such as the output of java -verbose:class), in JVM TI class loading events, in JFR events, and in stack traces. However, the name has a sufficiently unusual form that it effectively makes the class invisible to all other classes. The name is the concatenation of: The binary name in internal form (JVMS 4.2.1) specified by this_class in the ClassFile structure, say A/B/C; The '.' character; and An unqualified name (JVMS 4.2.2) that is chosen by the JVM implementation. For example, if this_class specifies com/example/Foo (the internal form of the binary name com.example.Foo), then a hidden class derived from the ClassFile structure may be named com/example/Foo.1234. This string is neither a binary name nor the internal form of a binary name. Given a hidden class whose name is A/B/C.x, the result of Class::getName is the concatenation of: The binary name A.B.C (obtained by taking A/B/C and replacing each '/' with '.'); The '/' character; and The unqualified name x. For example, if a hidden class is named com/example/Foo.1234, then the result of Class::getName is com.example.Foo/1234. Again, this string is neither a binary name nor the internal form of a binary name. The namespace of hidden classes is disjoint from the namespace of normal classes. Given a ClassFile structure where this_class specifies com/example/Foo/1234, invoking cl.defineClass(\"com.example.Foo.1234\", bytes, ...) merely results in a normal class named com.example.Foo.1234, distinct from the hidden class named com.example.Foo/1234. It is impossible to create a normal class named com.example.Foo/1234 because cl.defineClass(\"com.example.Foo/1234\", bytes, ...) will reject the string argument as being not a binary name. We acknowledge that not using binary names for the names of hidden classes is potentially a source of problems, but it is compatible with the longstanding practice of Unsafe::defineAnonymousClass (see discussion here). The use of / to indicate a hidden class in the Class::getName output is also aligned stylistically with the use of / in stack traces to qualify a class by its defining module and loader (see StackTraceElement::toString). The error log below reveals two hidden classes, both in module m1: one hidden class has a method test, the other has a method apply. java.lang.Error: thrown from hidden class com.example.Foo/0x0000000800b7a470 at m1/com.example.Foo/0x0000000800b7a470.toString(Foo.java:16) at m1/com.example.Foo_0x0000000800b7a470$$Lambda$29/0x0000000800b7c040.apply(&lt;Unknown&gt;:1000001) at m1/com.example.Foo/0x0000000800b7a470.test(Foo.java:11) Hidden classes and class loaders Despite the fact that a hidden class has a corresponding Class object, and the fact that a hidden class's supertypes are created by class loaders, no class loader is involved in the creation of the hidden class itself. Notice that this JEP never says that a hidden class is \"loaded\". No class loaders are recorded as initiating loaders of a hidden class, and no loading constraints are generated that involve hidden classes. Consequently, hidden classes are not known to any class loader: A symbolic reference in the run-time constant pool of a class D to a class C denoted by N will never resolve to a hidden class for any value of D, C, and N. The reflective methods Class::forName, ClassLoader::findLoadedClass, and Lookup::findClass will not find hidden classes. Notwithstanding this detachment from class loaders, a hidden class is deemed to have a defining class loader. This is necessary to resolve types used by the hidden class's own fields and methods. In particular, a hidden class has the same defining class loader, runtime package, and protection domain as the lookup class, which is the class that originally obtained the lookup object on which Lookup::defineHiddenClass is invoked. Using a hidden class Lookup::defineHiddenClass returns a Lookup object whose lookup class is the newly created hidden class. A Class object can be obtained for the hidden class by invoking Lookup::lookupClass on the returned Lookup object. Via the Class object, the hidden class can be instantiated and its members accessed as if it was a normal class, except for four restrictions: Class::getName returns a string that is not a binary name, as described earlier. Class::getCanonicalName returns null, indicating the hidden class has no canonical name. (Note that the Class object for an anonymous class in the Java language has the same behavior.) Final fields declared in a hidden class are not modifiable. Field::set and other setter methods on a final field of a hidden class will throw IllegalAccessException regardless of the field's accessible flag. The Class object is not modifiable by instrumentation agents, and cannot be redefined or retransformed by JVM TI agents. We will, however, extend JVM TI and JDI to support hidden classes, such as testing whether a class is hidden, including hidden classes in any list of \"loaded\" classes, and sending JVM TI events when hidden classes are created. It is important to realize that the only way for other classes to use a hidden class is indirectly, via its Class object. The hidden class cannot be used directly by bytecode instructions in other classes because it cannot be referenced nominally, that is, by name. For example, suppose a framework learns of a hidden class named com.example.Foo/1234, and manufactures a class file which attempts to instantiate the hidden class. Code in the class file would contain a new instruction that ultimately points to a constant pool entry which denotes the name. If the framework attempts to denote the name as com/example/Foo.1234, then the class file will be invalid -- com/example/Foo.1234 is not a valid internal form of a binary name. On the other hand, if the framework attempts to denote the name in the valid internal form com/example/Foo/1234, then the JVM would resolve the constant pool entry by first converting the name in internal form to a binary name, com.example.Foo.1234, and then trying to load a class of that name; this will most likely fail, and will certainly not find the hidden class named com.example.Foo/1234. The hidden class is not truly anonymous, since its name is exposed, but it is effectively invisible. Without the ability of the constant pool to refer nominally to a hidden class, there is no way to use a hidden class as a superclass, field type, return type, or parameter type. This lack of usability is reminiscent of anonymous classes in the Java language, but hidden classes go further: An anonymous class can enclose other classes in order to let them access its members, but a hidden class cannot enclose other classes (their InnerClasses attributes cannot name it). Even a hidden class is unable to use itself as a field type, return type, or parameter type in its own field and method declarations. Importantly, code in a hidden class can use the hidden class directly, without relying on the Class object. This is because bytecode instructions in a hidden class can refer to the hidden class symbolically (without concern for its name) rather than nominally. For example, a new instruction in a hidden class can instantiate the hidden class via a constant pool entry which refers directly to the this_class item in the current ClassFile. Other instructions, such as getstatic, getfield, putstatic, putfield, invokestatic, and invokevirtual, can access members of the hidden class via the same constant pool entry. Direct use inside the hidden class is important because it simplifies generation of hidden classes by language runtimes and frameworks. A hidden class generally has the same powers of reflection as a normal class. That is, code in a hidden class may define normal classes and hidden classes, and may manipulate normal classes and hidden classes via their Class objects. A hidden class may even act as a lookup class. That is, code in a hidden class may obtain a lookup object on itself, which helps with hidden nestmates (see below). Hidden classes in stack traces Methods of hidden classes are not shown in stack traces by default. They represent implementation details of language runtimes, and are never expected to be useful to developers diagnosing application issues. However, they can be included in stack traces via the options -XX:+UnlockDiagnosticVMOptions -XX:+ShowHiddenFrames. There are three APIs which reify stack traces: Throwable::getStackTrace, Thread::getStackTrace and the newer StackWalker API introduced in Java 9. For the Throwable::getStackTrace and Thread::getStackTrace API, stack frames for hidden classes are omitted by default; they can be included with the same options as for stack traces above. For the StackWalker API, stack frames for hidden classes should be included by a JVM implementation only if the SHOW_HIDDEN_FRAMES option is set. This allows stack-trace filtering to omit unnecessary information when developers are diagnosing application issues. Hidden classes in access control nests Introduced in Java 11 by JEP 181, a nest is a set of classes that allow access to each other's private members but without any of the backdoor accessibility-broadening methods usually associated with nested classes in the Java language. The set is defined statically: One class serves as the nest host, its class file enumerating the other classes that are nest members; in turn, the nest members indicate in their class files which class hosts the nest. While static membership works well for class files generated from Java source code, it is usually insufficient for class files generated dynamically by language runtimes. To help such runtimes, and to encourage the use of Lookup::defineHiddenClass over Unsafe::defineAnonymousClass, a hidden class can join a nest at run time; a normal class cannot. A hidden class can be created as a member of an existing nest by passing the NESTMATE option to Lookup::defineHiddenClass. The nest which the hidden class joins is not determined by an argument to Lookup::defineHiddenClass. Instead, the nest to be joined is inferred from the lookup class, that is, from the class whose code initially obtained the lookup object: The hidden class is a member of the same nest as the lookup class (see below). In order for Lookup::defineHiddenClass to add hidden classes to the nest, the lookup object must have the proper permissions, namely PRIVATE and MODULE access. These permissions assert that the lookup object was obtained by the lookup class with the intent of allowing other code to expand the nest. The JVM disallows nested nests. A member of one nest cannot serve as the host of another nest, regardless of whether nest membership is defined statically or dynamically. The lookup class's membership of a nest may be indicated statically (via NestHost) if the lookup class is a normal class, or it may have been set dynamically if the lookup class is a hidden class. Static nest membership is validated lazily. It is important for a language runtime or framework library to be able to add hidden classes to the nest of a lookup class that may have a bad nest membership. As an example, consider the LambdaMetaFactory framework introduced in Java 8. When the source code of a class C contains a lambda expression, the corresponding C.class file uses LambdaMetaFactory at run time to define a hidden class that holds the body of the lambda expression and implements the required functional interface. C.class may have a bad NestHost attribute but the execution of C never references the class H named in the NestHost attribute. Since the lambda body may access private members of C, the hidden class needs to be able to access them too; accordingly, LambdaMetaFactory attempts to define the hidden class as a member of the nest hosted by C. Suppose that we have a lookup class, C, and that defineHiddenClass is invoked with the NESTMATE option to create a hidden class and add it into a nest of C. The nest host of the hidden class is determined as follows: If C is a normal class and lacks a NestHost attribute, then C is its own host and also is the nest host of the hidden class. If C is a normal class with a valid NestHost attribute named H, then the nest host of C, H, is the nest host of the hidden class. In this case, the hidden class is added as a member of the nest of H. If C is a normal class with a bad NestHost attribute, then C is used as the nest host of the hidden class. If C is a hidden class created without NESTMATE option, then C is its own host and also is the nest host of the hidden class. If C is a hidden class created with NESTMATE option and dynamically added to the nest of D, then the nest host of D is used as the nest host of the hidden class. If a hidden class is created without the NESTMATE option then the hidden class is the host of its own nest. This aligns with the policy that every class is either a member of a nest with another class as nest host, or else is itself the nest host of a nest. The hidden class can create additional hidden classes as members of its nest: Code in the hidden class first obtains a lookup object on itself, then invokes Lookup::defineHiddenClass on the object and passes the NESTMATE option. Given the Class object for a hidden class created as a member of a nest, Class::getNestHost and Class::isNestmateOf will work as expected. Class::getNestMembers can be called on the Class object of any class in the nest -- whether member or host, whether normal or hidden -- but returns only the members defined statically (that is, the normal classes enumerated by NestMembers in the host) along with the nest host. Class::getNestMembers does not include the hidden classes added to the nest dynamically because hidden classes are non-discoverable and should only be of interest to the code that created them, which knows the nest membership already. This prevents a hidden class from leaking through the nest membership if intended to be kept private. Unloading hidden classes A class defined by a class loader has a strong relationship with that class loader. In particular, every Class object has a reference to the ClassLoader that defined it. This tells the JVM which loader to use when resolving symbols in the class. One consequence of this relationship is that a normal class cannot be unloaded unless its defining loader can be reclaimed by the garbage collector (JLS 12.7). Being able to reclaim the defining loader implies there are no live references to the loader, which in turn implies there are no live references to any of the classes defined by the loader. (Such classes, if they were reachable, would refer to the loader.) This widespread lack of liveness is the only state where it is safe to unload a normal class. Accordingly, to maximize the chance of unloading a normal class, it is important to minimize references to both the class and its defining loader. Language runtimes typically achieve this by creating many class loaders, each dedicated to defining just one class, or perhaps a small number of related classes. When all instances of a class are reclaimed, and assuming the runtime does not hold on to the class loader, both the class and its defining loader can be reclaimed. However, the resulting large number of class loaders is demanding on memory. In addition, ClassLoader::defineClass is considerably slower than Unsafe::defineAnonymousClass according to microbenchmarks. A hidden class is not created by a class loader and has only a loose connection to the class loader deemed to be its defining loader. We can turn these facts to our advantage by allowing a hidden class to be unloaded even if its notional defining loader cannot be reclaimed by the garbage collector. As long as there are live references to a hidden class -- either to instances of the hidden class, or to its Class object -- then the hidden class keeps its notional defining loader alive so that the JVM can use that loader to resolve symbols in the hidden class. When the last live reference to the hidden class goes away, however, the loader need not return the favor by keeping the hidden class alive. Unloading a normal class while its defining loader is reachable is unsafe because the loader may later be asked, either by the JVM or or by code using reflection, to reload the class, that is, to load a class with the same name. This can have unpredictable effects when static initializers are run for a second time. There is no such concern about unloading a hidden class, since hidden classes are not created in the same manner. Because a hidden class's name is an output of Lookup::defineHiddenClass, not an input, there is no way to recreate the \"same\" hidden class that was unloaded previously. By default, Lookup::defineHiddenClass will create a hidden class that can be unloaded regardless of whether its notional defining loader is still alive. That is, when all instances of the hidden class are reclaimed and the hidden class is no longer reachable, it may be unloaded even though its notional defining loader is still reachable. This behavior is useful when a language runtime creates a hidden class to serve multiple classes defined by arbitrary class loaders: The runtime will see an improvement in footprint and performance relative to both ClassLoader::defineClass and Unsafe::defineAnonymousClass. In other cases, a language runtime may link a hidden class to just one normal class, or perhaps a small number of normal classes, with the same defining loader as the hidden class. In such cases, where the hidden class must be coterminous with a normal class, the STRONG option may be passed to Lookup::defineHiddenClass. This arranges for a hidden class to have the same strong relationship with its notional defining loader as a normal class has with its defining loader, which is to say, the hidden class will only be unloaded if its notional defining loader can be reclaimed.",
    "specification": ""
  },
  {
    "number": "JEP 334",
    "title": "JVM Constants API",
    "url": "https://openjdk.org/jeps/334",
    "summary": "Introduce an API to model nominal descriptions of key class-file and run-time artifacts, in particular constants that are loadable from the constant pool.",
    "goals": "",
    "motivation": "Every Java class file has a constant pool which stores the operands for bytecode instructions in the class. Broadly speaking, entries in the constant pool describe either run-time artifacts such as classes and methods, or simple values such as strings and integers. All these entries are known as loadable constants because they may serve as operands for the ldc instruction (\"load constant\"). They may also appear in the static argument list of a bootstrap method for the invokedynamic instruction. Executing an ldc or invokedynamic instruction causes the loadable constant to be resolved into a &#8220;live&#8221; value of a standard Java type such as Class, String, or int. Programs which manipulate class files need to model bytecode instructions, and in turn loadable constants. However, using the standard Java types to model loadable constants is inadequate. It may be acceptable for a loadable constant that describes a string (a CONSTANT_String_info entry), since producing a \"live\" String object is straightforward, but it is problematic for a loadable constant that describes a class (a CONSTANT_Class_info entry), because producing a \"live\" Class object relies on the correctness and consistency of class loading. Unfortunately, class loading has many environmental dependencies and failure modes: the desired class does not exist or may not be accessible to the requester; the result of class loading varies with context; loading classes has side-effects; and sometimes class loading may not be possible at all (such as when the classes being described do not yet exist or are otherwise not loadable, as in during compilation of those same classes, or during jlink-time transformation). Consequently, programs which deal with loadable constants would be simpler if they could manipulate classes and methods, and less well-known artifacts such as method handles and dynamically-computed constants, in a purely nominal, symbolic form: Bytecode parsing and generation libraries must describe classes and method handles in symbolic form. Without a standard mechanism, they must resort to ad-hoc mechanisms, whether descriptor types such as ASM's Handle, or tuples of strings (method owner, method name, method descriptor), or ad-hoc (and error-prone) encodings of these into a single string. Bootstraps for invokedynamic that operate by spinning bytecode (such as LambdaMetafactory) would be simpler if they could work in a symbolic domain rather than with \"live\" classes and method handles. Compilers and offline transformers (such as jlink plugins) need to describe classes and members for classes that cannot be loaded into the running VM. Compiler plugins (such as annotation processors) similarly need to describe program elements in symbolic terms. These kinds of libraries and tools would all benefit from having a single, standard way to describe loadable constants.",
    "description": "We define a family of value-based symbolic reference (JVMS 5.1) types, in the new package java.lang.invoke.constant, capable of describing each kind of loadable constant. A symbolic reference describes a loadable constant in purely nominal form, separate from class loading or accessibility context. Some classes can act as their own symbolic references (e.g., String); for linkable constants we define a family of symbolic reference types (ClassDesc, MethodTypeDesc, MethodHandleDesc, and DynamicConstantDesc) that contain the nominal information to describe these constants. A draft snapshot of the API specification can be found here, and more information on its relationship with the features in JEP 303 can be found in this companion document.",
    "specification": ""
  },
  {
    "number": "JEP 415",
    "title": "Context-Specific Deserialization Filters",
    "url": "https://openjdk.org/jeps/415",
    "summary": "Allow applications to configure context-specific and dynamically-selected deserialization filters via a JVM-wide filter factory that is invoked to select a filter for each individual deserialization operation.",
    "goals": "",
    "motivation": "Deserializing untrusted data is an inherently dangerous activity because the content of the incoming data stream determines the objects that are created, the values of their fields, and the references between them. In many typical uses the bytes in the stream are received from an unknown, untrusted, or unauthenticated client. By careful construction of the stream, an adversary can cause code in arbitrary classes to be executed with malicious intent. If object construction has side effects that change state or invoke other actions, those actions can compromise the integrity of application objects, library objects, and even the Java runtime. The key to disabling deserialization attacks is to prevent instances of arbitrary classes from being deserialized, thereby preventing the direct or indirect execution of their methods. We introduced deserialization filters (JEP 290) in Java&#160;9 to enable application and library code to validate incoming data streams before deserializing them. Such code supplies validation logic as a java.io.ObjectInputFilter when it creates a deserialization stream (i.e., a java.io.ObjectInputStream). Relying on a stream's creator to explicitly request validation has several limitations. This approach does not scale, and makes it difficult to update filters after code has been shipped. It also cannot impose filtering on deserialization operations performed by third-party libraries in an application. To address these limitations, JEP 290 also introduced a JVM-wide deserialization filter which can be set via an API, system properties, or security properties. This filter is static since it is specified exactly once, at startup. Experience with the static JVM-wide filter has revealed that it, too, has limitations, particularly in complex applications with layers of libraries and multiple execution contexts. Using the JVM-wide filter for every ObjectInputStream requires the filter to cover every execution context in the application, so the filter usually winds up being either too inclusive or too restrictive. A better approach would be to configure per-stream filters in a way that does not require the participation of every stream creator. To protect the JVM against deserialization vulnerabilities, application developers need a clear description of the objects that can be serialized or deserialized by each component or library. For each context and use case, developers should construct and apply an appropriate filter. For example, if the application uses a specific library to deserialize a particular cohort of objects then a filter for the relevant classes can be applied when calling the library. Creating an allow-list of classes, and rejecting everything else, gives protection against objects in a stream that are otherwise unknown or unexpected. Encapsulation or other natural application or library partitioning boundaries can be used to narrow the set of objects that are allowed or definitely not allowed. If it is not practical to have an allow-list then a reject-list should include classes, packages, and modules that are known not to occur in the stream or are known to be malicious. An application&#8217;s developer is in the best position to understand the structure and operation of the application&#8217;s components. This enhancement enables the application developer to construct and apply filters to every deserialization operation.",
    "description": "As noted above, JEP 290 introduced both per-stream deserialization filters and a static JVM-wide filter. Whenever an ObjectInputStream is created, its per-stream filter is initialized to be the static JVM-wide filter. That per-stream filter can later be changed to a different filter, if desired. Here we introduce a configurable JVM-wide filter factory. Whenever an ObjectInputStream is created, its per-stream filter is initialized to the value returned by invoking the static JVM-wide filter factory. Thus these filters are dynamic and context-specific, unlike the single static JVM-wide deserialization filter. For backward compatibility, if a filter factory is not set then a built-in factory returns the static JVM-wide filter if one was configured. The filter factory is used for every deserialization operation in the Java runtime, whether in application code, library code, or code in the JDK itself. The factory is specific to the application and should take into account every deserialization execution context within the application. The filter factory is called from the ObjectInputStream constructor and also from ObjectInputStream.setObjectInputFilter. The arguments are the current filter and a new filter. When called from the constructor, the current filter is null and the new filter is the static JVM-wide filter. The factory determines and returns the initial filter for the stream. The factory can create a composite filter with other context-specific controls or just return the static JVM-wide filter. If ObjectInputStream.setObjectInputFilter is called, the factory is called a second time with the filter returned from the first call and the requested new filter. The factory determines how to combine the two filters and returns the filter, replacing the filter on the stream. For simple cases, the filter factory can return a fixed filter for the entire application. For example, here is a filter that allows example classes, allows classes in the java.base module, and rejects all other classes: var filter = ObjectInputFilter.Config.createFilter(\"example.*;java.base/*;!*\") In an application with multiple execution contexts, the filter factory can better protect individual contexts by providing a custom filter for each. When the stream is constructed, the filter factory can identify the execution context based upon the current thread-local state, hierarchy of callers, library, module, and class loader. At that point, a policy for creating or selecting filters can choose a specific filter or composition of filters based on the context. If multiple filters are present then their results can be combined. A useful way to combine filters is to reject deserialization if any of the filters reject it, allow it if any filter allows it, and otherwise remain undecided. Command Line Use The properties jdk.serialFilter and jdk.serialFilterFactory can be set on the command line to set the filter and filter factory. The existing jdk.serialFilter property sets a pattern based filter. The jdk.serialFilterFactory property is the class name of the filter factory to be set before the first deserialization. The class must be public and accessible to the application class loader. For compatibility with JEP 290, if jdk.serialFilterFactory property is not set, the filter factory is set to a builtin that provides compatibility with earlier versions. API We define two methods in the ObjectInputFilter.Config class to set and get the JVM-wide filter factory. The filter factory is a function with two arguments, a current filter and a next filter, and it returns a filter. /** * Return the JVM-wide deserialization filter factory. * * @return the JVM-wide serialization filter factory; non-null */ public static BinaryOperator&lt;ObjectInputFilter&gt; getSerialFilterFactory(); /** * Set the JVM-wide deserialization filter factory. * * The filter factory is a function of two parameters, the current filter * and the next filter, that returns the filter to be used for the stream. * * @param filterFactory the serialization filter factory to set as the * JVM-wide filter factory; not null */ public static void setSerialFilterFactory(BinaryOperator&lt;ObjectInputFilter&gt; filterFactory); Example This class shows how to filter to every deserialization operation that takes place in the current thread. It defines a thread-local variable to hold the per-thread filter, defines a filter factory to return that filter, configures the factory as the JVM-wide filter factory, and provides a utility function to run a Runnable in the context of a specific per-thread filter. public class FilterInThread implements BinaryOperator&lt;ObjectInputFilter&gt; { // ThreadLocal to hold the serial filter to be applied private final ThreadLocal&lt;ObjectInputFilter&gt; filterThreadLocal = new ThreadLocal&lt;&gt;(); // Construct a FilterInThread deserialization filter factory. public FilterInThread() {} /** * The filter factory, which is invoked every time a new ObjectInputStream * is created. If a per-stream filter is already set then it returns a * filter that combines the results of invoking each filter. * * @param curr the current filter on the stream * @param next a per stream filter * @return the selected filter */ public ObjectInputFilter apply(ObjectInputFilter curr, ObjectInputFilter next) { if (curr == null) { // Called from the OIS constructor or perhaps OIS.setObjectInputFilter with no current filter var filter = filterThreadLocal.get(); if (filter != null) { // Prepend a filter to assert that all classes have been Allowed or Rejected filter = ObjectInputFilter.rejectUndecidedClass(filter); } if (next != null) { // Prepend the next filter to the thread filter, if any // Initially this is the static JVM-wide filter passed from the OIS constructor // Append the filter to reject all UNDECIDED results filter = ObjectInputFilter.merge(next, filter); filter = ObjectInputFilter.rejectUndecidedClass(filter); } return filter; } else { // Called from OIS.setObjectInputFilter with a current filter and a stream-specific filter. // The curr filter already incorporates the thread filter and static JVM-wide filter // and rejection of undecided classes // If there is a stream-specific filter prepend it and a filter to recheck for undecided if (next != null) { next = ObjectInputFilter.merge(next, curr); next = ObjectInputFilter.rejectUndecidedClass(next); return next; } return curr; } } /** * Apply the filter and invoke the runnable. * * @param filter the serial filter to apply to every deserialization in the thread * @param runnable a Runnable to invoke */ public void doWithSerialFilter(ObjectInputFilter filter, Runnable runnable) { var prevFilter = filterThreadLocal.get(); try { filterThreadLocal.set(filter); runnable.run(); } finally { filterThreadLocal.set(prevFilter); } } } If a stream-specific filter was already set with ObjectInputStream::setObjectFilter then the filter factory combines that filter with the next filter. If either filter rejects a class then that class is rejected. If either filter allows the class then that class is allowed. Otherwise, the result is undecided. Here&#8217;s a simple example of using the FilterInThread class: // Create a FilterInThread filter factory and set var filterInThread = new FilterInThread(); ObjectInputFilter.Config.setSerialFilterFactory(filterInThread); // Create a filter to allow example.* classes and reject all others var filter = ObjectInputFilter.Config.createFilter(\"example.*;java.base/*;!*\"); filterInThread.doWithSerialFilter(filter, () -&gt; { byte[] bytes = ...; var o = deserializeObject(bytes); });",
    "specification": ""
  },
  {
    "number": "JEP 356",
    "title": "Enhanced Pseudo-Random Number Generators",
    "url": "https://openjdk.org/jeps/356",
    "summary": "Provide new interface types and implementations for pseudorandom number generators (PRNGs), including jumpable PRNGs and an additional class of splittable PRNG algorithms (LXM).",
    "goals": "Make it easier to use various PRNG algorithms interchangeably in applications. Better support stream-based programming by providing streams of PRNG objects. Eliminate code duplication in existing PRNG classes. Carefully preserve existing behavior of class java.util.Random.",
    "motivation": "We focus on five areas for improvement in the area of pseudorandom number generators in Java: With the legacy PRNG classes Random, ThreadLocalRandom, and SplittableRandom, it is difficult to replace any one of them in an application with some other algorithm, despite the fact that they all support pretty much the same set of methods. For example, if an application uses instances of class Random, it will necessarily declare variables of type Random, which cannot hold instances of class SplittableRandom; changing the application to use SplittableRandom would require changing the type of every variable (including method parameters) used to hold a PRNG object. The one exception is that ThreadLocalRandom is a subclass of Random, purely to allow variables of type Random to hold instances of ThreadLocalRandom, yet ThreadLocalRandom overrides nearly all the methods of Random. Interfaces can easily address this. Legacy classes Random, ThreadLocalRandom, and SplittableRandom all support such methods as nextDouble() and nextBoolean() as well as stream-producing methods such as ints() and longs(), but they have completely independent and nearly copy-and-paste identical implementations. Refactoring this code would made it easier to maintain and, moreover, documentation would makes it much easier for third parties to create new PRNG classes that also support the same complete suite of methods. In 2016, testing revealed two new weaknesses in the algorithm used by class SplittableRandom. On the one hand, a relatively minor revision can avoid those weaknesses. On the other hand, a new class of splittable PRNG algorithms (LXM) has also been discovered that are almost as fast, even easier to implement, and appear to completely avoid the three classes of weakness to which SplittableRandom is prone. Being able to obtain a stream of PRNG objects from a PRNG makes it much easier to express certain sorts of code using streaming methods. There are many PRNG algorithms in the literature that are not splittable but are jumpable (and perhaps also leapable, that is, capable of very long jumps as well as ordinary jumps), a property quite different from splitting that nevertheless also lends itself to supporting streams of PRNG objects. In the past, it has been difficult to take advantage of this property in Java. Examples of jumpable PRNG algorithms are Xoshiro256**, and Xoroshiro128+. Xoshiro256** and Xoroshiro128+: http://xoshiro.di.unimi.it",
    "description": "We provide a new interface, RandomGenerator, which supplies a uniform API for all existing and new PRNGs. RandomGenerators provide methods named ints, longs, doubles, nextBoolean, nextInt, nextLong, nextDouble, and nextFloat, with all their current parameter variations. We provide four new specialized RandomGenerator interfaces: SplittableRandomGenerator extends RandomGenerator and also provides methods named split and splits. Splittability allows the user to spawn a new RandomGenerator from an existing RandomGenerator that will generally produce statistically independent results. JumpableRandomGenerator extendsRandomGenerator and also provides methods named jump and jumps. Jumpability allows a user to jump ahead a moderate number of draws. LeapableRandomGenerator extends RandomGenerator and also provides methods named leap and leaps. Leapability allows a user to jump ahead a large number of draws. ArbitrarilyJumpableRandomGenerator extends LeapableRandomGenerator and also provides additional variations of jump and jumps that allow an arbitrary jump distance to be specified. We provide a new class RandomGeneratorFactory which is used to locate and construct instances of RandomGenerator implementations. The RandomGeneratorFactory uses the ServiceLoader.Provider API to register RandomGenerator implementations. We have refactored Random, ThreadLocalRandom, and SplittableRandom so as to share most of their implementation code and, furthermore, make that code reusable by other algorithms as well. This refactoring creates underlying non-public abstract classes AbstractRandomGenerator, AbstractSplittableRandomGenerator, AbstractJumpableRandomGenerator, AbstractLeapableRandomGenerator, and AbstractArbitrarilyJumpableRandomGenerator, each provide only implementations for methods nextInt(), nextLong(), and (if relevant) either split(), or jump(), or jump() and leap(), or jump(distance). After this refactoring, Random, ThreadLocalRandom, and SplittableRandom inherit the RandomGenerator interface. Note that because SecureRandom is a subclass of Random, all instances of SecureRandom also automatically support the RandomGenerator interface, with no need to recode the SecureRandom class or any of its associated implementation engines. We also added underlying non-public classes that extend AbstractSplittableRandomGenerator (and therefore implement SplittableRandomGenerator and RandomGenerator) to support six specific members of the LXM family of PRNG algorithms: L32X64MixRandom L32X64StarStarRandom L64X128MixRandom L64X128StarStarRandom L64X256MixRandom L64X1024MixRandom L128X128MixRandom L128X256MixRandom L128X1024MixRandom The structure of the central nextLong (or nextInt) method of an LXM algorithm follows a suggestion in December 2017 by Sebastiano Vigna that using one LCG subgenerator and one xor-based subgenerator (rather than two LCG subgenerators) would provide a longer period, superior equidistribution, scalability, and better quality. Each of the specific implementations here combines one of the best currently known xor-based generators (xoroshiro or xoshiro, described by Blackman and Vigna in \"Scrambled Linear Pseudorandom Number Generators\", ACM Trans. Math. Softw., 2021) with an LCG that uses one of the best currently known multipliers (found by a search for better multipliers in 2019 by Steele and Vigna), and then applies a mixing function identified by Doug Lea. Testing has confirmed that the LXM algorithm is far superior in quality to the SplitMix algorithm (2014) used by SplittableRandom. We also provide implementations of these widely-used PRNG algorithms: Xoshiro256PlusPlus Xoroshiro128PlusPlus The non-public abstract implementations mentioned above may be supplied as part of a random number implementor SPI in the future. This suite of algorithms provide Java programmers with a reasonable range of tradeoffs among space, time, quality, and compatibility with other languages.",
    "specification": ""
  },
  {
    "number": "JEP 392",
    "title": "Packaging Tool",
    "url": "https://openjdk.org/jeps/392",
    "summary": "Provide the jpackage tool, for packaging self-contained Java applications.",
    "goals": "Create a packaging tool, based on the legacy JavaFX javapackager tool, that: Supports native packaging formats to give end users a natural installation experience. These formats include msi and exe on Windows, pkg and dmg on macOS, and deb and rpm on Linux. Allows launch-time parameters to be specified at packaging time. Can be invoked directly, from the command line, or programmatically, via the ToolProvider API.",
    "motivation": "Many Java applications need to be installed on native platforms in a first-class way, rather than simply being placed on the class path or the module path. It is not sufficient for the application developer to deliver a simple JAR file; they must deliver an installable package suitable for the native platform. This allows Java applications to be distributed, installed, and uninstalled in a manner that is familiar to users. For example, on Windows users expect to be able to double-click on a package to install their software, and then use the control panel to remove the software; on macOS, users expect to be able to double-click on a DMG file and drag their application to the Application folder. The jpackage tool can also help fill gaps left by past technologies such as Java Web Start, which was removed from Oracle's JDK 11, and pack200, which was removed in JDK 14 (JEP 367). Developers can use jlink to strip the JDK down to the minimal set of modules that are needed, and then use the packaging tool to produce a compressed, installable image that can be deployed to target machines. To address these requirements previously, a packaging tool called javapackager was distributed with Oracle's JDK 8. However, it was removed from Oracle's JDK 11 as part of the removal of JavaFX.",
    "description": "The jpackage tool packages a Java application into a platform-specific package that includes all of the necessary dependencies. The application may be provided as a collection of ordinary JAR files or as a collection of modules. The supported platform-specific package formats are: Linux: deb and rpm macOS: pkg and dmg Windows: msi and exe By default, jpackage produces a package in the format most appropriate for the system on which it is run. Basic usage: Non-modular applications Suppose you have an application composed of JAR files, all in a directory named lib, and that lib/main.jar contains the main class. Then the command $ jpackage --name myapp --input lib --main-jar main.jar will package the application in the local system's default format, leaving the resulting package file in the current directory. If the MANIFEST.MF file in main.jar does not have a Main-Class attribute then you must specify the main class explicitly: $ jpackage --name myapp --input lib --main-jar main.jar \\ --main-class myapp.Main The name of the package will be myapp, though the name of the package file itself will be longer, and end with the package type (e.g., myapp.exe). The package will include a launcher for the application, also called myapp. To start the application, the launcher will place every JAR file that was copied from the input directory on the class path of the JVM. If you wish to produce a package in a format other than the default, then use the --type option. For example, to produce a pkg file rather than dmg file on macOS: $ jpackage --name myapp --input lib --main-jar main.jar --type pkg Basic usage: Modular applications If you have a modular application, composed of modular JAR files and/or JMOD files in a lib directory, with the main class in the module myapp, then the command $ jpackage --name myapp --module-path lib -m myapp will package it. If the myapp module does not identify its main class then, again, you must specify that explicitly: $ jpackage --name myapp --module-path lib -m myapp/myapp.Main (When creating a modular JAR or a JMOD file you can specify the main class with the --main-class option to the jar and jmod tools.) Package metadata The jpackage tool allows you to specify various kinds of platform independent metadata such as name and app-version, as well as platform-specific metadata for each platform. The description of all jpackage options can be found in the jpackage man page. File associations You can define one or more file-type associations for your application via the --file-associations option, which can be used more than once. The argument to this option is a properties file with values for one or more of the following keys: extension specifies the extension of files to be associated with the application, mime-type specifies the MIME type of files to be associated with the application, icon specifies an icon, within the application image, for use with this association, and description specifies a short description of the association. Launchers By default, the jpackage tool creates a simple native launcher for your application. You can customize the default launcher via the following options: --arguments &lt;string&gt; &#8212; Command-line arguments to pass to the main class if no command line arguments are given to the launcher (this option can be used multiple times) --java-options &lt;string&gt; &#8212; Options to pass to the JVM (this option can be used multiple times) If your application requires additional launchers then you can add them via the --add-launcher option: --add-launcher &lt;launcher-name&gt;=&lt;file&gt; The named &lt;file&gt; should be a properties file with values for one or more of the keys app-version icon arguments java-options main-class main-jar module, or win-console. The values of these keys will be interpreted as arguments to the options of the same name, but with respect to the launcher being created rather than the default launcher. The --add-launcher option can be used multiple times. Application images The jpackage tool constructs an application image as input to the platform-specific packaging tool that it invokes in its final step. Normally this image is a temporary artifact, but sometimes you need to customize it before packaging it. You can, therefore, run the jpackage tool in two steps. First, create the initial application image with the special package type app-image: $ jpackage --name myapp --module-path lib -m myapp --type app-image This will produce an application image in the myapp directory. Customize that image as needed, and then create the final package via the --app-image option: $ jpackage --name myapp --app-image myapp Runtime images An application image contains both the files comprising your application as well as the JDK runtime image that will run your application. By default, the jpackage tool invokes the the jlink tool to create the runtime image. The content of the image depends upon the type of the application: For a non-modular application composed of JAR files, the runtime image contains the same set of JDK modules that is provided to class-path applications in the unnamed module by the regular java launcher. For a modular application composed of modular JAR files and/or JMOD files, the runtime image contains the application's main module and the transitive closure of all of its dependencies. The default set of jlink options used by jpackage is --strip-native-commands --strip-debug --no-man-pages --no-header-files but this can be changed via the --jlink-options option. The resulting image will not include all available service providers; if you want those to be bound then use --jlink-options and include --bind-services in the list of jlink options. In either case, if you want additional modules to be included to the runtime image you can use the jpackage tool's --add-modules option. The list of modules in a runtime image is available in the image's release file. Runtime images created by the jpackage tool do not contain a src.zip file. If you wish to customize the runtime image further then you can invoke jlink yourself and pass the resulting image to the jpackage tool via the --runtime-image option. For example, if you've used the jdeps tool to determine that your non-modular application only needs the java.base and java.sql modules, you could reduce the size of your package significantly: $ jlink --add-modules java.base,java.sql --output myjre $ jpackage --name myapp --input lib --main-jar main.jar --runtime-image myjre Application-image layout and content The layout and content of application images are platform-specific. Actual images contain some files not show in the layouts below; such files are implementation details that are subject to change at any time. Linux myapp/ bin/ // Application launcher(s) myapp lib/ app/ myapp.cfg // Configuration info, created by jpackage myapp.jar // JAR files, copied from the --input directory mylib.jar ... runtime/ // JDK runtime image The default installation directory on Linux is /opt. This can be overridden via the --install-dir option. macOS MyApp.app/ Contents/ Info.plist MacOS/ // Application launcher(s) MyApp Resources/ // Icons, etc. app/ MyApp.cfg // Configuration info, created by jpackage myapp.jar // JAR files, copied from the --input directory mylib.jar ... runtime/ // JDK runtime image The default installation directory on macOS is /Applications. This can be overridden via the --install-dir option. Windows MyApp/ MyApp.exe // Application launcher(s) app/ MyApp.cfg // Configuration info, created by jpackage myapp.jar // JAR files, copied from the --input directory mylib.jar ... runtime/ // JDK runtime image The default installation directory on Windows is C:\\Program Files\\. This can be overridden via the --install-dir option. Delivering jpackage The jpackage tool is delivered in the JDK in a module named jdk.jpackage. The command-line interface conforms to JEP 293 (Guidelines for JDK Command-Line Tool Options). In addition to the command-line interface, jpackage is accessible via the ToolProvider API (java.util.spi.ToolProvider) under the name \"jpackage\".",
    "specification": ""
  },
  {
    "number": "JEP 406",
    "title": "Pattern Matching for switch (Preview)",
    "url": "https://openjdk.org/jeps/406",
    "summary": "Enhance the Java programming language with pattern matching for switch expressions and statements, along with extensions to the language of patterns. Extending pattern matching to switch allows an expression to be tested against a number of patterns, each with a specific action, so that complex data-oriented queries can be expressed concisely and safely. This is a preview language feature in JDK 17.",
    "goals": "Expand the expressiveness and applicability of switch expressions and statements by allowing patterns to appear in case labels. Allow the historical null-hostility of switch to be relaxed when desired. Introduce two new kinds of patterns: guarded patterns, to allow pattern matching logic to be refined with arbitrary boolean expressions, and parenthesized patterns, to resolve some parsing ambiguities. Ensure that all existing switch expressions and statements continue to compile with no changes and execute with identical semantics. Do not introduce a new switch-like expression or statement with pattern-matching semantics that is separate from the traditional switch construct. Do not make the switch expression or statement behave differently when case labels are patterns versus when case labels are traditional constants.",
    "motivation": "In Java 16, JEP 394 extended the instanceof operator to take a type pattern and perform pattern matching. This modest extension allows the familiar instanceof-and-cast idiom to be simplified: // Old code if (o instanceof String) { String s = (String)o; ... use s ... } // New code if (o instanceof String s) { ... use s ... } We often want to compare a variable such as o against multiple alternatives. Java supports multi-way comparisons with switch statements and, since Java&#160;14, switch expressions (JEP 361), but unfortunately switch is very limited. You can only switch on values of a few types &#8212; numeric types, enum types, and String &#8212; and you can only test for exact equality against constants. We might like to use patterns to test the same variable against a number of possibilities, taking a specific action on each, but since the existing switch does not support that, we end up with a chain of if...else tests such as: static String formatter(Object o) { String formatted = \"unknown\"; if (o instanceof Integer i) { formatted = String.format(\"int %d\", i); } else if (o instanceof Long l) { formatted = String.format(\"long %d\", l); } else if (o instanceof Double d) { formatted = String.format(\"double %f\", d); } else if (o instanceof String s) { formatted = String.format(\"String %s\", s); } return formatted; } This code benefits from using pattern instanceof expressions, but it is far from perfect. First and foremost, this approach allows coding errors to remain hidden because we have used an overly general control construct. The intent is to assign something to formatted in each arm of the if...else chain, but there is nothing that enables the compiler to identify and verify this invariant. If some block &#8212; perhaps one that is executed rarely &#8212; does not assign to formatted, we have a bug. (Declaring formatted as a blank local would at least enlist the compiler&#8217;s definite-assignment analysis in this effort, but such declarations are not always written.) In addition, the above code is not optimizable; absent compiler heroics it will have O(n) time complexity, even though the underlying problem is often O(1). But switch is a perfect match for pattern matching! If we extend switch statements and expressions to work on any type, and allow case labels with patterns rather than just constants, then we could rewrite the above code more clearly and reliably: static String formatterPatternSwitch(Object o) { return switch (o) { case Integer i -&gt; String.format(\"int %d\", i); case Long l -&gt; String.format(\"long %d\", l); case Double d -&gt; String.format(\"double %f\", d); case String s -&gt; String.format(\"String %s\", s); default -&gt; o.toString(); }; } The semantics of this switch are clear: A case label with a pattern matches the value of the selector expression o if the value matches the pattern. (We have shown a switch expression for brevity but could instead have shown a switch statement; the switch block, including the case labels, would be unchanged.) The intent of this code is clearer because we are using the right control construct: We are saying, \"the parameter o matches at most one of the following conditions, figure it out and evaluate the corresponding arm.\" As a bonus, it is optimizable; in this case we are more likely to be able to perform the dispatch in O(1) time. Pattern matching and null Traditionally, switch statements and expressions throw NullPointerException if the selector expression evaluates to null, so testing for null must be done outside of the switch: static void testFooBar(String s) { if (s == null) { System.out.println(\"oops!\"); return; } switch (s) { case \"Foo\", \"Bar\" -&gt; System.out.println(\"Great\"); default -&gt; System.out.println(\"Ok\"); } } This was reasonable when switch supported only a few reference types. However, if switch allows a selector expression of any type, and case labels can have type patterns, then the standalone null test feels like an arbitrary distinction, and invites needless boilerplate and opportunity for error. It would be better to integrate the null test into the switch: static void testFooBar(String s) { switch (s) { case null -&gt; System.out.println(\"Oops\"); case \"Foo\", \"Bar\" -&gt; System.out.println(\"Great\"); default -&gt; System.out.println(\"Ok\"); } } The behavior of the switch when the value of the selector expression is null is always determined by its case labels. With a case null (or a total type pattern; see 4a below) the switch executes the code associated with that label; without a case null, the switch throws NullPointerException, just as before. (To maintain backward compatibility with the current semantics of switch, the default label does not match a null selector.) We may wish to handle null in the same way as another case label. For example, in the following code, case null, String s would match both the null value and all String values: static void testStringOrNull(Object o) { switch (o) { case null, String s -&gt; System.out.println(\"String: \" + s); } } Refining patterns in switch Experimentation with patterns in switch suggests it is common to want to refine patterns. Consider the following code that switches over a Shape value: class Shape {} class Rectangle extends Shape {} class Triangle extends Shape { int calculateArea() { ... } } static void testTriangle(Shape s) { switch (s) { case null: break; case Triangle t: if (t.calculateArea() &gt; 100) { System.out.println(\"Large triangle\"); break; } default: System.out.println(\"A shape, possibly a small triangle\"); } } The intent of this code is to have a special case for large triangles (with area over 100), and a default case for everything else (including small triangles). However, we cannot express this directly with a single pattern. We first have to write a case label that matches all triangles, and then place the test of the area of the triangle rather uncomfortably within the corresponding statement group. Then we have to use fall-through to get the correct behavior when the triangle has an area less than 100. (Note the careful placement of break; inside the if block.) The problem here is that using a single pattern to discriminate among cases does not scale beyond a single condition. We need some way to express a refinement to a pattern. One approach might be to allow case labels to be refined; such a refinement is called a guard in other programming languages. For example, we could introduce a new keyword where to appear at the end of a case label and be followed by a boolean expression, e.g., case Triangle t where t.calculateArea() &gt; 100. However, there is a more expressive approach. Rather than extend the functionality of case labels, we can extend the language of patterns themselves. We can add a new kind of pattern called a guarded pattern, written p &amp;&amp; b, that allows a pattern p to be refined by an arbitrary boolean expression b. With this approach, we can revisit the testTriangle code to express the special case for large triangles directly. This eliminates the use of fall-through in the switch statement, which in turn means we can enjoy concise arrow-style (-&gt;) rules: static void testTriangle(Shape s) { switch (s) { case Triangle t &amp;&amp; (t.calculateArea() &gt; 100) -&gt; System.out.println(\"Large triangle\"); default -&gt; System.out.println(\"A shape, possibly a small triangle\"); } } The value of s matches the pattern Triangle t &amp;&amp; (t.calculateArea() &gt; 100) if, first, it matches the type pattern Triangle t and, if so, the expression t.calculateArea() &gt; 100 evaluates to true. Using switch makes it easy to understand and change case labels when application requirements change. For example, we might want to split triangles out of the default path; we can do that by using both a refined pattern and a non-refined pattern: static void testTriangle(Shape s) { switch (s) { case Triangle t &amp;&amp; (t.calculateArea() &gt; 100) -&gt; System.out.println(\"Large triangle\"); case Triangle t -&gt; System.out.println(\"Small triangle\"); default -&gt; System.out.println(\"Non-triangle\"); } }",
    "description": "We enhance switch statements and expressions in two ways: Extend case labels to include patterns in addition to constants, and Introduce two new kinds of patterns: guarded patterns and parenthesized patterns. Patterns in switch labels The heart of the proposal is to introduce a new case p switch label, where p is a pattern. However, the essence of a switch is unchanged: The value of the selector expression is compared to the switch labels, one of the labels is selected, and the code associated with that label is executed. The difference is now that for case labels with patterns, that selection is determined by pattern matching rather than by an equality check. For example, in the following code, the value of o matches the pattern Long l, and the code associated with case Long l will be executed: Object o = 123L; String formatted = switch (o) { case Integer i -&gt; String.format(\"int %d\", i); case Long l -&gt; String.format(\"long %d\", l); case Double d -&gt; String.format(\"double %f\", d); case String s -&gt; String.format(\"String %s\", s); default -&gt; o.toString(); }; There are four major design issues when case labels can have patterns: Enhanced type checking Completeness of switch expressions and statements Scope of pattern variable declarations Dealing with null 1. Enhanced type checking 1a. Selector expression typing Supporting patterns in switch means that we can relax the current restrictions on the type of the selector expression. Currently the type of the selector expression of a normal switch must be either an integral primitive type (char, byte, short, or int), the corresponding boxed form (Character, Byte, Short, or Integer), String, or an enum type. We extend this and require that the type of the selector expression be either an integral primitive type or any reference type. For example, in the following pattern switch the selector expression o is matched with type patterns involving a class type, an enum type, a record type, and an array type (along with a null case label and a default): record Point(int i, int j) {} enum Color { RED, GREEN, BLUE; } static void typeTester(Object o) { switch (o) { case null -&gt; System.out.println(\"null\"); case String s -&gt; System.out.println(\"String\"); case Color c -&gt; System.out.println(\"Color with \" + Color.values().length + \" values\"); case Point p -&gt; System.out.println(\"Record class: \" + p.toString()); case int[] ia -&gt; System.out.println(\"Array of ints of length\" + ia.length); default -&gt; System.out.println(\"Something else\"); } } Every case label in the switch block must be compatible with the selector expression. For a case label with a pattern, known as a pattern label, we use the existing notion of compatibility of an expression with a pattern (JLS &#167;14.30.1). 1b. Dominance of pattern labels It is possible for the selector expression to match multiple labels in a switch block. Consider this problematic example: static void error(Object o) { switch(o) { case CharSequence cs -&gt; System.out.println(\"A sequence of length \" + cs.length()); case String s -&gt; // Error - pattern is dominated by previous pattern System.out.println(\"A string: \" + s); default -&gt; { break; } } } The first pattern label case CharSequence cs dominates the second pattern label case String s because every value that matches the pattern String s also matches the pattern CharSequence cs, but not vice versa. This is because the type of the second pattern, String, is a subtype of the type of the first pattern, CharSequence. A pattern label of the form case p where p is a total pattern for the type of the selector expression dominates a label case null. This is because a total pattern matches all values, including null. A pattern label of the form case p dominates a pattern label of the form case p &amp;&amp; e, i.e., where the pattern is a guarded version of the original pattern. For example, the pattern label case String s dominates the pattern label case String s &amp;&amp; s.length() &gt; 0, since every value that matches the guarded pattern String s &amp;&amp; s.length() &gt; 0 also matches the pattern String s. The compiler checks all pattern labels. It is a compile-time error if a pattern label in a switch block is dominated by an earlier pattern label in that switch block. This dominance requirement ensures that if a switch block contains only type pattern case labels, these will appear in subtype order. The notion of dominance is analogous to conditions on the catch clauses of a try statement, where it is an error if a catch clause that catches an exception class E is preceded by a catch clause that can catch E or a superclass of E (JLS &#167;11.2.3). Logically, the preceding catch clause dominates the subsequent catch clause. It is also a compile-time error if a switch block has more than one match-all switch label. The two match-all labels are default and total type patterns (see 4a below). 2. Completeness of pattern labels in switch expressions and statements A switch expression requires that all possible values of the selector expression are handled in the switch block. This maintains the property that successful evaluation of a switch expression will always yield a value. For normal switch expressions, this is enforced by a fairly straightforward set of extra conditions on the switch block. For pattern switch expressions, we define a notion of type coverage of a switch block. Consider this (erroneous) pattern switch expression: static int coverage(Object o) { return switch (o) { // Error - incomplete case String s -&gt; s.length(); }; } The switch block has only one case label, case String s. This matches any value of the selector expression whose type is a subtype of String. We therefore say that the type coverage of this arrow rule is every subtype of String. This pattern switch expression is incomplete because the type coverage of its switch block does not include the type of the selector expression. Consider this (still erroneous) example: static int coverage(Object o) { return switch (o) { // Error - incomplete case String s -&gt; s.length(); case Integer i -&gt; i; }; } The type coverage of this switch block is the union of the coverage of its two arrow rules. In other words, the type coverage is the set of all subtypes of String and the set of all subtypes of Integer. But, again, the type coverage still does not include the type of the selector expression, so this pattern switch expression is also incomplete and causes a compile-time error. The type coverage of default is all types, so this example is (at last!) legal: static int coverage(Object o) { return switch (o) { case String s -&gt; s.length(); case Integer i -&gt; i; default -&gt; 0; }; } If the type of the selector expression is a sealed class (JEP 409), then the type coverage check can take into account the permits clause of the sealed class to determine whether a switch block is complete. Consider the following example of a sealed interface S with three permitted subclasses A, B, and C: sealed interface S permits A, B, C {} final class A implements S {} final class B implements S {} record C(int i) implements S {} // Implicitly final static int testSealedCoverage(S s) { return switch (s) { case A a -&gt; 1; case B b -&gt; 2; case C c -&gt; 3; }; } The compiler can determine that the type coverage of the switch block is the types A, B, and C. Since the type of the selector expression, S, is a sealed interface whose permitted subclasses are exactly A, B, and C, this switch block is complete. As a result, no default label is needed. To defend against incompatible separate compilation, the compiler automatically adds a default label whose code throws an IncompatibleClassChangeError. This label will only be reached if the sealed interface is changed and the switch code is not recompiled. In effect, the compiler hardens your code for you. The requirement for a pattern switch expression to be complete is analogous to the treatment of a switch expression whose selector expression is an enum class, where a default label is not required if there is a clause for every constant of the enum class. The usefulness of having the compiler verify that switch expressions are complete is extremely useful. Rather than keep this check solely for switch expressions, we extend this to switch statements also. For backwards compatibility reasons, all existing switch statements will compile unchanged. But if a switch statement uses any of the new features detailed in this JEP, then the compiler will check that it is complete. More precisely, completeness is required of switch statements that use pattern or null labels or whose selector expression is not one of the legacy types (char, byte, short, int, Character, Byte, Short, Integer, String, or an enum type). This means that now both switch expressions and switch statements get the benefits of stricter type checking. For example: sealed interface S permits A, B, C {} final class A implements S {} final class B implements S {} record C(int i) implements S {} // Implicitly final static void switchStatementComplete(S s) { switch (s) { // Error - incomplete; missing clause for permitted class B! case A a : System.out.println(\"A\"); break; case C c : System.out.println(\"B\"); break; }; } Making most switch statements complete is simply a matter of adding a simple default clause at the end of the switch body. This leads to clearer and easier to verify code. For example, the following switch statement is not complete and is erroneous: Object o = ... switch (o) { // Error - incomplete! case String s: System.out.println(s); break; case Integer i: System.out.println(\"Integer\"); break; } It can be made complete, trivially: Object o = ... switch (o) { case String s: System.out.println(s); break; case Integer i: System.out.println(\"Integer\"); break; default: // Now complete! break; } It may be the case that future compilers of the Java language will emit warnings for legacy switch statements that are not complete. 3. Scope of pattern variable declarations Pattern variables (JEP 394) are local variables that are declared by patterns. Pattern variable declarations are unusual in that their scope is flow-sensitive. As a recap consider the following example, where the type pattern String s declares the pattern variable s: static void test(Object o) { if ((o instanceof String s) &amp;&amp; s.length() &gt; 3) { System.out.println(s); } else { System.out.println(\"Not a string\"); } } The declaration of s is in scope in the right-hand operand of the &amp;&amp; expression, as well as in the \"then\" block. However, it is not in scope in the \"else\" block; in order for control to transfer to the \"else\" block the pattern match must fail, in which case the pattern variable will not have been initialized. We extend this flow-sensitive notion of scope for pattern variable declarations to encompass pattern declarations occurring in case labels with two new rules: The scope of a pattern variable declaration which occurs in a case label of a switch rule includes the expression, block, or throw statement that appears to the right of the arrow. The scope of a pattern variable declaration which occurs in a case label of a switch labeled statement group, where there are no further switch labels that follow, includes the block statements of the statement group. This example shows the first rule in action: static void test(Object o) { switch (o) { case Character c -&gt; { if (c.charValue() == 7) { System.out.println(\"Ding!\"); } System.out.println(\"Character\"); } case Integer i -&gt; throw new IllegalStateException(\"Invalid Integer argument of value \" + i.intValue()); default -&gt; { break; } } } The scope of the declaration of the pattern variable c is the block to the right of the first arrow. The scope of the declaration of the pattern variable i is the throw statement to the right of the second arrow. The second rule is more complicated. Let us first consider an example where there is only one case label for a switch labeled statement group: static void test(Object o) { switch (o) { case Character c: if (c.charValue() == 7) { System.out.print(\"Ding \"); } if (c.charValue() == 9) { System.out.print(\"Tab \"); } System.out.println(\"character\"); default: System.out.println(); } } The scope of the declaration of the pattern variable c includes all the statements of the statement group, namely the two if statements and the println statement. The scope does not include the statements of the default statement group, even though the execution of the first statement group can fall through the default switch label and execute these statements. The possibility of falling through a case label that declares a pattern variable must be excluded as a compile-time error. Consider this erroneous example: static void test(Object o) { switch (o) { case Character c: if (c.charValue() == 7) { System.out.print(\"Ding \"); } if (c.charValue() == 9) { System.out.print(\"Tab \"); } System.out.println(\"character\"); case Integer i: // Compile-time error System.out.println(\"An integer \" + i); default: break; } } If this were allowed and the value of the selector expression o was a Character, then execution of the switch block could fall through the second statement group (after case Integer i:) where the pattern variable i would not have been initialized. Allowing execution to fall through a case label that declares a pattern variable is therefore a compile-time error. This is why case Character c: case Integer i: ... is not permitted. Similar reasoning applies to the prohibition of multiple patterns in a case label: Neither case Character c, Integer i: ... nor case Character c, Integer i -&gt; ... is allowed. If such case labels were allowed then both c and i would be in scope after the colon or arrow, yet only one of c and i would have been initialized depending on whether the value of o was a Character or an Integer. On the other hand, falling through a label that does not declare a pattern variable is safe, as this example shows: void test(Object o) { switch (o) { case String s: System.out.println(\"A string\"); default: System.out.println(\"Done\"); } } 4. Dealing with null 4a. Matching null Traditionally, a switch throws NullPointerException if the selector expression evaluates to null. This is well-understood behavior and we do not propose to change it for any existing switch code. However, given that there is a reasonable and non-exception-bearing semantics for pattern matching and null values, there is an opportunity to make pattern switch more null-friendly while remaining compatible with existing switch semantics. First, we introduce a new null label for a case, which clearly matches when the value of the selector expression is null. Second, we observe that if a pattern that is total for the type of the selector expression appears a pattern case label, then that label will also match when the value of the selector expression is null. A type pattern p of type U is total for a type T, if T is a subtype of U. For example, the type pattern Object o is total for the type String. We lift the blanket rule that a switch immediately throws NullPointerException if the value of the selector expression is null. Instead, we inspect the case labels to determine the behavior of a switch: If the selector expression evaluates to null then any null case label or a total pattern case label is said to match. If there is no such label associated with the switch block then the switch throws NullPointerException, as before. If the selector expression evaluates to a non-null value then we select a matching case label, as normal. If no case label matches then any match-all label is considered to match. For example, given the declaration below, evaluating test(null) will print null! rather than throw NullPointerException: static void test(Object o) { switch (o) { case null -&gt; System.out.println(\"null!\"); case String s -&gt; System.out.println(\"String\"); default -&gt; System.out.println(\"Something else\"); } } This new behavior around null is as if the compiler automatically enriches the switch block with a case null whose body throws NullPointerException. In other words, this code: static void test(Object o) { switch (o) { case String s -&gt; System.out.println(\"String: \" + s); case Integer i -&gt; System.out.println(\"Integer\"); default -&gt; System.out.println(\"default\"); } } is equivalent to: static void test(Object o) { switch (o) { case null -&gt; throw new NullPointerException(); case String s -&gt; System.out.println(\"String: \"+s); case Integer i -&gt; System.out.println(\"Integer\"); default -&gt; System.out.println(\"default\"); } } In both examples, evaluating test(null) will cause NullPointerException to be thrown. We preserve the intuition from the existing switch construct that performing a switch over null is an exceptional thing to do. The difference in a pattern switch is that you have a mechanism to directly handle this case inside the switch rather than outside. If you choose not to have a null-matching case label in a switch block then switching over a null value will throw NullPointerException, as before. 4b. New label forms arising from null labels Switch blocks in JDK 16 support two styles: one based on labeled groups of statements (the : form) where fallthrough is possible, and one based on single-consequent form (the -&gt; form) where fallthrough is not possible. In the former style, multiple labels are typically written case l1: case l2: whereas in the latter style, multiple labels are written case l1, l2:. Supporting null labels means that a number of special cases can be expressed in the : form. For example: Object o = ... switch(o) { case null: case String s: System.out.println(\"String, including null\"); break; ... } There is an expectation that both : and -&gt; should be equally expressive, and that if case A: case B: is supported in the former style, then case A, B -&gt; should be supported in the latter style. Consequently, the previous example suggests that we should support a case null, String s -&gt; label, as follows: Object o = ... switch(o) { case null, String s -&gt; System.out.println(\"String, including null\"); ... } The value of o matches this label when either it is the null reference, or it is a String. In both cases, the pattern variable s is initialized with the value of o. (The reverse form, case String s, null should also be allowed and behave identically.) It is also meaningful (and not uncommon) to combine a null case with a default label, i.e. Object o = ... switch(o) { ... case null: default: System.out.println(\"The rest (including null)\"); } Again, this should be supported in the -&gt; form. To do so we introduce a new default case label: Object o = ... switch(o) { ... case null, default -&gt; System.out.println(\"The rest (including null)\"); } The value of o matches this label if either it is the null reference value, or no other labels match. Guarded and parenthesized patterns After a successful pattern match we often further test the result of the match. This can lead to cumbersome code, such as: static void test(Object o) { switch (o) { case String s: if (s.length() == 1) { ... } else { ... } break; ... } } The desired test &#8212; that o is a String of length 1 &#8212; is unfortunately split between the case label and the ensuing if statement. We could improve readability if a pattern switch supported the combination of a pattern and a boolean expression in a case label. Rather than add another special case label, we enhance the pattern language by adding guarded patterns, written p &amp;&amp; e. This allows the above code to be rewritten so that all the conditional logic is lifted into the case label: static void test(Object o) { switch (o) { case String s &amp;&amp; (s.length() == 1) -&gt; ... case String s -&gt; ... ... } } The first case matches if o is both a String and of length 1. The second case matches if o is a String of some other length. Sometimes we need to parenthesize patterns to avoid parsing ambiguities. We therefore extend the language of patterns to support parenthesized patterns written (p), where p is a pattern. More precisely, we change the grammar of patterns. Assuming that the record patterns and array patterns of JEP 405 are added, the grammar for patterns will become: Pattern: PrimaryPattern GuardedPattern GuardedPattern: PrimaryPattern &amp;&amp; ConditionalAndExpression PrimaryPattern: TypePattern RecordPattern ArrayPattern ( Pattern ) A guarded pattern is of the form p &amp;&amp; e, where p is a pattern and e is a boolean expression. In a guarded pattern any local variable, formal parameter, or exceptional parameter that is used but not declared in the subexpression must either be final or effectively final. A guarded pattern p &amp;&amp; e introduces the union of the pattern variables introduced by pattern p and expression e. The scope of any pattern variable declaration in p includes the expression e. This allows for patterns such as String s &amp;&amp; (s.length() &gt; 1), which matches a value that can be cast to a String such that the string has a length greater than one. A value matches a guarded pattern p &amp;&amp; e if, first, it matches the pattern p and, second, the expression e evaluates to true. If the value does not match p then no attempt is made to evaluate the expression e. A parenthesized pattern is of the form (p), where p is a pattern. A parenthesized pattern (p) introduces the pattern variables that are introduced by the subpattern p. A value matches a parenthesized pattern (p) if it matches the pattern p. We also change the grammar for instanceof expressions to: InstanceofExpression: RelationalExpression instanceof ReferenceType RelationalExpression instanceof PrimaryPattern This change, and the non-terminal ConditionalAndExpression in the grammar rule for a guarded pattern, ensure that, for example, the expression e instanceof String s &amp;&amp; s.length() &gt; 1 continues to unambiguously parse as the expression (e instanceof String s) &amp;&amp; (s.length() &gt; 1). If the trailing &amp;&amp; is intended to be part of a guarded pattern then the entire pattern should be parenthesized, e.g., e instanceof (String s &amp;&amp; s.length() &gt; 1). The use of the non-terminal ConditionalAndExpression in the grammar rule for a guarded pattern also removes another potential ambiguity concerning a case label with a guarded pattern. For example: boolean b = true; switch (o) { case String s &amp;&amp; b -&gt; s -&gt; s; } If the guard expression of a guarded pattern were allowed to be an arbitrary expression then there would be an ambiguity as to whether the first occurrence of -&gt; is part of a lambda expression or part of the switch rule, whose body is a lambda expression. Since a lambda expression can never be a valid boolean expression, it is safe to restrict the grammar of the guard expression.",
    "specification": ""
  },
  {
    "number": "JEP 412",
    "title": "Foreign Function &amp; Memory API (Incubator)",
    "url": "https://openjdk.org/jeps/412",
    "summary": "Introduce an API by which Java programs can interoperate with code and data outside of the Java runtime. By efficiently invoking foreign functions (i.e., code outside the JVM), and by safely accessing foreign memory (i.e., memory not managed by the JVM), the API enables Java programs to call native libraries and process native data without the brittleness and danger of JNI.",
    "goals": "Ease of use &#8212; Replace the Java Native Interface (JNI) with a superior, pure-Java development model. Performance &#8212; Provide performance that is comparable to, if not better than, existing APIs such as JNI and sun.misc.Unsafe. Generality &#8212; Provide ways to operate on different kinds of foreign memory (e.g., native memory, persistent memory, and managed heap memory) and, over time, to accommodate other platforms (e.g., 32-bit x86) and foreign functions written in languages other than C (e.g., C++, Fortran). Safety &#8212; Disable unsafe operations by default, allowing them only after explicit opt-in from application developers or end users.",
    "motivation": "The Java Platform has always offered a rich foundation to library and application developers who wish to reach beyond the JVM and interact with other platforms. Java APIs expose non-Java resources in a convenient and reliable fashion, whether to access remote data (JDBC), invoke web services (HTTP client), serve remote clients (NIO channels), or communicate with local processes (Unix-domain sockets). Unfortunately, Java developers still face significant obstacles in accessing an important kind of non-Java resource: code and data on the same machine as the JVM, but outside the Java runtime. Foreign memory Data stored in memory outside the Java runtime is referred to as off-heap data. (The heap is where Java objects live &#8212; on-heap data &#8212; and where garbage collectors do their work.) Accessing off-heap data is critical for the performance of popular Java libraries such as Tensorflow, Ignite, Lucene, and Netty, primarily because it lets them avoid the cost and unpredictability associated with garbage collection. It also allows data structures to be serialized and deserialized by mapping files into memory via, e.g., mmap. However, the Java Platform does not today provide a satisfactory solution for accessing off-heap data. The ByteBuffer API allows for the creation of direct byte buffers that are allocated off-heap, but their maximum size is two gigabytes and they are not deallocated promptly. These and other limitations stem from the fact that the ByteBuffer API was designed not only for off-heap memory access but also for producer/consumer exchanges of bulk data in areas such as charset encoding/decoding and partial I/O operations. In that context it has not been possible to satisfy the many requests for off-heap enhancements filed over the years (e.g., 4496703, 6558368, 4837564, and 5029431). The sun.misc.Unsafe API exposes memory access operations for on-heap data that also work for off-heap data. Using Unsafe is efficient because its memory access operations are defined as HotSpot JVM intrinsics and optimized by the JIT compiler. However, using Unsafe is dangerous because it allows access to any memory location. This means that a Java program can crash the JVM by accessing an already-freed location; for this and other reasons, the use of Unsafe has always been strongly discouraged. Using JNI to call a native library which then accesses off-heap data is possible, but the performance overhead seldom makes it applicable: Going from Java to native is several orders of magnitude slower than accessing memory because JNI method calls do not benefit from many common JIT optimizations such as inlining. In summary, when it comes to accessing off-heap data, Java developers face a dilemma: Should they choose a safe but inefficient path (ByteBuffer) or should they abandon safety in favor of performance (Unsafe)? What they require is a supported API for accessing off-heap data (i.e., foreign memory) designed from the ground up to be safe and with JIT optimizations in mind. Foreign functions JNI has supported the invocation of native code (i.e., foreign functions) since Java 1.1, but it is inadequate for many reasons. JNI involves several tedious artifacts: a Java API (native methods), a C header file derived from the Java API, and a C implementation that calls the native library of interest. Java developers must work across multiple toolchains to keep platform-dependent artifacts in sync, which is especially burdensome when the native library evolves rapidly. JNI can only interoperate with libraries written in languages, typically C and C++, that use the calling convention of the operating system and CPU for which the JVM was built. A native method cannot be used to invoke a function written in a language that uses a different convention. JNI does not reconcile the Java type system with the C type system. Aggregate data in Java is represented with objects, but aggregate data in C is represented with structs, so any Java object passed to a native method must be laboriously unpacked by native code. For example, consider a record class Person in Java: Passing a Person object to a native method will require the native code to use JNI's C API to extract fields (e.g., firstName and lastName) from the object. As a result, Java developers sometimes flatten their data into a single object (e.g., a byte array or a direct byte buffer) but more often, since passing Java objects via JNI is slow, they use the Unsafe API to allocate off-heap memory and pass its address to a native method as a long &#8212; which makes the Java code tragically unsafe! Over the years, numerous frameworks have emerged to fill the gaps left by JNI, including JNA, JNR and JavaCPP. While these frameworks are often a marked improvement over JNI, the situation is still less than ideal, especially when compared with languages which offer first-class native interoperation. For example, Python's ctypes package can dynamically wrap functions in native libraries without any glue code. Other languages, such as Rust, provide tools which mechanically derive native wrappers from C/C++ header files. Ultimately, Java developers should have a supported API that lets them straightforwardly consume any native library deemed useful for a particular task, without the tedious glue and clunk of JNI. An excellent abstraction to build upon is method handles, introduced in Java 7 to support fast dynamic languages on the JVM. Exposing native code via method handles would radically simplify the task of writing, building, and distributing Java libraries which depend upon native libraries. Furthermore, an API capable of modeling foreign functions (i.e., native code) and foreign memory (i.e., off-heap data) would provide a solid foundation for third-party native interoperation frameworks.",
    "description": "The Foreign Function &amp; Memory API (FFM API) defines classes and interfaces so that client code in libraries and applications can Allocate foreign memory(MemorySegment, MemoryAddress, and SegmentAllocator), Manipulate and access structured foreign memory(MemoryLayout, MemoryHandles, and MemoryAccess), Manage the lifecycle of foreign resources (ResourceScope), and Call foreign functions (SymbolLookup and CLinker). The FFM API resides in the jdk.incubator.foreign package of the jdk.incubator.foreign module. Example As a brief example of using the FFM API, here is Java code that obtains a method handle for a C library function radixsort and then uses it to sort four strings which start life in a Java array (a few details are elided): // 1. Find foreign function on the C library path MethodHandle radixSort = CLinker.getInstance().downcallHandle( CLinker.systemLookup().lookup(\"radixsort\"), ...); // 2. Allocate on-heap memory to store four strings String[] javaStrings = { \"mouse\", \"cat\", \"dog\", \"car\" }; // 3. Allocate off-heap memory to store four pointers MemorySegment offHeap = MemorySegment.allocateNative( MemoryLayout.ofSequence(javaStrings.length, CLinker.C_POINTER), ...); // 4. Copy the strings from on-heap to off-heap for (int i = 0; i &lt; javaStrings.length; i++) { // Allocate a string off-heap, then store a pointer to it MemorySegment cString = CLinker.toCString(javaStrings[i], newImplicitScope()); MemoryAccess.setAddressAtIndex(offHeap, i, cString.address()); } // 5. Sort the off-heap data by calling the foreign function radixSort.invoke(offHeap.address(), javaStrings.length, MemoryAddress.NULL, '\\0'); // 6. Copy the (reordered) strings from off-heap to on-heap for (int i = 0; i &lt; javaStrings.length; i++) { MemoryAddress cStringPtr = MemoryAccess.getAddressAtIndex(offHeap, i); javaStrings[i] = CLinker.toJavaStringRestricted(cStringPtr); } assert Arrays.equals(javaStrings, new String[] {\"car\", \"cat\", \"dog\", \"mouse\"}); // true This code is far clearer than any solution that uses JNI, since implicit conversions and memory dereferences that would have been hidden behind native method calls are now expressed directly in Java. Modern Java idioms can also be used; for example, streams can allow for multiple threads to copy data between on-heap and off-heap memory in parallel. Memory segments A memory segment is an abstraction that models a contiguous region of memory, located either off-heap or on-heap. Memory segments can be Native segments, allocated from scratch in native memory (e.g., via malloc), Mapped segments, wrapped around a region of mapped native memory (e.g., via mmap), or Array or buffer segments, wrapped around memory associated with existing Java arrays or byte buffers, respectively. All memory segments provide spatial, temporal, and thread-confinement guarantees which are strongly enforced in order to make memory dereference operations safe. For example, the following code allocates 100 bytes off-heap: MemorySegment segment = MemorySegment.allocateNative(100, newImplicitScope()); The spatial bounds of a segment determine the range of memory addresses associated with the segment. The bounds of the segment in the code above are defined by a base address b, expressed as a MemoryAddress instance, and a size in bytes (100), resulting in a range of addresses from b to b + 99, inclusive. The temporal bounds of a segment determine the lifetime of the segment, that is, when the segment will be deallocated. A segment's lifetime and thread-confinement state is modeled by a ResourceScope abstraction, discussed below. The resource scope in the code above is a new implicit scope, which ensures that the memory associated with this segment is freed when the MemorySegment object is deemed unreachable by the garbage collector. The implicit scope also ensures that the memory segment is accessible from multiple threads. In other words, the code above creates a segment whose behavior closely matches that of a ByteBuffer allocated with the allocateDirect factory. The FFM API also supports deterministic memory release and other thread-confinement options, discussed below. Dereferencing memory segments Dereferencing the memory associated with a segment is achieved by obtaining a var handle, an abstraction for data access introduced in Java 9. In particular, a segment is dereferenced with a memory-access var handle. This kind of var handle uses a pair of access coordinates: A coordinate of type MemorySegment &#8212; the segment whose memory is to be dereferenced, and A coordinate of type long &#8212; the offset, from the segment's base address, at which dereference occurs. Memory-access var handles are obtained via factory methods in the MemoryHandles class. For example, this code obtains a memory-access var handle that can write an int value into a native memory segment, and uses it to write 25 four-byte values at consecutive offsets: MemorySegment segment = MemorySegment.allocateNative(100, newImplicitScope()); VarHandle intHandle = MemoryHandles.varHandle(int.class, ByteOrder.nativeOrder()); for (int i = 0; i &lt; 25; i++) { intHandle.set(segment, /* offset */ i * 4, /* value to write */ i); } More advanced access idioms can be expressed by combining memory-access var handles using one or more of the combinator methods provided by the MemoryHandles class. With these a client can, e.g., reorder the coordinates of a given memory-access var handle, drop one or more coordinates, and insert new coordinates. This allows the creation of memory access var handles which accept one or more logical indices into a multi-dimensional array backed by a flat off-heap memory region. To make the FFM API more approachable, the MemoryAccess class provides static accessors to dereference memory segments without the need to construct memory-access var handles. For example, there is an accessor to set an int value in a segment at a given offset, allowing the code above to be simplified to: MemorySegment segment = MemorySegment.allocateNative(100, newImplicitScope()); for (int i = 0; i &lt; 25; i++) { MemoryAccess.setIntAtOffset(segment, i * 4, i); } Memory layouts To reduce the need for tedious calculations about memory layout (e.g., i * 4 in the example above), a MemoryLayout can be used to describe the content of a memory segment in a more declarative fashion. For example, the desired layout of the native memory segment in the examples above can be described in the following way: SequenceLayout intArrayLayout = MemoryLayout.sequenceLayout(25, MemoryLayout.valueLayout(32, ByteOrder.nativeOrder())); This creates a sequence memory layout in which a 32-bit value layout (a layout describing a single 32-bit value) is repeated 25 times. Given a memory layout, we can avoid calculating offsets in our code and simplify both memory allocation and the creation of memory-access var handles: MemorySegment segment = MemorySegment.allocateNative(intArrayLayout, newImplicitScope()); VarHandle indexedElementHandle = intArrayLayout.varHandle(int.class, PathElement.sequenceElement()); for (int i = 0; i &lt; intArrayLayout.elementCount().getAsLong(); i++) { indexedElementHandle.set(segment, (long) i, i); } The intArrayLayout object drives the creation of the memory-access var handle through the creation of a layout path, which is used to select a nested layout from a complex layout expression. The intArrayLayout object also drives the allocation of the native memory segment, which is based upon size and alignment information derived from the layout. The loop constant in the previous examples, 25, has been replaced with the sequence layout's element count. Resource scopes All of the memory segments seen in the previous examples use non-deterministic deallocation: The memory associated with these segments is deallocated by the garbage collector, once the memory segment instance becomes unreachable. We say that such segments are implicitly deallocated. There are cases where the client might want to control when memory deallocation occurs. Suppose, e.g., that a large memory segment is mapped from a file using MemorySegment::map. The client might prefer to release (i.e., unmap) the memory associated with the segment as soon as the segment is no longer required rather than wait for the garbage collector to do so, since waiting could adversely affect the application's performance. Memory segments support deterministic deallocation through resource scopes. A resource scope models the lifecycle associated with one or more resources, such as memory segments. A newly-created resource scope is in the alive state, which means that all the resources it manages can be safely accessed. At the client's request a resource scope can be closed, which means that access to the resources managed by the scope is no longer allowed. The ResourceScope class implements the AutoCloseable interface so that resource scopes work with the try-with-resources statement: try (ResourceScope scope = ResourceScope.newConfinedScope()) { MemorySegment s1 = MemorySegment.map(Path.of(\"someFile\"), 0, 100000, MapMode.READ_WRITE, scope); MemorySegment s2 = MemorySegment.allocateNative(100, scope); ... } // both segments released here This code creates a confined resource scope and uses it in the creation of two segments: a mapped segment (s1) and a native segment (s2). The lifecycle of the two segments is tied to the lifetime of the resource scope, so accessing the segments (e.g., dereferencing them with memory-access var handles) after the try-with-resources statement has completed will cause a runtime exception to be thrown. In addition to managing a memory segment's lifetime, a resource scope also serves as a means to control which threads can access the segment. A confined resource scope restricts access to the thread which created the scope, whereas a shared resource scope allows access from any thread. A resource scope, whether confined or shared, may be associated with a java.lang.ref.Cleaner object that takes care of performing implicit deallocation in case the resource scope object becomes unreachable before the close method is called by the client. Some resource scopes, referred to as implicit resource scopes, do not support explicit deallocation &#8212; calling close will fail. Implicit resource scopes always manage their resources using a Cleaner. Implicit scopes can be created using the ResourceScope::newImplicitScope factory, as shown in earlier examples. Segment allocators Memory allocation can often be a bottleneck when clients use off-heap memory. The FFM API includes a SegmentAllocator abstraction, which defines useful operations to allocate and initialize memory segments. Segment allocators are obtained via factories in the SegmentAllocator interface. For example, the following code creates an arena-based allocator and uses it to allocate a segment whose content is initialized from a Java int array: try (ResourceScope scope = ResourceScope.newConfinedScope()) { SegmentAllocator allocator = SegmentAllocator.arenaAllocator(scope); for (int i = 0 ; i &lt; 100 ; i++) { MemorySegment s = allocator.allocateArray(C_INT, new int[] { 1, 2, 3, 4, 5 }); ... } ... } // all memory allocated is released here This code creates a confined resource scope and then creates an unbounded arena allocator associated with that scope. This allocator will allocate slabs of memory, of a specific size, and respond to allocation requests by returning different slices of the pre-allocated slab. If a slab does not have sufficient space to accommodate a new allocation request, a new slab is allocated. If the resource scope associated with the arena allocator is closed, all memory associated with the segments created by the allocator (i.e., in the body of the for loop) is deallocated atomically. This idiom combines the advantages of deterministic deallocation, provided by the ResourceScope abstraction, with a more flexible and scalable allocation scheme. It can be very useful when writing code which manages a large number of off-heap segments. Unsafe memory segments So far, we have seen memory segments, memory addresses, and memory layouts. Dereference operations are only possible on memory segments. Since a memory segment has spatial and temporal bounds, the Java runtime can always ensure that memory associated with a given segment is dereferenced safely. However, there are situations where clients might only have a MemoryAddress instance, as is often the case when interacting with native code. Since the Java runtime has no way to know the spatial and temporal bounds associated with a memory address, directly dereferencing memory addresses is forbidden by the FFM API. To dereference a memory address, a client has two options. If the address is known to fall within a memory segment, the client can perform a rebase operation via MemoryAddress::segmentOffset. The rebasing operation re-interprets the address's offset relative to the segment's base address to yield a new offset which can be applied to the existing segment &#8212; which can then be safely dereferenced. Alternatively, if no such segment exists then the client can create one unsafely, using the MemoryAddress::asSegment factory. This factory effectively attaches fresh spatial and temporal bounds to an otherwise raw memory address, so as to allow dereference operations. The memory segment returned by this factory is unsafe: A raw memory address might be associated with a memory region that is 10 bytes long, but the client might accidentally overestimate the size of the region and create an unsafe memory segment that is 100 bytes long. This might result, later, in attempts to dereference memory outside the bounds of the memory region associated with the unsafe segment, which might cause a JVM crash or, worse, result in silent memory corruption. For this reason, creating unsafe segments is regarded as a restricted operation, and is disabled by default (see more below). Looking up foreign functions The first ingredient of any support for foreign functions is a mechanism to load native libraries. With JNI, this is accomplished with the System::loadLibrary and System::load methods, which internally map into calls to dlopen or its equivalent. Libraries loaded using these methods are always associated with a class loader (namely, the loader of the class which called the System method). The association between libraries and class loaders is crucial because it governs the lifecycle of loaded libraries: only when a class loader is no longer reachable, can all its libraries be unloaded safely. The FFM API does not provide new methods for loading native libraries. Developers use the System::loadLibrary and System::load methods to load native libraries that will invoked via the FFM API. The association between libraries and class loaders is preserved, so libraries will be unloaded in the same predictable manner as with JNI. The FFM API, unlike JNI, provides the capability to find the address of a given symbol in a loaded library. This capability, represented by a SymbolLookup object, is crucial for linking Java code to foreign functions (see below). There are two ways to obtain a SymbolLookup object: SymbolLookup::loaderLookup returns a symbol lookup which sees all the symbols in all the libraries that were loaded by the current class loader. CLinker::systemLookup returns a platform-specific symbol lookup which sees symbols in the standard C library. Given a symbol lookup, a client can find a foreign function with the SymbolLookup::lookup(String) method. If the named function is present among the symbols seen by the symbol lookup, then the method returns a MemoryAddress which points to the entry point of the function. For example, the following code loads the OpenGL library (causing it to be associated with the current class loader) and finds the address of its glGetString function: System.loadLibrary(\"GL\"); SymbolLookup loaderLookup = SymbolLookup.loaderLookup(); MemoryAddress clangVersion = loaderLookup.lookup(\"glGetString\").get(); Linking Java code to foreign functions The CLinker interface is the core of how Java code interoperates with native code. While the CLinker is focused on providing interoperation between Java and C libraries, the concepts in the interface are general enough to support other, non-Java languages in future. The interface enables both downcalls (calls from Java code to native code) and upcalls (calls from native code back to Java code). interface CLinker { MethodHandle downcallHandle(MemoryAddress func, MethodType type, FunctionDescriptor function); MemoryAddress upcallStub(MethodHandle target, FunctionDescriptor function, ResourceScope scope); } For downcalls, the downcallHandle method takes the address of a foreign function &#8212; typically, a MemoryAddress obtained from a library lookup &#8212; and exposes the foreign function as a downcall method handle. Later, Java code invokes the downcall method handle by calling its invokeExact method, and the foreign function runs. Any arguments passed to the method handle's invokeExact method are passed on to the foreign function. For upcalls, the upcallStub method takes a method handle &#8212; typically, one which refers to a Java method, rather than a downcall method handle &#8212; and converts it to a memory address. Later, the memory address is passed as an argument when Java code invokes a downcall method handle. In effect, the memory address serves as a function pointer. (For more information on upcalls, see below.) Suppose we wish to downcall from Java to the strlen function defined in the standard C library: size_t strlen(const char *s); A downcall method handle that exposes strlen can be obtained as follows (the details of MethodType and FunctionDescriptor will be described shortly): MethodHandle strlen = CLinker.getInstance().downcallHandle( CLinker.systemLookup().lookup(\"strlen\").get(), MethodType.methodType(long.class, MemoryAddress.class), FunctionDescriptor.of(C_LONG, C_POINTER) ); Invoking the downcall method handle will run strlen and make its result available in Java. For the argument to strlen, we use a helper method to convert a Java string into an off-heap memory segment and pass that segment's address: MemorySegment str = CLinker.toCString(\"Hello\", newImplicitScope()); long len = strlen.invokeExact(str.address()); // 5 Method handles work well for exposing foreign functions because the JVM already optimizes the invocation of method handles all the way down to native code. When a method handle refers to a method in a class file, invoking the method handle typically causes the target method to be JIT-compiled; subsequently, the JVM interprets the Java bytecode that calls MethodHandle::invokeExact by transferring control to the assembly code generated for the target method. Thus, invoking a traditional method handle is already a quasi-foreign invocation; a downcall method handle that targets a function in a C library is just a more-foreign form of method handle. Method handles also enjoy a property called signature polymorphism that allows box-free invocation with primitive arguments. In sum, method handles let the CLinker expose foreign functions in a natural, efficient, and extensible manner. Describing C types in Java To create a downcall method handle, the FFM API requires the client to provide a two-sided view of the target C function: a high-level signature using opaque Java objects (MemoryAddress, MemorySegment), and a low-level signature using transparent Java objects (MemoryLayout). Taking each signature in turn: The high-level signature, a MethodType, serves as the type of the downcall method handle. Every method handle is strongly typed, which means it is stringent about the number and types of the arguments that can be passed to its invokeExact method. For example, a method handle created to take one MemoryAddress argument cannot be invoked via invokeExact(&lt;MemoryAddress&gt;, &lt;MemoryAddress&gt;) or via invokeExact(\"Hello\"). Thus, the MethodType describes the Java signature which clients must use when invoking the downcall method handle. It is, effectively, the Java view of the C function. The low-level signature, a FunctionDescriptor, consists of MemoryLayout objects. This gives the CLinker a precise understanding of the C function&#8217;s arguments so that it can arrange them properly, as described below. Clients usually have MemoryLayout objects on hand in order to dereference data in foreign memory, and such objects can be reused here as foreign function signatures. As an example, obtaining a downcall method handle for a C function which takes an int and returns a long would require the following MethodType and FunctionDescriptor arguments to downcallHandle: MethodType mtype = MethodType.methodType(long.class, int.class); FunctionDescriptor fdesc = FunctionDescriptor.of(C_LONG, C_INT); (This example targets Linux/x64 and macOS/x64, where the Java types long and int are associated with the predefined CLinker layouts C_LONG and C_INT respectively. The association of Java types with memory layouts varies by platform; for example, on Windows/x64, a Java long is associated with the C_LONG_LONG layout.) As another example, obtaining a downcall method handle for a void C function which takes a pointer would require the following MethodType and FunctionDescriptor: MethodType mtype = MethodType.methodType(void.class, MemoryAddress.class); FunctionDescriptor fdesc = FunctionDescriptor.ofVoid(C_POINTER); (All pointer types in C are expressed as MemoryAddress objects in Java; the corresponding layout, whose size depends on the current platform, is C_POINTER. Clients do not distinguish between, e.g., int* and char**, because the Java types and memory layouts passed to the CLinker jointly contain enough information to pass Java arguments correctly to the C function.) Finally, unlike JNI, the CLinker supports passing structured data to foreign functions. Obtaining a downcall method handle to a void C function which takes a struct would require the following MethodType and FunctionDescriptor: MethodType mtype = MethodType.methodType(void.class, MemorySegment.class); MemoryLayout SYSTEMTIME = MemoryLayout.ofStruct( C_SHORT.withName(\"wYear\"), C_SHORT.withName(\"wMonth\"), C_SHORT.withName(\"wDayOfWeek\"), C_SHORT.withName(\"wDay\"), C_SHORT.withName(\"wHour\"), C_SHORT.withName(\"wMinute\"), C_SHORT.withName(\"wSecond\"), C_SHORT.withName(\"wMilliseconds\") ); FunctionDescriptor fdesc = FunctionDescriptor.ofVoid(SYSTEMTIME); (For the high-level MethodType signature, the Java client always uses the opaque type MemorySegment where a C function expects a struct passed by value. For the low-level FunctionDescriptor signature, the memory layout associated with a C struct type must be a composite layout which defines the sub-layouts for all the fields in the C struct, including padding which might be inserted by a native compiler.) If a C function returns a by-value struct, as expressed by the low-level signature, then a fresh memory segment must be allocated off-heap and returned to the Java client. To achieve this, the method handle returned by downcallHandle requires an additional SegmentAllocator argument, which the FFM API uses to allocate a memory segment to hold the struct returned by the C function. Packaging Java arguments for C functions Interoperation between different languages requires a calling convention to specify how code in one language invokes a function in another language, how it passes arguments, and how it receives any results. The CLinker implementation has knowledge of several calling conventions out-of-the-box: Linux/x64, Linux/AArch64, macOS/x64, and Windows/x64. Being written in Java, it is far easier to maintain and extend than JNI, whose calling conventions are hardwired into HotSpot's C++ code. Consider the function descriptor shown above for the SYSTEMTIME struct and layout. Given the calling convention of the OS and CPU where the JVM is running, the CLinker uses the function descriptor to infer how the struct's fields should be passed to the C function when a downcall method handle is invoked with a MemorySegment argument. For one calling convention, the CLinker could arrange to decompose the incoming memory segment, pass the first four fields using general CPU registers, and pass the remaining fields on the C stack. For different calling convention, the CLinker could arrange for the FFM API to pass the struct indirectly by allocating a region of memory, bulk-copying the contents of the incoming memory segment into that region, and passing a pointer to that memory region to the C function. This lowest-level packaging of arguments happens behind the scenes, without any need for supervision by client code. Upcalls Sometimes it is useful to pass Java code as a function pointer to some foreign function. We can achieve that by using the CLinker support for upcalls. In this section we build, piece by piece, a more sophisticated example which demonstrates the full power of the CLinker, with full bidirectional interoperation of both code and data across the Java/native boundary. Consider the following function defined in the standard C library: void qsort(void *base, size_t nmemb, size_t size, int (*compar)(const void *, const void *)); To call qsort from Java, we first need to create a downcall method handle: MethodHandle qsort = CLinker.getInstance().downcallHandle( CLinker.systemLookup().lookup(\"qsort\").get(), MethodType.methodType(void.class, MemoryAddress.class, long.class, long.class, MemoryAddress.class), FunctionDescriptor.ofVoid(C_POINTER, C_LONG, C_LONG, C_POINTER) ); As before, we use C_LONG and long.class to map the C size_t type, and we use MemoryAddess.class both for the first pointer parameter (the array pointer) and the last parameter (the function pointer). qsort sorts the contents of an array using a custom comparator function, compar, passed as a function pointer. Therefore, to invoke the downcall method handle, we need a function pointer to pass as the last parameter to the method handle's invokeExact method. CLinker::upcallStub helps us create function pointers by using existing method handles, as follows. First, we write a static method in Java that compares two long values, represented indirectly as MemoryAddress objects: class Qsort { static int qsortCompare(MemoryAddress addr1, MemoryAddress addr2) { return MemoryAccess.getIntAtOffset(MemorySegment.globalNativeSegment(), addr1.toRawLongValue()) - MemoryAccess.getIntAtOffset(MemorySegment.globalNativeSegment(), addr2.toRawLongValue()); } } Second, we create a method handle pointing to the Java comparator method: MethodHandle comparHandle = MethodHandles.lookup() .findStatic(Qsort.class, \"qsortCompare\", MethodType.methodType(int.class, MemoryAddress.class, MemoryAddress.class)); Third, now that we have a method handle for our Java comparator, we can create a function pointer using CLinker::upcallStub. Just as for downcalls, we describe the signature of the function pointer using layouts in the CLinker class: MemoryAddress comparFunc = CLinker.getInstance().upcallStub(comparHandle, FunctionDescriptor.of(C_INT, C_POINTER, C_POINTER), newImplicitScope()); ); We finally have a memory address, comparFunc, which points to a stub that can be used to invoke our Java comparator function, and so we now have all we need to invoke the qsort downcall handle: MemorySegment array = MemorySegment.allocateNative(4 * 10, newImplicitScope()); array.copyFrom(MemorySegment.ofArray(new int[] { 0, 9, 3, 4, 6, 5, 1, 8, 2, 7 })); qsort.invokeExact(array.address(), 10L, 4L, comparFunc); int[] sorted = array.toIntArray(); // [ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 ] This code creates an off-heap array, copies the contents of a Java array into it, and then passes the array to the qsort handle along with the comparator function we obtained from the CLinker. After the invocation the contents of the off-heap array will be sorted according to our comparator function, written in Java. We then extract a new Java array from the segment, which contains the sorted elements. Safety Fundamentally, any interaction between Java code and native code can compromise the integrity of the Java Platform. Linking to a C function in a precompiled library is inherently unreliable because the Java runtime cannot guarantee that the function's signature matches the expectations of the Java code, or even that a symbol in a C library is really a function. Moreover, if a suitable function is linked, actually calling the function can lead to low-level failures, such as segmentation faults, that end up crashing the VM. Such failures cannot be prevented by the Java runtime or caught by Java code. Native code that uses JNI functions is especially dangerous. Such code can access JDK internals without command-line flags (e.g., --add-opens), by using functions such as getStaticField and callVirtualMethod. It can also change the values of final fields long after they are initialized. Allowing native code to bypass the checks applied to Java code undermines every boundary and assumption in the JDK. In other words, JNI is inherently unsafe. JNI cannot be disabled, so there is no way to ensure that Java code will not call native code which uses dangerous JNI functions. This is a risk to platform integrity that is almost invisible to application developers and end users because 99% of the use of these functions is typically from third, fourth, and fifth-party libraries sandwiched between the application and the JDK. Most of the FFM API is safe by design. Many scenarios that required the use of JNI and native code in the past can be accomplished by calling methods in the FFM API which cannot compromise the Java Platform. For example, a primary use case for JNI, flexible memory allocation, is supported with a simple method, MemorySegment::allocateNative, that involves no native code and always returns memory managed by the Java runtime. Generally speaking, Java code that uses the FFM API cannot crash the JVM. Part of the FFM API, however, is inherently unsafe. When interacting with the CLinker, Java code can request a downcall method handle by specifying parameter types that are incompatible with those of the underlying C function. Invoking the downcall method handle in Java will result in the same kind of outcome &#8212; a VM crash, or undefined behavior &#8212; that can occur when invoking a native method in JNI. The FFM API can also produce unsafe segments, that is, memory segments whose spatial and temporal bounds are user-provided and cannot be verified by the Java runtime (see MemoryAddress::asSegment). The unsafe methods in the FFM API do not pose the same risks as JNI functions; they cannot, e.g., change the values of final fields in Java objects. On the other hand, the unsafe methods in the FFM API are easy to call from Java code. For this reason the use of unsafe methods in the FFM API is restricted: Access to unsafe methods is disabled by default, so that invoking such methods throws an IllegalAccessException. To enable access to unsafe methods for code in some module M, specify java --enable-native-access=M on the command line. (Specify multiple modules in a comma-separated list; specify ALL-UNNAMED to enable access for all code on the class path.) Most methods of the FFM API are safe, and Java code can use those methods regardless of whether --enable-native-access is given. We do not propose here to restrict any aspect of JNI. It will still be possible to call native methods in Java, and for native code to call unsafe JNI functions. However, it is likely that we will restrict JNI in some way in a future release. For example, unsafe JNI functions such as newDirectByteBuffer may be disabled by default, just like unsafe methods in the FFM API. More broadly, the JNI mechanism is so irredeemably dangerous that we hope libraries will prefer the pure-Java FFM API for both safe and unsafe operations so that, in time, we can disable all of JNI by default. This aligns with the broader Java roadmap of making the platform safe out-of-the-box, requiring end users to opt in to unsafe activities such as breaking strong encapsulation or linking to unknown code. We do not propose here to change sun.misc.Unsafe in any way. The FFM API's support for off-heap memory is an excellent alternative to the wrappers around malloc and free in sun.misc.Unsafe, namely allocateMemory, setMemory, copyMemory, and freeMemory. We hope that libraries and applications that require off-heap storage adopt the FFM API so that, in time, we can deprecate and then eventually remove these sun.misc.Unsafe methods.",
    "specification": ""
  },
  {
    "number": "JEP 414",
    "title": "Vector API (Second Incubator)",
    "url": "https://openjdk.org/jeps/414",
    "summary": "Introduce an API to express vector computations that reliably compile at runtime to optimal vector instructions on supported CPU architectures, thus achieving performance superior to equivalent scalar computations.",
    "goals": "Clear and concise API &#8212; The API should be capable of clearly and concisely expressing a wide range of vector computations consisting of sequences of vector operations composed within loops and possibly with control flow. It should be possible to express a computation that is generic with respect to vector size, or the number of lanes per vector, thus enabling such computations to be portable across hardware supporting different vector sizes. Platform agnostic &#8212; The API should be CPU architecture agnostic, enabling implementations on multiple architectures supporting vector instructions. As is usual in Java APIs, where platform optimization and portability conflict then the bias will be toward making the API portable, even if that results in some platform-specific idioms not being expressible in portable code. Reliable runtime compilation and performance on x64 and AArch64 architectures &#8212; On capable x64 architectures the Java runtime, specifically the HotSpot C2 compiler, should compile vector operations to corresponding efficient and performant vector instructions, such as those supported by Streaming SIMD Extensions (SSE) and Advanced Vector Extensions (AVX). Developers should have confidence that the vector operations they express will reliably map closely to relevant vector instructions. On capable ARM AArch64 architectures C2 will, similarly, compile vector operations to the vector instructions supported by NEON. Graceful degradation &#8212; Sometimes a vector computation cannot be fully expressed at runtime as a sequence of vector instructions, perhaps because the architecture does not support some of the required instructions. In such cases the Vector API implementation should degrade gracefully and still function. This may involve issuing warnings if a vector computation cannot be efficiently compiled to vector instructions. On platforms without vectors, graceful degradation will yield code competitive with manually-unrolled loops, where the unroll factor is the number of lanes in the selected vector.",
    "motivation": "A vector computation consists of a sequence of operations on vectors. A vector comprises a (usually) fixed sequence of scalar values, where the scalar values correspond to the number of hardware-defined vector lanes. A binary operation applied to two vectors with the same number of lanes would, for each lane, apply the equivalent scalar operation on the corresponding two scalar values from each vector. This is commonly referred to as Single Instruction Multiple Data (SIMD). Vector operations express a degree of parallelism that enables more work to be performed in a single CPU cycle and thus can result in significant performance gains. For example, given two vectors, each containing a sequence of eight integers (i.e., eight lanes), the two vectors can be added together using a single hardware instruction. The vector addition instruction operates on sixteen integers, performing eight integer additions, in the time it would ordinarily take to operate on two integers, performing one integer addition. HotSpot already supports auto-vectorization, which transforms scalar operations into superword operations which are then mapped to vector instructions. The set of transformable scalar operations is limited, and also fragile with respect to changes in code shape. Furthermore, only a subset of the available vector instructions might be utilized, limiting the performance of generated code. Today, a developer who wishes to write scalar operations that are reliably transformed into superword operations needs to understand HotSpot's auto-vectorization algorithm and its limitations in order to achieve reliable and sustainable performance. In some cases it may not be possible to write scalar operations that are transformable. For example, HotSpot does not transform the simple scalar operations for calculating the hash code of an array (thus the Arrays::hashCode methods), nor can it auto-vectorize code to lexicographically compare two arrays (thus we added an intrinsic for lexicographic comparison). The Vector API aims to improve the situation by providing a way to write complex vector algorithms in Java, using the existing HotSpot auto-vectorizer but with a user model which makes vectorization far more predictable and robust. Hand-coded vector loops can express high-performance algorithms, such as vectorized hashCode or specialized array comparisons, which an auto-vectorizer may never optimize. Numerous domains can benefit from this explicit vector API including machine learning, linear algebra, cryptography, finance, and code within the JDK itself.",
    "description": "A vector is represented by the abstract class Vector&lt;E&gt;. The type variable E is instantiated as the boxed type of the scalar primitive integral or floating point element types covered by the vector. A vector also has a shape which defines the size, in bits, of the vector. The shape of a vector governs how an instance of Vector&lt;E&gt; is mapped to a hardware vector register when vector computations are compiled by the HotSpot C2 compiler. The length of a vector, i.e., the number of lanes or elements, is the vector size divided by the element size. The set of element types (E) supported is Byte, Short, Integer, Long, Float and Double, corresponding to the scalar primitive types byte, short, int, long, float and double, respectively. The set of shapes supported correspond to vector sizes of 64, 128, 256, and 512 bits, as well as max bits. A 512-bit shape can pack bytes into 64 lanes or pack ints into 16 lanes, and a vector of such a shape can operate on 64 bytes at a time or 16 ints at a time. A max-bits shape supports the maximum vector size of the current architectures. This enables support for the ARM SVE platform, where platform implementations can support any fixed size ranging from 128 to 2048 bits, in increments of 128 bits. We believe that these simple shapes are generic enough to be useful on all relevant platforms. However, as we experiment with future platforms during the incubation of this API we may further modify the design of the shape parameter. Such work is not in the early scope of this project, but these possibilities partly inform the present role of shapes in the Vector API. (For further discussion see the future work section, below.) The combination of element type and shape determines a vector's species, represented by VectorSpecies&lt;E&gt;. Operations on vectors are classified as either lane-wise or cross-lane. A lane-wise operation applies a scalar operator, such as addition, to each lane of one or more vectors in parallel. A lane-wise operation usually, but not always, produces a vector of the same length and shape. Lane-wise operations are further classified as unary, binary, ternary, test, or conversion operations. A cross-lane operation applies an operation across an entire vector. A cross-lane operation produces either a scalar or a vector of possibly a different shape. Cross-lane operations are further classified as permutation or reduction operations. To reduce the surface of the API, we define collective methods for each class of operation. These methods take operator constants as input; these constants are instances of the VectorOperator.Operator class and are defined in static final fields in the VectorOperators class. For convenience we define dedicated methods, which can be used in place of the generic methods, for some common full-service operations such as addition and multiplication. Certain operations on vectors, such conversion and reinterpretation, are inherently shape-changing; i.e., they produce vectors whose shapes are different from the shapes of their inputs. Shape-changing operations in a vector computation can negatively impact portability and performance. For this reason the API defines a shape-invariant flavor of each shape-changing operation when applicable. For best performance, developers should write shape-invariant code using shape-invariant operations insofar as possible. Shape-changing operations are identified as such in the API specification. The Vector&lt;E&gt; class declares a set of methods for common vector operations supported by all element types. For operations specific to an element type there are six abstract subclasses of Vector&lt;E&gt;, one for each supported element type: ByteVector, ShortVector, IntVector, LongVector, FloatVector, and DoubleVector. These type-specific subclasses define additional operations that are bound to the element type since the method signature refers either to the element type or to the related array type. Examples of such operations include reduction (e.g., summing all lanes to a scalar value), and copying a vector's elements into an array. These subclasses also define additional full-service operations specific to the integral subtypes (e.g., bitwise operations such as logical or), as well as operations specific to the floating point types (e.g., transcendental mathematical functions such as exponentiation). As an implementation matter, these type-specific subclasses of Vector&lt;E&gt; are further extended by concrete subclasses for different vector shapes. These concrete subclasses are not public since there is no need to provide operations specific to types and shapes. This reduces the API surface to a sum of concerns rather than a product. Instances of concrete Vector classes are obtained via factory methods defined in the base Vector&lt;E&gt; class and its type-specific subclasses. These factories take as input the species of the desired vector instance and produce various kinds of instances, for example the vector instance whose elements are default values (i.e., the zero vector), or a vector instance initialized from a given array. To support control flow, some vector operations optionally accept masks represented by the public abstract class VectorMask&lt;E&gt;. Each element in a mask is a boolean value corresponding to a vector lane. A mask selects the lanes to which an operation is applied: It is applied if the mask element for the lane is true, and some alternative action is taken if the mask is false. Similar to vectors, instances of VectorMask&lt;E&gt; are instances of non-public concrete subclasses defined for each element type and length combination. The instance of VectorMask&lt;E&gt; used in an operation should have the same type and length as the vector instances involved in the operation. Vector comparison operations produce masks, which can then be used as input to other operations to selectively operate on certain lanes and thereby emulate flow control. Masks can also be created using static factory methods in the VectorMask&lt;E&gt; class. We anticipate that masks will play an important role in the development of vector computations that are generic with respect to shape. This expectation is based on the central importance of predicate registers, the equivalent of masks, in the ARM Scalable Vector Extensions and in Intel's AVX-512. Example Here is a simple scalar computation over elements of arrays: void scalarComputation(float[] a, float[] b, float[] c) { for (int i = 0; i &lt; a.length; i++) { c[i] = (a[i] * a[i] + b[i] * b[i]) * -1.0f; } } (We assume that the array arguments are of the same length.) Here is an equivalent vector computation, using the Vector API: static final VectorSpecies&lt;Float&gt; SPECIES = FloatVector.SPECIES_PREFERRED; void vectorComputation(float[] a, float[] b, float[] c) { int i = 0; int upperBound = SPECIES.loopBound(a.length); for (; i &lt; upperBound; i += SPECIES.length()) { // FloatVector va, vb, vc; var va = FloatVector.fromArray(SPECIES, a, i); var vb = FloatVector.fromArray(SPECIES, b, i); var vc = va.mul(va) .add(vb.mul(vb)) .neg(); vc.intoArray(c, i); } for (; i &lt; a.length; i++) { c[i] = (a[i] * a[i] + b[i] * b[i]) * -1.0f; } } To start, we obtain a preferred species whose shape is optimal for the current architecture from FloatVector. We store it in a static final field so that the runtime compiler treats the value as constant and can therefore better optimize the vector computation. The main loop then iterates over the input arrays in strides of the vector length, i.e., the species length. It loads float vectors of the given species from arrays a and b at the corresponding index, fluently performs the arithmetic operations, and then stores the result into array c. If any array elements are left over after the last iteration then the results for those tail elements are computed with an ordinary scalar loop. This implementation achieves optimal performance on large arrays. The HotSpot C2 compiler generates machine code similar to the following on an Intel x64 processor supporting AVX: 0.43% / &#9474; 0x0000000113d43890: vmovdqu 0x10(%r8,%rbx,4),%ymm0 7.38% &#9474; &#9474; 0x0000000113d43897: vmovdqu 0x10(%r10,%rbx,4),%ymm1 8.70% &#9474; &#9474; 0x0000000113d4389e: vmulps %ymm0,%ymm0,%ymm0 5.60% &#9474; &#9474; 0x0000000113d438a2: vmulps %ymm1,%ymm1,%ymm1 13.16% &#9474; &#9474; 0x0000000113d438a6: vaddps %ymm0,%ymm1,%ymm0 21.86% &#9474; &#9474; 0x0000000113d438aa: vxorps -0x7ad76b2(%rip),%ymm0,%ymm0 7.66% &#9474; &#9474; 0x0000000113d438b2: vmovdqu %ymm0,0x10(%r9,%rbx,4) 26.20% &#9474; &#9474; 0x0000000113d438b9: add $0x8,%ebx 6.44% &#9474; &#9474; 0x0000000113d438bc: cmp %r11d,%ebx \\ &#9474; 0x0000000113d438bf: jl 0x0000000113d43890 This is the output of a JMH micro-benchmark for the above code using the prototype of the Vector API and implementation found on the vectorIntrinsics branch of Project Panama's development repository. These hot areas of generated machine code show a clear translation to vector registers and vector instructions. We disabled loop unrolling in order to make the translation clearer; otherwise, HotSpot would unroll this code using existing C2 loop optimizations. All Java object allocations are elided. Run-time compilation The Vector API has two implementations. The first implements operations in Java, thus it is functional but not optimal. The second defines intrinsic vector operations for the HotSpot C2 run-time compiler so that it can compile vector computations to appropriate hardware registers and vector instructions when available. To avoid an explosion of C2 intrinsics we define generalized intrinsics corresponding to the various kinds of operations such as unary, binary, conversion, and so on, which take a parameter describing the specific operation to be performed. Approximately twenty new intrinsics support the intrinsification of the entire API. We expect ultimately to declare vector classes as primitive classes, as proposed by Project Valhalla in JEP 401 (Primitive Objects). In the meantime Vector&lt;E&gt; and its subclasses are considered value-based classes, so identity-sensitive operations on their instances should be avoided. Although vector instances are abstractly composed of elements in lanes, those elements are not scalarized by C2 &#8212; a vector&#8217;s value is treated as a whole unit, like an int or a long, that maps to a vector register of the appropriate size. Vector instances are treated specially by C2 in order to overcome limitations in escape analysis and avoid boxing. Intel SVML intrinsics for transcendental operations The Vector API supports transcendental and trigonometric lanewise operations on floating point vectors. On x64 we leverage the Intel Short Vector Math Library (SVML) to provide optimized intrinsic implementations for such operations. The intrinsic operations have the same numerical properties as the corresponding scalar operations defined in java.lang.Math. The assembly source files for SVML operations are in the source code of the jdk.incubator.vector module, under OS-specific directories. The JDK build process compiles these source files for the target operating system into an SVML-specific shared library. This library is fairly large, weighing in at just under a megabyte. If a JDK image, built via jlink, omits the jdk.incubator.vector module then the SVML library will not be copied into the image. The implementation only supports Linux and Windows at this time. We will consider macOS support later, since it is a non-trivial amount of work to provide assembly source files with the required directives. The HotSpot runtime will attempt to load the SVML library and, if present, bind the operations in the SVML library to named stub routines. The C2 compiler generates code that calls the appropriate stub routine based on the operation and vector species (i.e., element type and shape). In the future, if Project Panama expands its support of native calling conventions to support vector values then it may be possible for the Vector API implementation to load the SVML library from an external source. If there is no performance impact with this approach then it would no longer be necessary to include SVML in source form and build it into the JDK. Until then we deem the above approach acceptable, given the potential performance gains. Future work As mentioned above, we expect ultimately to declare vector classes as primitive classes. We expect, further, to leverage Project Valhalla&#8217;s generic specialization of primitive classes so that instances of Vector&lt;E&gt; can be primitive values whose concrete types are primitive types. This will make it easier to optimize and express vector computations. Subtypes of Vector&lt;E&gt; for specific types, such as IntVector, might not be required once we have generic specialization over primitive classes. We intend to incubate the API over multiple releases and adapt it as primitive classes and related facilities become available. We hope to improve the performance of vector operations that accept masks on architectures that support masking in hardware. If masks were more efficient then the example above could be written more simply, without the scalar loop to process the tail elements, while still achieving optimal performance: void vectorComputation(float[] a, float[] b, float[] c) { for (int i = 0; i &lt; a.length; i += SPECIES.length()) { // VectorMask&lt;Float&gt; m; var m = SPECIES.indexInRange(i, a.length); // FloatVector va, vb, vc; var va = FloatVector.fromArray(SPECIES, a, i, m); var vb = FloatVector.fromArray(SPECIES, b, i, m); var vc = va.mul(va) .add(vb.mul(vb)) .neg(); vc.intoArray(c, i, m); } } We intend to enhance the API to load and store vectors using JEP 412 (Foreign Function &amp; Memory API) when that API transitions out of incubation. Memory layouts that describe vector species may prove useful, for example to stride over a memory segment comprised of vector elements. We anticipate enhancing the implementation to improve the optimization of loops containing vectorized code, support the ARM SVE platform, and generally improve performance incrementally over time.",
    "specification": ""
  },
  {
    "number": "JEP 374",
    "title": "Deprecate and Disable Biased Locking",
    "url": "https://openjdk.org/jeps/374",
    "summary": "Disable biased locking by default, and deprecate all related command-line options.",
    "goals": "Determine the need for continued support of the legacy synchronization optimization of biased locking, which is costly to maintain.",
    "motivation": "Biased locking is an optimization technique used in the HotSpot Virtual Machine to reduce the overhead of uncontended locking. It aims to avoid executing a compare-and-swap atomic operation when acquiring a monitor by assuming that a monitor remains owned by a given thread until a different thread tries to acquire it. The initial lock of the monitor biases the monitor towards that thread, avoiding the need for atomic instructions in subsequent synchronized operations on the same object. When many threads perform many synchronized operations on objects used in a single-threaded fashion, biasing the locks has historically led to significant performance improvements over regular locking techniques. The performance gains seen in the past are far less evident today. Many applications that benefited from biased locking are older, legacy applications that use the early Java collection APIs, which synchronize on every access (e.g., Hashtable and Vector). Newer applications generally use the non-synchronized collections (e.g., HashMap and ArrayList), introduced in Java 1.2 for single-threaded scenarios, or the even more-performant concurrent data structures, introduced in Java 5, for multi-threaded scenarios. This means that applications that benefit from biased locking due to unnecessary synchronization will likely see a performance improvement if the code is updated to use these newer classes. Furthermore, applications built around a thread-pool queue and worker threads generally perform better with biased locking disabled. (SPECjbb2015 was designed that way, e.g., while SPECjvm98 and SPECjbb2005 were not). Biased locking comes with the cost of requiring an expensive revocation operation in case of contention. Applications that benefit from it are therefore only those that exhibit significant amounts of uncontended synchronized operations, like those mentioned above, so that the cost of executing cheap lock owner checks plus an occasional expensive revocation is still lower than the cost of executing the eluded compare-and-swap atomic instructions. Changes in the cost of atomic instructions since the introduction of biased locking into HotSpot also change the amount of uncontended operations needed for that relation to remain true. Another aspect worth noting is that applications won't have noticeable performance improvements from biased locking even when the previous cost relation is true when the time spend on synchronized operations is still only a small fraction of the total application workload. Biased locking introduced a lot of complex code into the synchronization subsystem and is invasive to other HotSpot components as well. This complexity is a barrier to understanding various parts of the code and an impediment to making significant design changes within the synchronization subsystem. To that end we would like to disable, deprecate, and eventually remove support for biased locking.",
    "description": "Prior to JDK 15, biased locking is always enabled and available. With this JEP, biased locking will no longer be enabled when HotSpot is started unless -XX:+UseBiasedLocking is set on the command line. We will deprecate the UseBiasedLocking option and all options related to the configuration and use of biased locking. Product options: BiasedLockingStartupDelay, BiasedLockingBulkRebiasThreshold, BiasedLockingBulkRevokeThreshold, BiasedLockingDecayTime and UseOptoBiasInlining Diagnostic options: PrintBiasedLockingStatistics and PrintPreciseBiasedLockingStatistics The options will still be accepted and acted upon, but a deprecation warning will be issued.",
    "specification": ""
  },
  {
    "number": "JEP 366",
    "title": "Deprecate the ParallelScavenge + SerialOld GC Combination",
    "url": "https://openjdk.org/jeps/366",
    "summary": "Deprecate the combination of the Parallel Scavenge and Serial Old garbage collection algorithms.",
    "goals": "",
    "motivation": "There is one combination of GC algorithms that we believe is very little used but requires a significant amount of maintenance effort: The pairing of the parallel young generation GC (called ParallelScavenge) and the serial old GC (called SerialOld). This combination must be specifically enabled by the user with the -XX:+UseParallelGC -XX:-UseParallelOldGC command line options. This combination is unusual since it pairs the parallel young generation and serial old generation GC algorithms. We think this combination is only useful for deployments with a very large young generation and a very small old generation. In this scenario the full collection pause times might be bearable due to the small size of the old generation. In practice this is a very rare and risky deployment, since a slight shift in liveness for objects in the young generation will result in an OutOfMemoryException, since the old generation is significantly smaller than the young generation. The only advantage of this combination compared to using a parallel GC algorithm for both the young and old generations is slightly lower total memory usage. We believe that this small memory footprint advantage (at most ~3% of the Java heap size) is not enough to outweigh the costs of maintaining this GC combination.",
    "description": "In addition to deprecating the option combination -XX:+UseParallelGC -XX:-UseParallelOldGC we will also deprecate the option -XX:UseParallelOldGC, since its only use is to deselect the parallel old generation GC, thereby enabling the serial old generation GC. As a result, any explicit use of the UseParallelOldGC option will display a deprecation warning. A warning will, in particular, be displayed when -XX:+UseParallelOldGC is used standalone (without -XX:+UseParallelGC) to select the parallel young and old generation GC algorithms. The only way to select the parallel young and old generation GC algorithms without a deprecation warning will to specify only -XX:+UseParallelGC on the command line.",
    "specification": ""
  },
  {
    "number": "JEP 398",
    "title": "Deprecate the Applet API for Removal",
    "url": "https://openjdk.org/jeps/398",
    "summary": "Deprecate the Applet API for removal. It is essentially irrelevant since all web-browser vendors have either removed support for Java browser plug-ins or announced plans to do so.",
    "goals": "",
    "motivation": "",
    "description": "Deprecate, for removal, these classes and interfaces of the standard Java API: java.applet.Applet java.applet.AppletStub java.applet.AppletContext java.applet.AudioClip javax.swing.JApplet java.beans.AppletInitializer Deprecate, for removal, any API elements that reference the above classes and interfaces, including methods and fields in: java.beans.Beans javax.swing.RepaintManager javax.naming.Context",
    "specification": ""
  },
  {
    "number": "JEP 411",
    "title": "Deprecate the Security Manager for Removal",
    "url": "https://openjdk.org/jeps/411",
    "summary": "Deprecate the Security Manager for removal in a future release. The Security Manager dates from Java 1.0. It has not been the primary means of securing client-side Java code for many years, it has rarely been used to secure server-side code, and it is costly to maintain. To move the Java Platform forward, we will deprecate the Security Manager for removal in concert with the legacy Applet API (JEP 398).",
    "goals": "Prepare developers for the removal of the Security Manager in a future version of Java. Warn users if their Java applications rely on the Security Manager. Evaluate whether new APIs or mechanisms are needed to address specific narrow use cases for which the Security Manager has been employed, such as blocking System::exit.",
    "motivation": "The Java Platform emphasizes security. The integrity of data is protected by the Java language and VM's built-in memory safety: Variables are initialized before use, array bounds are checked, and memory deallocation is completely automatic. Meanwhile, the confidentiality of data is protected by the Java class libraries' trusted implementations of modern cryptographic algorithms and protocols such as SHA-3, EdDSA, and TLS 1.3. Security is a dynamic science, thus we continuously update the Java Platform to address new vulnerabilities and to reflect new industry postures, for example by deprecating weak cryptographic protocols. One long-time element of security is the Security Manager, which dates from Java&#160;1.0. In the era of Java applets downloaded by web browsers, the Security Manager protected the integrity of users' machines and the confidentiality of their data by running applets in a sandbox, which denied access to resources such as the file system or the network. The small size of the Java class libraries &#8212; only eight java.* packages in Java&#160;1.0 &#8212; made it feasible for code in, e.g., java.io to consult with the Security Manager before performing any operation. The Security Manager drew a bright line between untrusted code (applets from a remote machine) and trusted code (classes on the local machine): It would approve all operations involving resource access for trusted code but reject them for untrusted code. As interest in Java grew, we introduced signed applets to allow the Security Manager to place trust in remote code, thereby allowing applets to access the same resources as local code run via java on the command line. Simultaneously, the Java class libraries were expanding rapidly &#8212; Java&#160;1.1 introduced JavaBeans, JDBC, Reflection, RMI, and Serialization &#8212; which meant that trusted code gained access to significant new resources such as database connections, RMI servers, and reflective objects. Allowing all trusted code to access all resources was undesirable, so in Java&#160;1.2 we redesigned the Security Manager to focus on applying the principle of least privilege: All code would be treated as untrusted by default, subject to sandbox-style controls that prevented access to resources, and users would place trust in specific codebases by granting them specific permissions to access specific resources. In theory, an application JAR on the class path might be more limited in how it uses the JDK than an applet from the Internet. Limiting permissions was seen as a way to constrain the impact of any vulnerabilities that might exist in a body of code &#8212; in effect, a defense-in-depth mechanism. The Security Manager, then, had ambitions to protect against two kinds of threat: Malicious intent, especially in remote code, and accidental vulnerabilities, especially in local code. The threat of malicious intent by remote code has receded because the Java Platform no longer supports applets. The Applet API was deprecated in Java&#160;9 in 2017, then deprecated for removal in Java&#160;17 in 2021 with the intent to remove it in a future release. The closed-source browser plugin that ran applets was removed from Oracle's JDK&#160;11 in 2018 along with the closed-source Java Web Start technology. Accordingly, many of the risks that the Security Manager protects against are no longer significant. Furthermore, the Security Manager cannot protect against many risks that now are significant. The Security Manager cannot address 19 of the 25 most dangerous issues identified by industry leaders in 2020, so issues such as XML external entity reference (XXE) injection and improper input validation have required direct countermeasures in the Java class libraries. (For example, JAXP can protect against XXE attacks and XML entity expansion, while serialization filtering can prevent malicious data from being deserialized before it can do any damage.) The Security Manager is also incapable of preventing malicious behavior based on speculative-execution vulnerabilities. The Security Manager's lack of efficacy against malicious intent is unfortunate because the Security Manager is, of necessity, woven into the fabric of the Java class libraries. As such, it is an ongoing maintenance burden. All new features and APIs must be evaluated to ensure that they behave correctly when the Security Manager is enabled. Access control based on the least-privilege principle may have been feasible in the class libraries of Java&#160;1.0, but the rapid growth of java.* and javax.* packages led to dozens of permissions and hundreds of permission checks throughout the JDK. This is a significant surface area to keep secure, especially since permissions can interact in surprising ways. Some permissions, e.g., allow application or library code to perform a series of safe operations whose overall effect is sufficiently unsafe that it would require a more powerful permission if granted directly. The threat of accidental vulnerabilities in local code is almost impossible to address with the Security Manager. Many of the claims that the Security Manager is widely used to secure local code do not stand up to scrutiny; it is used far less in production than many people assume. There are many reasons for its lack of use: Brittle permission model &#8212; An application developer wishing to benefit from the Security Manager must carefully grant all of the permissions that the application requires for all of the operations that it performs. There is no way to have partial security, where only a few resources are subject to access control. For example, suppose a developer is concerned with illegitimate access to data and thus wishes to grant permission to read files only from a particular directory. Granting the file-read permission is insufficient because the application will almost certainly use other operations in the Java class libraries besides reading files (for example, writing files), and those other operations will be rejected by the Security Manager since the code will not have the appropriate permission. Only a developer who carefully documents how their code interacts with security-sensitive operations in the Java class libraries will be able to grant the necessary permissions. This is not a common developer workflow. (The Security Manager does not allow negative permissions that could express \"grant permission for all operations except reading files\".) Difficult programming model &#8212; The Security Manager approves a security-sensitive operation by checking the permissions of all running code that led up to the operation. This makes it difficult to write libraries that run with the Security Manager because it is not sufficient for a library developer to document the permissions that their library code needs. It is also necessary for an application developer who uses the library to grant those same permissions to their application code, in addition to any permissions already granted to that code. This violates the principle of least privilege, since the application code may not need the library's permissions for its own operations. The library developer can mitigate this viral growth of permissions by carefully using the java.security.AccessController API to request that the Security Manager take only the library's permissions into account, but the complexity of this and other secure coding guidelines is far beyond the interest of most developers. The path of least resistance for application developers is often to grant AllPermission to any relevant JAR file, but this again runs counter to the principle of least privilege. Poor performance &#8212; The heart of the Security Manager is a complex access-control algorithm which often imposes an unacceptable performance penalty. For this reason, the Security Manager has always been disabled by default for JVMs run on the command line. This further reduces developers' interest in investing to make libraries and applications run with the Security Manager. The lack of tooling to help infer and validate permissions is an additional impediment. In the quarter-century since the Security Manager was introduced, adoption has been low. Only a handful of applications ship with policy files that constrain their own operations (e.g., ElasticSearch). Similarly, only a handful of frameworks ship with policy files (e.g., Tomcat), and developers building applications with those frameworks still face the practically insurmountable challenge of figuring out the permissions needed by their own code and by the libraries they use. Some frameworks (e.g., NetBeans) eschew policy files and instead implement a custom Security Manager in order to prevent plugins from calling System::exit or to gain insight into code's behavior, such as whether it opens files and network connections &#8212; use cases which we think are better served by other means. In summary, there is no significant interest in developing modern Java applications with the Security Manager. Making access-control decisions based on permissions is unwieldy, slow, and falling out of favor across the industry; .NET, e.g., no longer supports it. Security is better achieved by providing integrity at lower levels of the Java Platform &#8212; by, for example, strengthening module boundaries (JEP 403) to prevent access to JDK implementation details, and hardening the implementation itself &#8212; and by isolating the entire Java runtime from sensitive resources via out-of-process mechanisms such as containers and hypervisors. To move the Java Platform forward, we will deprecate the legacy Security Manager technology for removal from the JDK. We plan to deprecate and attenuate the capabilities of the Security Manager over a number of releases, simultaneously creating alternative APIs for such tasks as blocking System::exit and other use cases considered important enough to have replacements.",
    "description": "In Java 17, we will: Deprecate, for removal, most Security Manager related classes and methods. Issue a warning message at startup if the Security Manager is enabled on the command line. Issue a warning message at run time if a Java application or library installs a Security Manager dynamically. In Java 18, we will prevent a Java application or library from dynamically installing a Security Manager unless the end user has explicitly opted to allow it. Historically, a Java application or library was always allowed to dynamically install a Security Manager, but since Java 12, the end user has been able to prevent it by setting the system property java.security.manager to disallow on the command line (java -Djava.security.manager=disallow ...) -- this causes System::setSecurityManager to throw an UnsupportedOperationException. Starting in Java 18, the default value of java.security.manager will be disallow if not otherwise set via java -D.... As a result, applications and libraries that call System::setSecurityManager may fail due to an unexpected UnsupportedOperationException. In order for System::setSecurityManager to work as before, the end user will have to set java.security.manager to allow on the command line (java -Djava.security.manager=allow ...). In feature releases after Java 18, we will degrade other Security Manager APIs so that they remain in place but with limited or no functionality. For example, we may revise AccessController::doPrivileged simply to run the given action, or revise System::getSecurityManager always to return null. This will allow libraries that support the Security Manager and were compiled against previous Java releases to continue to work without change or even recompilation. We expect to remove the APIs once the compatibility risk of doing so declines to an acceptable level. In feature releases after Java 18, we may alter the Java SE API definition so that operations which previously performed permission checks no longer perform them, or perform fewer checks when a Security Manager is enabled. As a result, @throws SecurityException will appear on fewer methods in the API specification. Deprecate APIs for removal The Security Manager consists of the class java.lang.SecurityManager and a number of closely related APIs in the java.lang and java.security packages. We will terminally deprecate the following eight classes and two methods, by annotating them with @Deprecated(forRemoval=true): java.lang.SecurityManager &#8212; The primary API for the Security Manager. java.lang.System::{setSecurityManager, getSecurityManager} &#8212; Methods for setting and getting the Security Manager. java.security.{Policy, PolicySpi, Policy.Parameters} &#8212; The primary APIs for policy, which are used to determine if code running under the Security Manager has been granted permission to perform specific privileged operations. java.security.{AccessController, AccessControlContext, AccessControlException, DomainCombiner} &#8212; The primary APIs for the access controller, which is the default implementation to which the Security Manager delegates permission checks. These APIs do not have value without the Security Manager, since certain operations will not work without both a policy implementation and access-control context support in the VM. We will also terminally deprecate the following two classes and eight methods which depend strongly on the Security Manager: java.lang.Thread::checkAccess, java.lang.ThreadGroup::checkAccess, and java.util.logging.LogManager::checkAccess &#8212; These three methods are anomalous because they allow ordinary Java code to check whether it would be trusted to perform certain operations without actually performing them. They serve no purpose without the Security Manager. java.util.concurrent.Executors::{privilegedCallable, privilegedCallableUsingCurrentClassLoader, privilegedThreadFactory} &#8212; These utility methods are only useful when the Security Manager is enabled. java.rmi.RMISecurityManager &#8212; RMI&#8217;s Security Manager class. This class is obsolete and was deprecated in Java&#160;8. javax.security.auth.SubjectDomainCombiner and javax.security.auth.Subject::{doAsPrivileged, getSubject} &#8212; APIs for user-based authorization that are dependent on Security Manager APIs such as AccessControlContext and DomainCombiner. We plan to provide a replacement API for Subject::getSubject since it is commonly used for use cases that do not require the Security Manager, and to continue to support use cases involving Subject::doAs (see below). We will not deprecate some classes in the java.security package that are related to the Security Manager, for various reasons: SecureClassLoader &#8212; The superclass of java.net.URLClassLoader. Also, as of Java&#160;9, SecureClassLoader is instrumental in the implementation of the application class loader and the platform class loader. CodeSource &#8212; Although most often associated with granting permissions based on the location of code, CodeSource is not directly tied to the Security Manager and can provide independent value as a means to identify the source of a body of code and, optionally, who signed it. ProtectionDomain &#8212; Several significant APIs depend on ProtectionDomain, e.g., ClassLoader::defineClass and Class::getProtectionDomain. ProtectionDomain also has value independent of the Security Manager since it contains the CodeSource of a class. Permission and subclasses &#8212; Other significant classes, such as ProtectionDomain, depend on Permission. Many of the subclasses of Permission, however, are specific to use cases which will likely no longer be relevant after the Security Manager is removed. The maintainers of these subclasses can deprecate and remove them separately, after evaluating the compatibility risk. PermissionCollection and Permissions &#8212; Classes that hold collections of Permission objects and do not depend directly on the Security Manager. PrivilegedAction, PrivilegedExceptionAction, and PrivilegedActionException &#8212; These APIs do not depend directly on the Security Manager and are used by the javax.security.auth API for authentication and authorization (see below). SecurityException &#8212; A runtime exception thrown by Java APIs when a permission check fails. We may deprecate this API for removal at a later date, but for now the impact of doing so would be too high. We will not deprecate the javax.security.auth.Subject::doAs method since it can be used to transport a Subject across API boundaries by attaching it to the thread's AccessControlContext, serving a purpose similar to a ThreadLocal. The credentials of the Subject can then be obtained by an underlying authentication mechanism (e.g., a Kerberos implementation of GSSAPI) by calling Subject::getSubject. These credentials can be used for authentication or authorization purposes and do not require the Security Manager to be enabled. However, Subject::doAs depends on APIs tightly related to the Security Manager, such as AccessControlContext and DomainCombiner. Thus, we plan to create a new API that does not depend on the Security Manager APIs; subsequently we will then deprecate the Subject::doAs API for removal. We will not deprecate any tools. (We removed the policytool GUI for editing policy files in JDK&#160;10.) Issue warnings We will make the following changes to ensure that developers and users are aware that the Security Manager is deprecated for removal. If the default Security Manager or a custom Security Manager is enabled at startup: java -Djava.security.manager MyApp java -Djava.security.manager=\"\" MyApp java -Djava.security.manager=default MyApp java -Djava.security.manager=com.foo.bar.Server MyApp then the following warning is issued at startup: WARNING: A command line option has enabled the Security Manager WARNING: The Security Manager is deprecated and will be removed in a future release This warning, unlike compile-time deprecation warnings, cannot be suppressed. (The four invocations of java -D... shown above set the system property java.security.manager to, respectively, the empty string, the empty string, the string default, and the class name of a custom Security Manager. These invocations were the supported ways of enabling a Security Manager at startup in Java releases prior to Java 12. Java 12 added support for the strings allow and disallow, shown next.) If a Security Manager is not enabled at startup, but could be installed dynamically during run time: java MyApp java -Djava.security.manager=allow MyApp then no warning is issued at startup. Instead, a warning is issued at run time when System::setSecurityManager is called, as follows: WARNING: A terminally deprecated method in java.lang.System has been called WARNING: System::setSecurityManager has been called by com.foo.bar.Server (file:/tmp/foobarserver/thing.jar) WARNING: Please consider reporting this to the maintainers of com.foo.bar.Server WARNING: System::setSecurityManager will be removed in a future release This warning is shown once per caller, and unlike compile-time deprecation warnings, cannot be suppressed. If a Security Manager is not enabled at startup, and the system property java.security.manager is set to disallow: java -Djava.security.manager=disallow MyApp then no warning is issued at startup, nor at run time if an attempt is made to install a Security Manager dynamically by invoking System::setSecurityManager. However, every invocation of System::setSecurityManager throws an UnsupportedOperationException with the following detail message: The Security Manager is deprecated and will be removed in a future release In Java 18, disallow will become the default value of java.security.manager. The command line java MyApp will then have the same effect as java -Djava.security.manager=disallow MyApp had on Java 17.",
    "specification": ""
  },
  {
    "number": "JEP 363",
    "title": "Remove the Concurrent Mark Sweep (CMS) Garbage Collector",
    "url": "https://openjdk.org/jeps/363",
    "summary": "Remove the Concurrent Mark Sweep (CMS) garbage collector.",
    "goals": "",
    "motivation": "More than two years ago in, JEP 291, we deprecated the CMS collector for removal in a future release, in order to accelerate the development of other collectors. During this time, no credible contributors stepped up to take on the maintenance of CMS. During this time we&#8217;ve also seen the introduction of two new collectors, ZGC and Shenandoah, along with further improvements to G1, which has been the intended successor to CMS since JDK 6. At this point the garbage collectors available in the Hotspot JVM, if they do not surpass CMS&#8217;s performance, have a small enough overhead that it is now safe to remove CMS. We expect that future improvements to the existing collectors will reduce the need for CMS even further.",
    "description": "This change will disable compilation of CMS, remove the contents of the gc/cms directory in the source tree, and remove options that pertain solely to CMS. References to CMS in the documentation will also be purged. Tests that try to use CMS will be removed or adapted as necessary. Trying to use CMS via the -XX:+UseConcMarkSweepGC option will result in the following warning message: Java HotSpot(TM) 64-Bit Server VM warning: Ignoring option UseConcMarkSweepGC; \\ support was removed in &lt;version&gt; and the VM will continue execution using the default collector.",
    "specification": ""
  },
  {
    "number": "JEP 410",
    "title": "Remove the Experimental AOT and JIT Compiler",
    "url": "https://openjdk.org/jeps/410",
    "summary": "Remove the experimental Java-based ahead-of-time (AOT) and just-in-time (JIT) compiler. This compiler has seen little use since its introduction and the effort required to maintain it is significant. Retain the experimental Java-level JVM compiler interface (JVMCI) so that developers can continue to use externally-built versions of the compiler for JIT compilation.",
    "goals": "",
    "motivation": "Ahead-of-time compilation (the jaotc tool) was incorporated into JDK&#160;9 as an experimental feature via JEP 295. The jaotc tool uses the Graal compiler, which is itself written in Java, for AOT compilation. The Graal compiler was made available as an experimental JIT compiler in JDK&#160;10 via JEP 317. We have seen little use of these experimental features since they were introduced, and the effort required to maintain and enhance them is significant. These features were not included in the JDK&#160;16 builds published by Oracle, and no one complained.",
    "description": "Remove three JDK modules: jdk.aot &#8212; the jaotc tool jdk.internal.vm.compiler &#8212; the Graal compiler jdk.internal.vm.compiler.management &#8212; Graal's MBean Preserve these two Graal-related source files so that the JVMCI module (jdk.internal.vm.ci, JEP 243) continues to build: src/jdk.internal.vm.compiler/share/classes/module-info.java src/jdk.internal.vm.compiler.management/share/classes/module-info.java Remove HotSpot code related to AOT compilation: src/hotspot/share/aot &#8212; dumps and loads AOT code Additional code guarded by #if INCLUDE_AOT Finally, remove tests as well as code in makefiles related to Graal and AOT compilation.",
    "specification": ""
  },
  {
    "number": "JEP 381",
    "title": "Remove the Solaris and SPARC Ports",
    "url": "https://openjdk.org/jeps/381",
    "summary": "Remove the source code and build support for the Solaris/SPARC, Solaris/x64, and Linux/SPARC ports. These ports were deprecated for removal in JDK 14 with the express intent to remove them in a future release.",
    "goals": "Remove all source code specific to the Solaris operating system Remove all source code specific to the SPARC architecture Update documentation and source code comments for future releases",
    "motivation": "Many projects and features currently in development such as Valhalla, Loom, and Panama require significant changes to CPU-architecture and operating-system specific code. Dropping support for the Solaris and SPARC ports will enable contributors in the OpenJDK Community to accelerate the development of new features that will move the platform forward.",
    "description": "We will remove or adjust all of the Solaris- and SPARC-related code, build-system logic, and documentation. Remove directories: src/hotspot/cpu/sparc src/hotspot/os/solaris src/hotspot/os_cpu/solaris src/hotspot/os_cpu/linux_sparc src/hotspot/os_cpu/solaris_x86 src/java.base/solaris src/java.desktop/solaris src/jdk.attach/solaris src/jdk.crypto.cryptoki/solaris src/jdk.crypto.ucrypto/solaris src/jdk.management/solaris src/jdk.net/solaris Remove or adjust C/C++ code guarded by the following preprocessor definitions and macros: SPARC, __sparc__, __sparc, __sparcv9 SOLARIS, __solaris__ SPARC_ONLY, NOT_SPARC SOLARIS_ONLY, NOT_SOLARIS SOLARIS_MUTATOR_LIBTHREAD SPARC_WORKS Remove or adjust Java code that checks for Solaris or SunOS, for example: System.getProperty(&#8220;os.name&#8221;).contains(&#8220;Solaris&#8221;) System.getProperty(&#8220;os.name&#8221;).startsWith(\"SunOS\") Remove Solaris-specific features: The OracleUcrypto provider in the jdk.crypto.ucrypto module (8234870) The jdk.net.SocketFlow socket option in the jdk.net module (8234871) Remove or adjust build system (automake, etc.) logic related to Solaris, SPARC, or Oracle Studio; specifically, the following variables and values: OPENJDK_{BUILD,TARGET}_OS = Solaris OPENJDK_{BUILD,TARGET}_CPU_ARCH = sparc TOOLCHAIN_TYPE = solstudio is{Build,Target}Os = solaris is{Build,Target}Cpu = sparcv9 Remove or adjust tests only relevant to, or only executed on, Solaris or SPARC, for example: jtreg tests using @requires os.family == \"solaris\" @requires os.arch == \"sparc\" @requires os.arch == \"sparcv9\" @requires (vm.simpleArch == \"sparcv9\") The Platform.isSolaris() or Platform.isSparc() test library methods, and the methods themselves Clean up problem lists to remove any references to solaris or SPARC Adjust comments in the source code referring to Solaris or SPARC, with care In many cases the comments can simply be removed, but some references to Solaris and SPARC may still be relevant even after the ports are removed Remove the Solaris devkit creator scripts (under make/devkit) Remove any Solaris- or SPARC-specific logic in the JIB configuration file",
    "specification": ""
  },
  {
    "number": "JEP 407",
    "title": "Remove RMI Activation",
    "url": "https://openjdk.org/jeps/407",
    "summary": "Remove the Remote Method Invocation (RMI) Activation mechanism, while preserving the rest of RMI.",
    "goals": "",
    "motivation": "The RMI Activation mechanism is obsolete and disused. It was deprecated for removal by JEP 385 in Java SE 15. No comments were received in response to that deprecation. Please see JEP 385 for the full background, rationale, risks, and alternatives. The Java EE Platform contained a technology called the JavaBeans Activation Framework (JAF). This was later renamed to Jakarta Activation as part of the Eclipse EE4J initiative. The JavaBeans Activation and Jakarta Activation technologies are completely unrelated to RMI Activation, and they are unaffected by the removal of RMI Activation from Java SE.",
    "description": "Remove the java.rmi.activation package from the Java SE API specification Update the RMI Specification to remove mentions of RMI Activation Remove the JDK library code that implements the RMI Activation mechanism Remove the JDK regression tests for the RMI Activation mechanism Remove the JDK's rmid activation daemon and its documenation",
    "specification": ""
  },
  {
    "number": "JEP 372",
    "title": "Remove the Nashorn JavaScript Engine",
    "url": "https://openjdk.org/jeps/372",
    "summary": "Remove the Nashorn JavaScript script engine and APIs, and the jjs tool. The engine, the APIs, and the tool were deprecated for removal in Java 11 with the express intent to remove them in a future release.",
    "goals": "",
    "motivation": "The Nashorn JavaScript engine was first incorporated into JDK 8 via JEP 174 as a replacement for the Rhino scripting engine. When it was released, it was a complete implementation of the ECMAScript-262 5.1 standard. With the rapid pace at which ECMAScript language constructs, along with APIs, are adapted and modified, we have found Nashorn challenging to maintain.",
    "description": "Two JDK modules will be permanently removed: jdk.scripting.nashorn -- contains the jdk.nashorn.api.scripting and jdk.nashorn.api.tree packages. jdk.scripting.nashorn.shell -- contains the jjs tool.",
    "specification": ""
  },
  {
    "number": "JEP 367",
    "title": "Remove the Pack200 Tools and API",
    "url": "https://openjdk.org/jeps/367",
    "summary": "Remove the pack200 and unpack200 tools, and the Pack200 API in the java.util.jar package. These tools and API were deprecated for removal in Java SE 11 with the express intent to remove them in a future release.",
    "goals": "",
    "motivation": "Pack200 is a compression scheme for JAR files, introduced in Java SE 5.0 by JSR 200. Its goal is \"to decrease disk and bandwidth requirements for Java application packaging, transmission, and delivery.\" Developers use a pair of tools -- pack200 and unpack200 -- to compress and uncompress their JAR files. An API is available in the java.util.jar package. There are three reasons to remove Pack200: Historically, slow downloads of the JDK over 56k modems were an impediment to Java adoption. The relentless growth in JDK functionality caused the download size to swell, further impeding adoption. Compressing the JDK with Pack200 was a way to mitigate the problem. However, time has moved on: download speeds have improved, and JDK 9 introduced new compression schemes for both the Java runtime (JEP 220) and the modules used to build the runtime (JMOD). Consequently, JDK 9 and later do not rely on Pack200; JDK 8 was the last release compressed with pack200 at build time and uncompressed with unpack200 at install time. In summary, a major consumer of Pack200 -- the JDK itself -- no longer needs it. Beyond the JDK, it was attractive to compress client applications, and especially applets, with Pack200. Some deployment technologies, such as Oracle's browser plug-in, would uncompress applet JARs automatically. However, the landscape for client applications has changed, and most browsers have dropped support for plug-ins. Consequently, a major class of consumers of Pack200 -- applets running in browsers -- are no longer a driver for including Pack200 in the JDK. Pack200 is a complex and elaborate technology. Its file format is tightly coupled to the class file format and the JAR file format, both of which have evolved in ways unforeseen by JSR 200. (For example, JEP 309 added a new kind of constant pool entry to the class file format, and JEP 238 added versioning metadata to the JAR file format.) The implementation in the JDK is split between Java and native code, which makes it hard to maintain. The API in java.util.jar.Pack200 was detrimental to the modularization of the Java SE Platform, leading to the removal of four of its methods in Java SE 9. Overall, the cost of maintaining Pack200 is significant, and outweighs the benefit of including it in Java SE and the JDK.",
    "description": "Three types in the java.base module previously annotated with @Deprecated(forRemoval=true) will be removed in the JDK feature release to which this JEP is ultimately targeted: java.util.jar.Pack200 java.util.jar.Pack200.Packer java.util.jar.Pack200.Unpacker The jdk.pack module, which contains the pack200 and unpack200 tools, was previously annotated with @Deprecated(forRemoval=true) and will also be removed in the JDK feature release to which this JEP is ultimately targeted.",
    "specification": ""
  }
]